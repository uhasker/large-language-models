{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f037d7",
   "metadata": {},
   "source": [
    "# Computational graphs and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b154759",
   "metadata": {},
   "source": [
    "In this chapter we will introduce the fundamental concepts that underpin all deep learning - computational graphs and backpropagation. To showcase these ideas we will create and train a neural network from scratch which will solve a *very simple* classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8668de8",
   "metadata": {},
   "source": [
    "If you want to follow along, you will need to execute the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2f875e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:13.755560Z",
     "iopub.status.busy": "2022-06-23T18:43:13.754565Z",
     "iopub.status.idle": "2022-06-23T18:43:14.255232Z",
     "shell.execute_reply": "2022-06-23T18:43:14.254232Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a28fe0",
   "metadata": {},
   "source": [
    "## Computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ad159",
   "metadata": {},
   "source": [
    "The basic concept you need to understand in order to grasp neural networks is that of a **computational graph**. Remember that a *graph* has nodes and edges. In a *computational graph* the *nodes represent functions*. These functions take input values and produce (compute) output values. The *edges carry* these *values* (which are either input values to the graph or results of preceding computations). Computational graphs allow for simple representations of complex functions.\n",
    "\n",
    "Consider the function $f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}, f(x, y, z) = (x + y) \\cdot z$. We will write $f = (x + y) \\cdot z$ to simplify notation.\n",
    "\n",
    "This function really consists of two functions. The first function calculates $x + y$. The second function multiplies the result of $x + y$ by $z$. Put simply, we first compute $g = x + y$ and then calculate $f = g \\cdot z$.\n",
    "\n",
    "This is how the visual representation of the computational graph representing $f$ would look like:\n",
    "\n",
    "<img src=\"images/compgraph.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "> Do note that this is not the most common visualization of computational graphs. Nevertheless we will stick to it, since it drives home most of the important ideas quite nicely.\n",
    "\n",
    "We can see that the graph has two nodes. The first node represents an addition function. It takes the values $x$ and $y$ and outputs the value $g$. The second node represents a multiplication function. It takes the values $g$ and $z$ and outputs the value $f$. Note that here one of the inputs to the multiplication node (namely $g$) is the output of a preceding node (namely the addition node).\n",
    "\n",
    "By decomposing arbitrary functions into simple components we can create computational graphs that represent very complex functions. In fact **neural networks** are nothing more than just certain kinds of computational graphs, namely computational graphs which are *differentiable*. This means that every node function has a gradient (or more technically a subgradient, but this distinction largely doesn't matter right now). If all of this sounds like wizardry, don't worry - we will get into this in a few minutes.\n",
    "\n",
    "We just saw that the edges carry values. But which values do they carry? There aren't many restrictions here. While in the above examples we had numbers, there is nothing that prevents us from passing vectors, matrices or even more complicated objects around. In fact, this is what we will usually do!\n",
    "\n",
    "Consider the function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}, f(\\vec{x}) = \\max(W\\vec{x} + \\vec{b}, \\vec{0})$ where $W \\in \\mathbb{R}^{m \\times n}$, $\\vec{b} \\in \\mathbb{R}^{m}$ and $\\max$ is the elementwise maximum. \n",
    "\n",
    "> The form of this function is no accident. It represents a (very simple) neural network.\n",
    "\n",
    "We can break this function up as following:\n",
    "\n",
    "$\\vec{c} = W\\vec{x}$\n",
    "\n",
    "$\\vec{d} = \\vec{c} + \\vec{b}$\n",
    "\n",
    "$\\vec{f} = \\max(\\vec{d}, \\vec{0})$\n",
    "\n",
    "The visual representation looks like this:\n",
    "\n",
    "<img src=\"images/compgraph2.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d388c9",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd6a93",
   "metadata": {},
   "source": [
    "A **tensor** is nothing else than a *multidimensional array* (if you've ever used numpy arrays, this is very similar). For example a zero-dimensional tensor is just a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d477d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.262185Z",
     "iopub.status.busy": "2022-06-23T18:43:14.262185Z",
     "iopub.status.idle": "2022-06-23T18:43:14.267186Z",
     "shell.execute_reply": "2022-06-23T18:43:14.267186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor0 = torch.tensor(1)\n",
    "tensor0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b8fc2",
   "metadata": {},
   "source": [
    "A one-dimensional tensor is a vector (or an array or a list, depending on what language you like more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152d6a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.270198Z",
     "iopub.status.busy": "2022-06-23T18:43:14.270198Z",
     "iopub.status.idle": "2022-06-23T18:43:14.282186Z",
     "shell.execute_reply": "2022-06-23T18:43:14.282186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3, 4])\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8f031",
   "metadata": {},
   "source": [
    "A two-dimensional tensor is a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca168072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.285201Z",
     "iopub.status.busy": "2022-06-23T18:43:14.285201Z",
     "iopub.status.idle": "2022-06-23T18:43:14.301200Z",
     "shell.execute_reply": "2022-06-23T18:43:14.303233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fc51c",
   "metadata": {},
   "source": [
    "You can also construct tensors of three, four or any arbitrary number of dimensions. Here is how a three-dimensional tensor could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e7a5d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.316197Z",
     "iopub.status.busy": "2022-06-23T18:43:14.314196Z",
     "iopub.status.idle": "2022-06-23T18:43:14.329223Z",
     "shell.execute_reply": "2022-06-23T18:43:14.329223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [4, 5]],\n",
       "\n",
       "        [[6, 7],\n",
       "         [8, 9]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor3 = torch.tensor([[[1, 2], [4, 5]], [[6, 7], [8, 9]]])\n",
    "tensor3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e37bd",
   "metadata": {},
   "source": [
    "You can get the dimension of a tensor using the *dim* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8fd903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.332606Z",
     "iopub.status.busy": "2022-06-23T18:43:14.332606Z",
     "iopub.status.idle": "2022-06-23T18:43:14.350369Z",
     "shell.execute_reply": "2022-06-23T18:43:14.349367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor0.dim(), tensor1.dim(), tensor2.dim(), tensor3.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493cbe86",
   "metadata": {},
   "source": [
    "Why would we need such high-dimensional tensors? Consider an NLP task where each *word is represented by a high-dimensional vector* (this is the *first* dimension). A *text consists of multiple words* (this is the *second* dimension). To decrease training time, we often pass *multiple texts at the same time* through a neural network (this is the *third* dimension). I want to reiterate, that this is *not* some theoretical construction, but in fact a very common setup. So the earlier you get used to high-dimensional tensors, the better.\n",
    "\n",
    "This is not meant to intimidate you (of course I *would* say that, wouldn't I). High-dimensional tensors sound scary (mostly because people associate them with quantum mechanics and the like), but for our purposes we don't care about most of that scariness. In fact, a lot of concepts concerning vectors and matrices generalize quite nicely to tensors.\n",
    "\n",
    "For example **tensor addition** and **tensor-constant multiplication** are done componentwise (just like with vectors and matrices).\n",
    "\n",
    "Consider another operation we will commonly use with tensors - their **product** (which we denote by $\\otimes$). Do note that we will introduce a very particular tensor product (which is sometimes called the *matrix product of tensors*). Depending on your background, you may be familiar with other tensor products.\n",
    "\n",
    "> This is nothing unusual. Consider matrix products for example. There is the matrix multiplication and the Hadamard product (which is the element-wise multiplication of two matrices). In fact even pytorch has a multitude of functions for computing tensor products (for example matmul and tensordot, which do different things). I want to stress - we introduce the tensor product the way we do because that is what is useful for us in this chapter - no more, no less.\n",
    "\n",
    "The product of two one-dimensional tensors is essentially the dot product. For example if $\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$ and $\\vec{w} = \\begin{bmatrix} 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{bmatrix}$, then $\\vec{v} \\otimes \\vec{w} = 1 \\cdot 5 + 2 \\cdot 6 + 3 \\cdot 7 + 4 \\cdot 8 = 70$. This is where you might have a first clash with the tensor product as it is known in other areas (where $\\vec{v} \\otimes \\vec{w}$ may be the *outer* product of $\\vec{v}$ and $\\vec{w}$).\n",
    "\n",
    "To compute this in pytorch, we could use the dot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86fe72c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.364374Z",
     "iopub.status.busy": "2022-06-23T18:43:14.363327Z",
     "iopub.status.idle": "2022-06-23T18:43:14.382373Z",
     "shell.execute_reply": "2022-06-23T18:43:14.380373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(70)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor([1, 2, 3, 4])\n",
    "w = torch.tensor([5, 6, 7, 8])\n",
    "torch.dot(v, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830f47c",
   "metadata": {},
   "source": [
    "We could also use the matmul function, which also generalizes to higher-dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2583c1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.393376Z",
     "iopub.status.busy": "2022-06-23T18:43:14.392367Z",
     "iopub.status.idle": "2022-06-23T18:43:14.413624Z",
     "shell.execute_reply": "2022-06-23T18:43:14.412626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(70)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor([1, 2, 3, 4])\n",
    "w = torch.tensor([5, 6, 7, 8])\n",
    "torch.matmul(v, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ab7a4",
   "metadata": {},
   "source": [
    "The product of a one-dimensional tensor and a two-dimensional tensor is just regular matrix-vector multiplication. For example if $W = \\begin{bmatrix} 1 & 3 \\\\ 5 & 7 \\\\ 9 & 11 \\end{bmatrix}$ and $\\vec{x} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$, then $W \\otimes \\vec{x} = W \\cdot \\vec{x} = \\begin{bmatrix} 1 \\cdot 2 + 3 \\cdot 4 \\\\ 5 \\cdot 2 + 7 \\cdot 4 \\\\ 9 \\cdot 2 + 11 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ 38 \\\\ 62 \\end{bmatrix}$.\n",
    "\n",
    "Let us reproduce this with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af56c14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.425585Z",
     "iopub.status.busy": "2022-06-23T18:43:14.423586Z",
     "iopub.status.idle": "2022-06-23T18:43:14.443268Z",
     "shell.execute_reply": "2022-06-23T18:43:14.442243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 38, 62])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.tensor([[1, 3], [5, 7], [9, 11]])\n",
    "x = torch.tensor([2, 4])\n",
    "torch.matmul(W, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce9c38",
   "metadata": {},
   "source": [
    "Similarly the product of two two-dimensional tensors is just regular matrix-matrix multiplication. For example if $U = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}$ and $V = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}$, then $U \\otimes V = U \\cdot V = \\begin{bmatrix} 1 \\cdot 2 + 2 \\cdot 1 & 1 \\cdot 1 + 2 \\cdot 3 \\\\ 3 \\cdot 2 + 4 \\cdot 1 & 3 \\cdot 1 + 4 \\cdot 3 \\\\ 5 \\cdot 2 + 6 \\cdot 1 & 5 \\cdot 1 + 6 \\cdot 3 \\end{bmatrix} = \\begin{bmatrix} 4 & 7 \\\\ 10 & 15 \\\\ 16 & 23 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2886d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.453237Z",
     "iopub.status.busy": "2022-06-23T18:43:14.452258Z",
     "iopub.status.idle": "2022-06-23T18:43:14.472207Z",
     "shell.execute_reply": "2022-06-23T18:43:14.473212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  7],\n",
       "        [10, 15],\n",
       "        [16, 23]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "V = torch.tensor([[2, 1], [1, 3]])\n",
    "torch.matmul(U, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f699c",
   "metadata": {},
   "source": [
    "But what about products of tensors with more than two dimensions? The tensor product generalizes in a straighforward fashion. Consider the multiplication of a three-dimensional tensor $Q$ of dimension $m \\times n \\times d$ (i.e. $Q \\in \\mathbb{R}^{m \\times n \\times d}$) with a one-dimensional tensor $\\vec{x}$ of dimension $d$ (i.e. $\\vec{x} \\in \\mathbb{R}^d$). We can treat $Q$ as a list of $m$ matrices of dimension $n \\times d$ each. Then we simply compute $m$ vector-matrix products. Each such product is a product of a matrix $Q_i \\in \\mathbb{R}^{n \\times d}$ and the vector $\\vec{x} \\in \\mathbb{R}^d$. Therefore each product results in vector of dimension $n$. We can stack these vectors back into a matrix resulting in a matrix of dimension $\\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "Consider the tensor $Q = \\begin{bmatrix} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 1\\end{bmatrix}, \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix} \\end{bmatrix}$ and the vector $\\vec{x} = \\begin{bmatrix} 5 \\\\ 6 \\\\ 7 \\end{bmatrix}$. We can think of $Q$ as a list of two matrices $Q = \\begin{bmatrix} Q_1 & Q_2 \\end{bmatrix}$ where $Q_1 = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ and $Q_2 = \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix}$.\n",
    "\n",
    "Now we have $Q \\otimes \\vec{x} = \\begin{bmatrix} (Q_1 \\cdot \\vec{x})^\\intercal \\\\ (Q_2 \\cdot \\vec{x})^\\intercal \\end{bmatrix}$ (note that we need to transpose the individual dot products). Now we easily obtain $Q_1 \\cdot \\vec{x} = \\begin{bmatrix} 12 \\\\ 6 \\\\ 12 \\\\ 18 \\end{bmatrix}$ and $Q_2 \\cdot \\vec{x} = \\begin{bmatrix} 7 \\\\ 13 \\\\ 18 \\\\ 5 \\end{bmatrix}$. Stacking these vectors together, we get that $Q \\otimes \\vec{x} = \\begin{bmatrix} 12 & 6 & 12 & 18 \\\\ 7 & 13 & 18 & 5 \\end{bmatrix}$.\n",
    "\n",
    "Let's confirm this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd367e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.482171Z",
     "iopub.status.busy": "2022-06-23T18:43:14.481209Z",
     "iopub.status.idle": "2022-06-23T18:43:14.487174Z",
     "shell.execute_reply": "2022-06-23T18:43:14.488208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.tensor([\n",
    "    [[1, 0, 1], [0, 1, 0], [1, 0, 1], [1, 1, 1]],\n",
    "    [[0, 0, 1], [0, 1, 1], [1, 1, 1], [1, 0, 0]]\n",
    "])\n",
    "Q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9120b4df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.495215Z",
     "iopub.status.busy": "2022-06-23T18:43:14.494171Z",
     "iopub.status.idle": "2022-06-23T18:43:14.500573Z",
     "shell.execute_reply": "2022-06-23T18:43:14.500573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5, 6, 7])\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde8ed32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.504257Z",
     "iopub.status.busy": "2022-06-23T18:43:14.503251Z",
     "iopub.status.idle": "2022-06-23T18:43:14.522308Z",
     "shell.execute_reply": "2022-06-23T18:43:14.520274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.matmul(Q, x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b52092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.533274Z",
     "iopub.status.busy": "2022-06-23T18:43:14.531328Z",
     "iopub.status.idle": "2022-06-23T18:43:14.553029Z",
     "shell.execute_reply": "2022-06-23T18:43:14.552044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  6, 12, 18],\n",
       "        [ 7, 13, 18,  5]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f43d3",
   "metadata": {},
   "source": [
    "## Our first neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a21dce",
   "metadata": {},
   "source": [
    "Armed with the concepts we just learned, we can now construct our first very simple neural network. \n",
    "\n",
    "We will attempt to fit a toy classification dataset. Recall from chapter 1 that the neural network *fits* the data if it labels all points from a dataset (more or less) correctly. Note that will not worry about concepts like validation or testing in this chapter (and *only* in this chapter). The only thing we *will* worry about is the training (fitting) process.\n",
    "\n",
    "Let's create a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbae8f15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.566931Z",
     "iopub.status.busy": "2022-06-23T18:43:14.565929Z",
     "iopub.status.idle": "2022-06-23T18:43:14.582549Z",
     "shell.execute_reply": "2022-06-23T18:43:14.581551Z"
    }
   },
   "outputs": [],
   "source": [
    "x1s = [torch.tensor(0.0), torch.tensor(0.0), torch.tensor(1.0), torch.tensor(1.0)]\n",
    "x2s = [torch.tensor(0.0), torch.tensor(1.0), torch.tensor(0.0), torch.tensor(1.0)]\n",
    "ts = [torch.tensor(0), torch.tensor(0), torch.tensor(0), torch.tensor(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88e6cf",
   "metadata": {},
   "source": [
    "If you are familiar with basic logic, this is just the truth table of the *AND* function:\n",
    "\n",
    "Of course we can fit such a trivial dataset without the fanciness of deep learning. Nevertheless this is a very nice example that will allow us to show a bunch of important concepts.\n",
    "\n",
    "Let's think about the simplest meaningful computational graph we could construct here. This would obviously be some kind of affine function, i.e. something like $x_1 \\cdot w_1 + x_2 \\cdot w_2 + b$. However the output of an affine function can be arbitrary, but our classes can only take the values 0 and 1 (i.e. we are dealing with binary classification here). We would therefore like to squash the outputs of our function to the range $[0, 1]$. Then we can interpret the squashed value as the probability of the class represented by 1. For example if the squashed output is $0.7$, then with probability $0.7$ the values are of class 1. This has the added benefit that we can assign a *confidence* to our predictions. If the probability is very high (or very low), we have more confidence regarding our prediction than if the probability is e.g. $0.5$.\n",
    "\n",
    "A commonly used squashing function is the so called *sigmoid* function defined by $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96287cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:14.591548Z",
     "iopub.status.busy": "2022-06-23T18:43:14.590593Z",
     "iopub.status.idle": "2022-06-23T18:43:15.224727Z",
     "shell.execute_reply": "2022-06-23T18:43:15.223727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17f92c60370>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sigmoid_xs = np.arange(-10, 10, 0.01)\n",
    "sigmoid_ys = 1 / (1 + np.exp(-sigmoid_xs))\n",
    "plt.plot(sigmoid_xs, sigmoid_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72f171",
   "metadata": {},
   "source": [
    "We can see the very small values essentially become 0, very large values essentially become 1 and everything in between is mapped to an appropriate probability value (with $\\sigma(0) = 0.5$). This looks like a function that accomplishes what we intended.\n",
    "\n",
    "Therefore we define our first neural network by *applying a sigmoid function to an affine function*. Yes, that's really all we are going to do! Note that while this is the simplest meaningful setup for a classification task, it is still a very *real* setup that is used in practice for certain tasks. \n",
    "\n",
    "Formally speaking, our network is defined as $y = f(x_1, x_2) = \\sigma(x_1 \\cdot w_1 + x_2 \\cdot w_2 + b)$.\n",
    "\n",
    "Let us represent this function using a computational graph.\n",
    "\n",
    "First we split $f$ into its respective computations:\n",
    "\n",
    "$h_1 = x_1 \\cdot w_1$\n",
    "\n",
    "$h_2 = x_2 \\cdot w_2$\n",
    "\n",
    "$h = h_1 + h_2$\n",
    "\n",
    "$z = h + b$\n",
    "\n",
    "$y = \\sigma(z)$\n",
    "\n",
    "Let's explicitly draw the graph:\n",
    "\n",
    "<img src=\"images/simple_net.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Note that this computational graph is indeed *differentiable* since every function has a derivative.\n",
    "\n",
    "Recall that the basic idea behind the training of (any) machine learning model is quite simple. We start with randomly initialized parameters (we will discuss initialization schemes later). Then we iterate through all the examples and update the parameters in such a way that we improve our performance on those examples. We often do multiple iterations on the dataset. Each iteration is called an **epoch**. In its simplest form, the training loop therefore looks like this:\n",
    "\n",
    "```python\n",
    "model = NeuralNetwork()\n",
    "for epoch in range(epochs):\n",
    "    for x, t in dataset:\n",
    "        model.update(x, t)\n",
    "```\n",
    "\n",
    "Note that in practice this is a bit different, but we will talk about this later.\n",
    "\n",
    "The big question we will discuss in this chapter is how the update should be performed. Intuitively, we would like to update the model parameters in such a way that the network \"performs\" a bit better on the example $x$ labeled with the target value $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89c97c",
   "metadata": {},
   "source": [
    "## Updating our parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7bd8a",
   "metadata": {},
   "source": [
    "How could we accomplish that? First of all we need to define a **loss function** which is the measure of how far the output of the network is from the correct target. We then simply update our parameters in such a way that the *loss decreases*. Put differently, we want to *minimize the loss*.\n",
    "\n",
    "In this particular case we will use a loss function called **binary crossentropy** which is defined as $BCE(y, t) = -t \\cdot log(y) - (1 - t) \\cdot log(1 - y)$. Here $y$ is the value produced by our network and $t$ is the target value.\n",
    "\n",
    "Binary crossentropy is a concept deep learning shamelessly stole (a.k.a. *borrowed*) from information theory, but we will not go into all that and instead motivate it with very simple intuition. Basically if $t = 1$, then we want the loss to be proportional to $-\\log(y)$. This is because a lower $y$ in our interpretation means a higher probability of $t = 0$ which is bad if in reality $t = 1$. Similarly, if $t = 0$, we want the loss to be proportional to $-\\log(1 - y)$. Now we simply combine this into one expression which gets us the formula from above.\n",
    "\n",
    "How does the loss fit into the computational graph? Even deep learning practicioners sometimes get confused about that, but the simple truth is that the loss is just another node put on top of the network:\n",
    "\n",
    "<img src=\"images/simple_net_loss.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Let us take our simple neural net and set $w_1 = w_2 = 0.5$, $b = 0$. A quick calculation yields an output value of $y = 0.6225$ for $x_1 = 0$, $x_2 = 1$. We see that the output value is not quite right. It should be $t = 0$ and not $y = 0.6225$. Therefore we want to update the parameters of the network (i.e. $w_1, w_2$ and $b$) in such a way that the output $y$ becomes a bit closer to the target $t$.\n",
    "\n",
    "What this really means is that we have to update our weights in such a way that the loss (let us call it $L$) becomes smaller. How could we approach this?\n",
    "\n",
    "If you have some basic knowledge of optimization theory, you are probably smirking right now. This is what gradient descent (and a bunch of other optimization algorithms) are for! However even if you do not know any numerical optimization, you can still easily develop an idea for what we should do.\n",
    "\n",
    "Remember from high school that the derivative $\\frac{\\partial L}{\\partial b}$ tells you the way $L$ changes if we change $b$. For example if $\\frac{\\partial L}{\\partial b} > 0$ then increasing $b$ would lead to an increase in $L$. If $\\frac{\\partial L}{\\partial b} < 0$ then an increase in $b$ would lead to a decrease in $L$.\n",
    "\n",
    "Now assume that we somehow get our hands on $\\frac{\\partial L}{\\partial b}$. We want to change $b$ in such a way that $L$ decreases. After all, we want to *minimize* the loss. Therefore we should update $b$ by a value proportionate to $-\\frac{\\partial L}{\\partial b}$. Put differently, we want to update $b$ by $-\\alpha \\cdot \\frac{\\partial L}{\\partial b}$ where $\\alpha$ is some parameter that controls how big our update should be. Formally the update step therefore looks as follows:\n",
    "\n",
    "$b^{(t+1)} = b^{(t)} - \\alpha \\cdot \\frac{\\partial L}{\\partial b^{(t)}}$\n",
    "\n",
    "That is, at step $t$ we update $b$ by subtracting $\\alpha \\cdot \\frac{\\partial L}{\\partial b^{(t)}}$ from it.\n",
    "\n",
    "A similar logic applies to the other parameters, i.e.\n",
    "\n",
    "$w_1^{(t+1)} = w_1^{(t)} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_1^{(t)}}$\n",
    "\n",
    "$w_2^{(t+1)} = w_2^{(t)} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_2^{(t)}}$\n",
    "\n",
    "After such an update the loss would become smaller. But how could we could calculate the derivative $\\frac{\\partial L}{\\partial b}$? Calculating e.g. the derivative $\\frac{\\partial L}{\\partial y}$ would be easy - we would simply need to take the derivative of the loss function. But $b$ does not influence the loss directly, it influences the loss through $y$.\n",
    "\n",
    "Despair not, we can use the *chain rule*. We know that $\\frac{\\partial L}{\\partial b}$ can be calculated as $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial b}$. But now we have the same problem. Again $b$ is not a direct input to $y$. How do we get $\\frac{\\partial y}{\\partial b}$?\n",
    "\n",
    "You can probably already see where this is going. We see that $y$ depends on $b$ through $z$. By the chain rule we have $\\frac{\\partial y}{\\partial b} = \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial b}$.\n",
    "\n",
    "We therefore have: $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial b}$. Now we only need to calculate the individual derivatives using our knowledge of basic differentiation and we are all set.\n",
    "\n",
    "1. We begin with $\\frac{\\partial L}{\\partial y}$. Since $L = -t \\cdot log(y) - (1 - t) \\cdot log(1 - y)$ we have $\\frac{\\partial L}{\\partial y} = \\frac{y - t}{(1 - y)y}$.\n",
    "\n",
    "2. Similarly we get $\\frac{\\partial y}{\\partial z} = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$.\n",
    "\n",
    "3. Finally we have $\\frac{\\partial z}{\\partial b} = 1$.\n",
    "\n",
    "Therefore $\\frac{\\partial L}{\\partial b} = \\frac{y - t}{(1 - y)y} \\cdot \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$\n",
    "\n",
    "The same logic holds for $w_1$ and $w_2$. Here we have $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial h} \\frac{\\partial h}{\\partial h_1} \\frac{\\partial h_1}{\\partial w_1}$\n",
    "\n",
    "Since $\\frac{\\partial L}{\\partial y} = -\\frac{t}{y} + \\frac{1 - t}{1 - y}$, $\\frac{\\partial y}{\\partial z} = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$, $\\frac{\\partial z}{\\partial h} = 1$, $\\frac{\\partial h}{\\partial h_1} = 1$ and $\\frac{\\partial h_1}{\\partial w_1} = x_1$ we have $\\frac{\\partial L}{\\partial w_1} = \\frac{y - t}{(1 - y)y} \\cdot \\frac{\\exp(-z)}{(1 + \\exp(-z))^2} \\cdot x_1$.\n",
    "\n",
    "You *should* verify all of this. It's a nice and simple, but *very relevant* exercise in calculating derivatives.\n",
    "\n",
    "In a very similar fashion $\\frac{\\partial L}{\\partial w_2} = \\frac{y - t}{(1 - y)y} \\cdot \\frac{\\exp(-z)}{(1 + \\exp(-z))^2} \\cdot x_2$\n",
    "\n",
    "Let's write all of this down in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a29cb087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.236760Z",
     "iopub.status.busy": "2022-06-23T18:43:15.235760Z",
     "iopub.status.idle": "2022-06-23T18:43:15.257728Z",
     "shell.execute_reply": "2022-06-23T18:43:15.255731Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w1 = 0.5\n",
    "        self.w2 = 0.5\n",
    "        self.b = 0.0\n",
    "        \n",
    "        self.alpha = 1.0\n",
    "        \n",
    "    def forward(self, x1, x2, t):\n",
    "        h1 = x1 * self.w1\n",
    "        h2 = x2 * self.w2\n",
    "        h = h1 + h2\n",
    "        z = h + self.b\n",
    "        y = 1 / (1 + exp(-z))\n",
    "        L = -t * log(y) - (1 - t) * log(1 - y)\n",
    "        return { \"x1\": x1, \"x2\": x2, \"h1\": h1, \"h2\": h2, \"h\": h, \"z\": z, \"y\": y, \"L\": L }\n",
    "        \n",
    "    def update(self, x1, x2, t):\n",
    "        # Forward pass\n",
    "        forward_res = self.forward(x1, x2, t)\n",
    "        z, y = forward_res[\"z\"], forward_res[\"y\"]\n",
    "        \n",
    "        # Derivatives\n",
    "        dLdy = (y - t) / ((1 - y) * y)\n",
    "        dydz = exp(-z) / (1 + exp(-z)) ** 2\n",
    "        dLdw1 = dLdy * dydz * x1\n",
    "        dLdw2 = dLdy * dydz * x2\n",
    "        dLdb = dLdy * dydz\n",
    "        \n",
    "        # Update\n",
    "        self.w1 -= self.alpha * dLdw1\n",
    "        self.w2 -= self.alpha * dLdw2\n",
    "        self.b -= self.alpha * dLdb\n",
    "        return { \"dLdw1\": dLdw1, \"dLdw2\": dLdw2, \"dLdb\": dLdb }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92593a58",
   "metadata": {},
   "source": [
    "While this is pretty long, it's really just our above formulas written in Python.\n",
    "\n",
    "Let's take the first example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe2bd460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.267769Z",
     "iopub.status.busy": "2022-06-23T18:43:15.265780Z",
     "iopub.status.idle": "2022-06-23T18:43:15.272533Z",
     "shell.execute_reply": "2022-06-23T18:43:15.271520Z"
    }
   },
   "outputs": [],
   "source": [
    "x1, x2, t = 0.0, 0.0, 0.0\n",
    "\n",
    "net = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47723be9",
   "metadata": {},
   "source": [
    "We run a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97400598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.281489Z",
     "iopub.status.busy": "2022-06-23T18:43:15.280497Z",
     "iopub.status.idle": "2022-06-23T18:43:15.287495Z",
     "shell.execute_reply": "2022-06-23T18:43:15.286523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': 0.0,\n",
       " 'x2': 0.0,\n",
       " 'h1': 0.0,\n",
       " 'h2': 0.0,\n",
       " 'h': 0.0,\n",
       " 'z': 0.0,\n",
       " 'y': 0.5,\n",
       " 'L': 0.6931471805599453}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb06c4",
   "metadata": {},
   "source": [
    "Now we execute an update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d3216a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.295490Z",
     "iopub.status.busy": "2022-06-23T18:43:15.294490Z",
     "iopub.status.idle": "2022-06-23T18:43:15.303489Z",
     "shell.execute_reply": "2022-06-23T18:43:15.302489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dLdw1': 0.0, 'dLdw2': 0.0, 'dLdb': 0.5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.update(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb59e42",
   "metadata": {},
   "source": [
    "We can see that $w_1$, $w_2$ and $b$ changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b9aa35c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.311492Z",
     "iopub.status.busy": "2022-06-23T18:43:15.310537Z",
     "iopub.status.idle": "2022-06-23T18:43:15.318538Z",
     "shell.execute_reply": "2022-06-23T18:43:15.317501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, -0.5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.w1, net.w2, net.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4561d",
   "metadata": {},
   "source": [
    "But did that change make sense? We could check this by running a forward pass and seeing if the loss decreased, i.e. if $y$ is closer to $t$ now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc9aa5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.323499Z",
     "iopub.status.busy": "2022-06-23T18:43:15.323499Z",
     "iopub.status.idle": "2022-06-23T18:43:15.333727Z",
     "shell.execute_reply": "2022-06-23T18:43:15.332744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': 0.0,\n",
       " 'x2': 0.0,\n",
       " 'h1': 0.0,\n",
       " 'h2': 0.0,\n",
       " 'h': 0.0,\n",
       " 'z': -0.5,\n",
       " 'y': 0.3775406687981454,\n",
       " 'L': 0.47407698418010663}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8796634",
   "metadata": {},
   "source": [
    "This looks good! Now all we have to do is to repeatedly iterate over the dataset and do updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb7df73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.336689Z",
     "iopub.status.busy": "2022-06-23T18:43:15.336689Z",
     "iopub.status.idle": "2022-06-23T18:43:15.393607Z",
     "shell.execute_reply": "2022-06-23T18:43:15.393607Z"
    }
   },
   "outputs": [],
   "source": [
    "net = SimpleNet()\n",
    "for epoch in range(100):\n",
    "    for i in range(4):\n",
    "        x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "        net.update(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433de47",
   "metadata": {},
   "source": [
    "That's it! We successfully trained our first neural network. If you understood the preceding section, you've come a *tremendously long* way to understanding how neural networks operate. The rest of this chapter is really just about coming up with a more efficient algorithm for doing this (as calculating all the derivatives by hand is very tedious).\n",
    "\n",
    "Before we move on, let us verify that we indeed get useful predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3184afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.396636Z",
     "iopub.status.busy": "2022-06-23T18:43:15.395640Z",
     "iopub.status.idle": "2022-06-23T18:43:15.413955Z",
     "shell.execute_reply": "2022-06-23T18:43:15.411954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': tensor(0.), 'x2': tensor(0.), 'h1': tensor(0.), 'h2': tensor(0.), 'h': tensor(0.), 'z': tensor(-8.9435), 'y': 0.00013056159878900093, 'L': tensor(0.0001)}\n",
      "{'x1': tensor(0.), 'x2': tensor(1.), 'h1': tensor(0.), 'h2': tensor(5.8847), 'h': tensor(5.8847), 'z': tensor(-3.0589), 'y': 0.04483687464928194, 'L': tensor(0.0459)}\n",
      "{'x1': tensor(1.), 'x2': tensor(0.), 'h1': tensor(5.9416), 'h2': tensor(0.), 'h': tensor(5.9416), 'z': tensor(-3.0019), 'y': 0.0473383390833865, 'L': tensor(0.0485)}\n",
      "{'x1': tensor(1.), 'x2': tensor(1.), 'h1': tensor(5.9416), 'h2': tensor(5.8847), 'h': tensor(11.8263), 'z': tensor(2.8827), 'y': 0.9469867809360202, 'L': tensor(0.0545)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    result = net.forward(x1s[i], x2s[i], ts[i])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10e047",
   "metadata": {},
   "source": [
    "Incredible! We can see that loss has become very small for every point. Let us have a look at the learned parameters and try to gain an intuition for what the network learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85defcfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.423910Z",
     "iopub.status.busy": "2022-06-23T18:43:15.422911Z",
     "iopub.status.idle": "2022-06-23T18:43:15.443003Z",
     "shell.execute_reply": "2022-06-23T18:43:15.440963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.9416), tensor(5.8847), tensor(-8.9435))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.w1, net.w2, net.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fcdf5",
   "metadata": {},
   "source": [
    "Basically we learned the function $y = f(x_1, x_2) = \\sigma(5.94 \\cdot x_1 + 5.88 \\cdot x_2 - 8.94)$. This makes sense. Essentially the network said \"$x_1$ and $x_2$ both need to make a contribution to produce a large value, otherwise the bias will pull the total value down\", which is just another description for the *AND* function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51f73f",
   "metadata": {},
   "source": [
    "## Improving the parameter update with backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3153f2",
   "metadata": {},
   "source": [
    "Manual updates work, but involve calculating lots of derivatives. This would mean that for every new model we would need to recalculate all the derivatives by hand. This is annoying. We want to be able to do this more effectively. \n",
    "\n",
    "We should more closely examine the flow of derivatives through the network:\n",
    "\n",
    "<img src=\"images/backprop.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "As you can see the derivatives flow *backwards* (hence the name **backpropagation**). \n",
    "\n",
    "Let us look at a node up close. We take the sigmoid node. The derivative that flows out ($\\frac{\\partial L}{\\partial z}$) only depends on the derivative that flows in ($\\frac{\\partial L}{\\partial y}$) and the *local derivative* $\\frac{\\partial y}{\\partial z}$. After all the chain rule tells us that $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z}$.\n",
    "\n",
    "The same holds true for all the other nodes! This is such an important point that I even put an *exclamation mark* behind it. *Given the derivative flowing in each node can calculate the derivative flowing out.* Instead of calculating the derivatives for every network from scratch, we specify a bunch of nodes. Every node has a *forward* function which takes some inputs and produces some outputs and a *backward* function which takes the derivative flowing in and produces the derivative flowing out. We can then execute approximately the following **backpropagation** algorithm :\n",
    "\n",
    "```python\n",
    "forward_sorted_nodes = sort_forward(nodes)\n",
    "for node in forward_sorted_nodes:\n",
    "    incoming_nodes = get_incoming_nodes(node)\n",
    "    node.output_values = node.calc_output_values(incoming_nodes)\n",
    "    \n",
    "backward_sorted_nodes = sort_backward(nodes)\n",
    "for node in backward_sorted_nodes:\n",
    "    outgoing_node = get_outgoing_node(node)\n",
    "    local_grad = node.get_local_grad()\n",
    "    node.grad = local_grad * outgoing_node.grad\n",
    "```\n",
    "\n",
    "Let us now implement a bunch of nodes in pytorch. Note that we usually do not have to do that (the pytorch authors already implemented most of the important nodes). Nevertheless this is an extremely useful exercise that will teach you a great deal about how neural networks *really work*.\n",
    "\n",
    "We begin by implementing the addition node. The forward function would simply be $z = x + y$. For the backward function we would need to return two gradients - one for $x$ and one for $y$. They are pretty simple. We see that $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}$. Since $\\frac{\\partial z}{\\partial x} = 1$ we have $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}$. Similarly $\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial z}$. Let us put this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69661310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.452005Z",
     "iopub.status.busy": "2022-06-23T18:43:15.451007Z",
     "iopub.status.idle": "2022-06-23T18:43:15.458018Z",
     "shell.execute_reply": "2022-06-23T18:43:15.457003Z"
    }
   },
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        return x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b754d5",
   "metadata": {},
   "source": [
    "Now we turn to multiplication. The forward function is $z = x \\cdot y$. The backward pass again needs to calculate $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}$. However now the local gradients are different. We have $\\frac{\\partial z}{\\partial x} = y$ and $\\frac{\\partial z}{\\partial y} = x$. This means that the backward pass is dependent on the forward pass. This is no problem however, as we can store the values from the forward pass using the ctx object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53a40664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.467005Z",
     "iopub.status.busy": "2022-06-23T18:43:15.465964Z",
     "iopub.status.idle": "2022-06-23T18:43:15.473006Z",
     "shell.execute_reply": "2022-06-23T18:43:15.472002Z"
    }
   },
   "outputs": [],
   "source": [
    "class Mul(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        ctx.x, ctx.y = x, y\n",
    "        return x * y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, y = ctx.x, ctx.y\n",
    "        return grad_output * y, grad_output * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b0866",
   "metadata": {},
   "source": [
    "The sigmoid function is very similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "746b0607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.479996Z",
     "iopub.status.busy": "2022-06-23T18:43:15.478997Z",
     "iopub.status.idle": "2022-06-23T18:43:15.490116Z",
     "shell.execute_reply": "2022-06-23T18:43:15.492117Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.x = x\n",
    "        return torch.tensor(1 / (1 + exp(-x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.x\n",
    "        \n",
    "        # Note that here we need to convert the local gradient to a tensor\n",
    "        grad_local = torch.tensor(exp(-x) / (1 + exp(-x)) ** 2)\n",
    "        return grad_output * grad_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d84ac8",
   "metadata": {},
   "source": [
    "Finally this is how the node for the BCE loss would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd5d78ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.507102Z",
     "iopub.status.busy": "2022-06-23T18:43:15.506057Z",
     "iopub.status.idle": "2022-06-23T18:43:15.519395Z",
     "shell.execute_reply": "2022-06-23T18:43:15.518380Z"
    }
   },
   "outputs": [],
   "source": [
    "class BCELoss(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, y, t):\n",
    "        ctx.y, ctx.t = y, t          \n",
    "        return -t * log(y) - (1 - t) * log(1 - y)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y, t = ctx.y, ctx.t\n",
    "        y_grad_local = (y - t) / ((1 - y) * y)\n",
    "        t_grad_local = -log(y) + log(1 - y)\n",
    "        return grad_output * y_grad_local, grad_output * t_grad_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c3e67",
   "metadata": {},
   "source": [
    "Let us sanity check this. We pass the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b74c9d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.526398Z",
     "iopub.status.busy": "2022-06-23T18:43:15.526398Z",
     "iopub.status.idle": "2022-06-23T18:43:15.534683Z",
     "shell.execute_reply": "2022-06-23T18:43:15.533683Z"
    }
   },
   "outputs": [],
   "source": [
    "x1 = torch.tensor(0.0)\n",
    "x2 = torch.tensor(0.0)\n",
    "w1 = torch.tensor(0.5, requires_grad=True)\n",
    "w2 = torch.tensor(0.5, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253c2b8",
   "metadata": {},
   "source": [
    "Now we execute the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9358a2a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.543640Z",
     "iopub.status.busy": "2022-06-23T18:43:15.542639Z",
     "iopub.status.idle": "2022-06-23T18:43:15.565406Z",
     "shell.execute_reply": "2022-06-23T18:43:15.564405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h1': tensor(0., grad_fn=<MulBackward>),\n",
       " 'h2': tensor(0., grad_fn=<MulBackward>),\n",
       " 'h': tensor(0., grad_fn=<AddBackward>),\n",
       " 'z': tensor(0., grad_fn=<AddBackward>),\n",
       " 'y': tensor(0.5000, grad_fn=<SigmoidBackward>),\n",
       " 'L': tensor(0.6931, grad_fn=<BCELossBackward>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = Mul.apply(w1, x1)\n",
    "h2 = Mul.apply(w2, x2)\n",
    "h = Add.apply(h1, h2)\n",
    "z = Add.apply(h, b)\n",
    "y = Sigmoid.apply(z)\n",
    "t = torch.tensor(0.0)\n",
    "L = BCELoss.apply(y, t)\n",
    "{ \"h1\": h1, \"h2\": h2, \"h\": h, \"z\": z, \"y\": y, \"L\": L }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e6610",
   "metadata": {},
   "source": [
    "Looks good so far. Now we call the backward function on the loss node. Pytorch will automatically calculate the gradients using the algorithm from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c9189a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.569441Z",
     "iopub.status.busy": "2022-06-23T18:43:15.568405Z",
     "iopub.status.idle": "2022-06-23T18:43:15.580444Z",
     "shell.execute_reply": "2022-06-23T18:43:15.580444Z"
    }
   },
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "befe78b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.585444Z",
     "iopub.status.busy": "2022-06-23T18:43:15.585444Z",
     "iopub.status.idle": "2022-06-23T18:43:15.596648Z",
     "shell.execute_reply": "2022-06-23T18:43:15.595611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.), tensor(0.5000))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad, w2.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98548788",
   "metadata": {},
   "source": [
    "This is the same as the result we got in the manual version from above. Let us define the forward and update functions, but now using our nodes instead of manual derivative calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42df180e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.606645Z",
     "iopub.status.busy": "2022-06-23T18:43:15.605614Z",
     "iopub.status.idle": "2022-06-23T18:43:15.610945Z",
     "shell.execute_reply": "2022-06-23T18:43:15.610945Z"
    }
   },
   "outputs": [],
   "source": [
    "class AutoSimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w1 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.w2 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.b = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        self.alpha = 1.0\n",
    "        \n",
    "    def forward(self, x1, x2, t):\n",
    "        h1 = Mul.apply(self.w1, x1)\n",
    "        h2 = Mul.apply(self.w2, x2)\n",
    "        h = Add.apply(h1, h2)\n",
    "        z = Add.apply(h, self.b)\n",
    "        y = Sigmoid.apply(z)\n",
    "        L = BCELoss.apply(y, t)\n",
    "        return { \"x1\": x1, \"x2\": x2, \"h1\": h1, \"h2\": h2, \"h\": h, \"z\": z, \"y\": y, \"L\": L }\n",
    "        \n",
    "    def update(self, x1, x2, t):\n",
    "        # Forward pass\n",
    "        forward_res = self.forward(x1, x2, t)\n",
    "        L = forward_res[\"L\"]\n",
    "        \n",
    "        # Backward pass\n",
    "        L.backward()\n",
    "        \n",
    "        # Update\n",
    "        with torch.no_grad():\n",
    "            dLdw1 = float(self.w1.grad)\n",
    "            dLdw2 = float(self.w2.grad)\n",
    "            dLdb = float(self.b.grad)\n",
    "            self.w1 -= self.alpha * dLdw1\n",
    "            self.w2 -= self.alpha * dLdw2\n",
    "            self.b -= self.alpha * dLdb\n",
    "            \n",
    "            self.w1.grad.zero_()\n",
    "            self.w2.grad.zero_()\n",
    "            self.b.grad.zero_()\n",
    "        return forward_res, { \"dLdw1\": dLdw1, \"dLdw2\": dLdw2, \"dLdb\": dLdb }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f2f60",
   "metadata": {},
   "source": [
    "The training loop doesn't change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "858f9282",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.616945Z",
     "iopub.status.busy": "2022-06-23T18:43:15.615956Z",
     "iopub.status.idle": "2022-06-23T18:43:15.732549Z",
     "shell.execute_reply": "2022-06-23T18:43:15.732549Z"
    }
   },
   "outputs": [],
   "source": [
    "net = AutoSimpleNet()\n",
    "for epoch in range(100):\n",
    "    for i in range(4):\n",
    "        x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "        net.update(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05de96",
   "metadata": {},
   "source": [
    "And now for the moment of truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f216c15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.736550Z",
     "iopub.status.busy": "2022-06-23T18:43:15.735581Z",
     "iopub.status.idle": "2022-06-23T18:43:15.747583Z",
     "shell.execute_reply": "2022-06-23T18:43:15.747583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': tensor(0.), 'x2': tensor(0.), 'h1': tensor(0., grad_fn=<MulBackward>), 'h2': tensor(0., grad_fn=<MulBackward>), 'h': tensor(0., grad_fn=<AddBackward>), 'z': tensor(-8.9435, grad_fn=<AddBackward>), 'y': tensor(0.0001, grad_fn=<SigmoidBackward>), 'L': tensor(0.0001, grad_fn=<BCELossBackward>)}\n",
      "{'x1': tensor(0.), 'x2': tensor(1.), 'h1': tensor(0., grad_fn=<MulBackward>), 'h2': tensor(5.8847, grad_fn=<MulBackward>), 'h': tensor(5.8847, grad_fn=<AddBackward>), 'z': tensor(-3.0589, grad_fn=<AddBackward>), 'y': tensor(0.0448, grad_fn=<SigmoidBackward>), 'L': tensor(0.0459, grad_fn=<BCELossBackward>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(0.), 'h1': tensor(5.9416, grad_fn=<MulBackward>), 'h2': tensor(0., grad_fn=<MulBackward>), 'h': tensor(5.9416, grad_fn=<AddBackward>), 'z': tensor(-3.0019, grad_fn=<AddBackward>), 'y': tensor(0.0473, grad_fn=<SigmoidBackward>), 'L': tensor(0.0485, grad_fn=<BCELossBackward>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(1.), 'h1': tensor(5.9416, grad_fn=<MulBackward>), 'h2': tensor(5.8847, grad_fn=<MulBackward>), 'h': tensor(11.8263, grad_fn=<AddBackward>), 'z': tensor(2.8827, grad_fn=<AddBackward>), 'y': tensor(0.9470, grad_fn=<SigmoidBackward>), 'L': tensor(0.0545, grad_fn=<BCELossBackward>)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    result = net.forward(x1s[i], x2s[i], ts[i])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1182a1",
   "metadata": {},
   "source": [
    "That looks great! Let us perform an additional sanity check by confirming that $w_1$, $w_2$ and $b$ have the same values as in the manual version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "651ce3fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.750596Z",
     "iopub.status.busy": "2022-06-23T18:43:15.750596Z",
     "iopub.status.idle": "2022-06-23T18:43:15.763549Z",
     "shell.execute_reply": "2022-06-23T18:43:15.763549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.9416, requires_grad=True),\n",
       " tensor(5.8847, requires_grad=True),\n",
       " tensor(-8.9435, requires_grad=True))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.w1, net.w2, net.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2718807",
   "metadata": {},
   "source": [
    "Incredibly enough, there is not much more than this to backpropagation. We will now discuss a bunch of (important) technical details, but you can already go and impress everyone with your newfound knowledge! Go ahead and do just that - you now understand the algorithm which underpins basically all of deep learning - from the simplest perceptrons to the fanciest state of the art models out there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a9407",
   "metadata": {},
   "source": [
    "## Backpropagation in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf8803",
   "metadata": {},
   "source": [
    "Of course in reality we do not actually go around implementing all the backward functions from scratch. The pytorch team has already done that for *a lot* of functions. We can therefore simply declare the computational graph using those builtin functions, call `backward` and then do the gradient updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e878c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.769549Z",
     "iopub.status.busy": "2022-06-23T18:43:15.769549Z",
     "iopub.status.idle": "2022-06-23T18:43:15.779548Z",
     "shell.execute_reply": "2022-06-23T18:43:15.778581Z"
    }
   },
   "outputs": [],
   "source": [
    "class TorchSimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w1 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.w2 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.b = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        self.loss_fun = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, x1, x2, t):\n",
    "        h1 = torch.mul(self.w1, x1)\n",
    "        h2 = torch.mul(self.w2, x2)\n",
    "        h = torch.add(h1, h2)\n",
    "        z = torch.add(h, self.b)\n",
    "        y = torch.sigmoid(z)\n",
    "        z.retain_grad()\n",
    "        y.retain_grad()\n",
    "        \n",
    "        L = self.loss_fun(y, t.float())\n",
    "        return { \"x1\": x1, \"x2\": x2, \"h1\": h1, \"h2\": h2, \"h\": h, \"z\": z, \"y\": y, \"t\": t, \"L\": L }\n",
    "\n",
    "    def update(self, x1, x2, t):\n",
    "        forward_res = self.forward(x1, x2, t)\n",
    "        L = forward_res[\"L\"]\n",
    "        \n",
    "        # Backward\n",
    "        L.backward()\n",
    "        \n",
    "        # Update\n",
    "        with torch.no_grad():\n",
    "            dLdw1 = float(self.w1.grad)\n",
    "            dLdw2 = float(self.w2.grad)\n",
    "            dLdb = float(self.b.grad)\n",
    "            self.w1 -= self.alpha * dLdw1\n",
    "            self.w2 -= self.alpha * dLdw2\n",
    "            self.b -= self.alpha * dLdb\n",
    "            \n",
    "            self.w1.grad.zero_()\n",
    "            self.w2.grad.zero_()\n",
    "            self.b.grad.zero_()\n",
    "        \n",
    "        return forward_res, { \"dLdw1\": dLdw1, \"dLdw2\": dLdw2, \"dLdb\": dLdb }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635fb3f",
   "metadata": {},
   "source": [
    "Again the training loop doesn't change at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84379246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.782583Z",
     "iopub.status.busy": "2022-06-23T18:43:15.781612Z",
     "iopub.status.idle": "2022-06-23T18:43:15.855591Z",
     "shell.execute_reply": "2022-06-23T18:43:15.855591Z"
    }
   },
   "outputs": [],
   "source": [
    "net = TorchSimpleNet()\n",
    "for epoch in range(100):\n",
    "    for i in range(4):\n",
    "        x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "        net.update(x1, x2, t) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4495e0",
   "metadata": {},
   "source": [
    "We do the forward passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d667fb31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.858582Z",
     "iopub.status.busy": "2022-06-23T18:43:15.858582Z",
     "iopub.status.idle": "2022-06-23T18:43:15.870596Z",
     "shell.execute_reply": "2022-06-23T18:43:15.870596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': tensor(0.), 'x2': tensor(0.), 'h1': tensor(0., grad_fn=<MulBackward0>), 'h2': tensor(0., grad_fn=<MulBackward0>), 'h': tensor(0., grad_fn=<AddBackward0>), 'z': tensor(-8.9435, grad_fn=<AddBackward0>), 'y': tensor(0.0001, grad_fn=<SigmoidBackward0>), 't': tensor(0), 'L': tensor(0.0001, grad_fn=<BinaryCrossEntropyBackward0>)}\n",
      "{'x1': tensor(0.), 'x2': tensor(1.), 'h1': tensor(0., grad_fn=<MulBackward0>), 'h2': tensor(5.8847, grad_fn=<MulBackward0>), 'h': tensor(5.8847, grad_fn=<AddBackward0>), 'z': tensor(-3.0589, grad_fn=<AddBackward0>), 'y': tensor(0.0448, grad_fn=<SigmoidBackward0>), 't': tensor(0), 'L': tensor(0.0459, grad_fn=<BinaryCrossEntropyBackward0>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(0.), 'h1': tensor(5.9416, grad_fn=<MulBackward0>), 'h2': tensor(0., grad_fn=<MulBackward0>), 'h': tensor(5.9416, grad_fn=<AddBackward0>), 'z': tensor(-3.0019, grad_fn=<AddBackward0>), 'y': tensor(0.0473, grad_fn=<SigmoidBackward0>), 't': tensor(0), 'L': tensor(0.0485, grad_fn=<BinaryCrossEntropyBackward0>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(1.), 'h1': tensor(5.9416, grad_fn=<MulBackward0>), 'h2': tensor(5.8847, grad_fn=<MulBackward0>), 'h': tensor(11.8263, grad_fn=<AddBackward0>), 'z': tensor(2.8827, grad_fn=<AddBackward0>), 'y': tensor(0.9470, grad_fn=<SigmoidBackward0>), 't': tensor(1), 'L': tensor(0.0545, grad_fn=<BinaryCrossEntropyBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "    print(net.forward(x1, x2, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083124c3",
   "metadata": {},
   "source": [
    "Let us also sanity check the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea56037d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T18:43:15.873562Z",
     "iopub.status.busy": "2022-06-23T18:43:15.873562Z",
     "iopub.status.idle": "2022-06-23T18:43:15.886555Z",
     "shell.execute_reply": "2022-06-23T18:43:15.885577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.9416, requires_grad=True),\n",
       " tensor(5.8847, requires_grad=True),\n",
       " tensor(-8.9435, requires_grad=True))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.w1, net.w2, net.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d06e1",
   "metadata": {},
   "source": [
    "This is a very basic and simple, but *complete* example of a neural network. We can however tweak a bunch of things.\n",
    "\n",
    "First the **architecture** of our network will usually be more complex.\n",
    "\n",
    "Second for different cases we need to use different **loss functions**.\n",
    "\n",
    "Third the update step will often be different as defined by an **optimizer**.\n",
    "\n",
    "Fourth the initilization will be different as defined by an **initialization scheme**.\n",
    "\n",
    "We will tackle these things in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076cb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
