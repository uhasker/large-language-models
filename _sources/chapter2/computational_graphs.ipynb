{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f037d7",
   "metadata": {},
   "source": [
    "# Computational Graphs and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b154759",
   "metadata": {},
   "source": [
    "In this chapter we will introduce the fundamental concepts that underpin all of deep learning - computational graphs and backpropagation.\n",
    "To showcase these ideas we will create and train a neural network from scratch which will solve a *very simple* classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8668de8",
   "metadata": {},
   "source": [
    "If you want to follow along, you will need to execute the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2f875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a28fe0",
   "metadata": {},
   "source": [
    "## Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ad159",
   "metadata": {},
   "source": [
    "The basic concept you need to understand in order to grasp neural networks is that of a **computational graph**.\n",
    "\n",
    "Remember that a *graph* has nodes and edges.\n",
    "In a *computational graph* the *nodes represent functions*.\n",
    "These functions take input values and produce (compute) output values.\n",
    "The *edges carry* these *values* (which are either input values to the graph or results of preceding computations).\n",
    "Thus, computational graphs allow for simple representations of complex functions.\n",
    "\n",
    "Let's respresent an example function $f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}, f(x, y, z) = (x + y) \\cdot z$ using a computational graph.\n",
    "We will write $f = (x + y) \\cdot z$ to simplify notation.\n",
    "\n",
    "This function really consists of two functions.\n",
    "The first function calculates $x + y$. The second function multiplies the result of $x + y$ by $z$.\n",
    "Put more formally, we first compute $g = x + y$ and then calculate $f = g \\cdot z$.\n",
    "\n",
    "This is how the visual representation of the computational graph representing $f$ would look like:\n",
    "\n",
    "<img src=\"images/compgraph.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "> Do note that this is not the most common visualization of computational graphs.\n",
    "> Nevertheless we will stick to it, since it drives home most of the important ideas quite nicely.\n",
    "\n",
    "We can see that the graph has two nodes.\n",
    "The first node represents an addition function.\n",
    "It takes the values $x$ and $y$ and outputs the value $g$.\n",
    "The second node represents a multiplication function.\n",
    "It takes the values $g$ and $z$ and outputs the value $f$.\n",
    "Note that here one of the inputs to the multiplication node (namely $g$) is the output of a previous node (namely the addition node).\n",
    "\n",
    "By decomposing arbitrary functions into simple components we can create computational graphs that represent very complex functions.\n",
    "In fact **neural networks** are nothing more than just certain kinds of computational graphs, namely computational graphs which are *differentiable*.\n",
    "This means that every node function has a gradient (or more technically a subgradient, but this distinction largely doesn't matter right now).\n",
    "If all of this sounds like wizardry, don't worry - we will get into this in a few minutes.\n",
    "\n",
    "We just saw that the edges carry values.\n",
    "But which values do they carry?\n",
    "There aren't many restrictions here.\n",
    "While in the above examples we had numbers, there is nothing that prevents us from passing vectors, matrices or even more complicated objects around.\n",
    "In fact, this is what we will usually do!\n",
    "\n",
    "Consider the function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}, f(\\vec{x}) = \\max(W\\vec{x} + \\vec{b}, \\vec{0})$ where $W \\in \\mathbb{R}^{m \\times n}$, $\\vec{b} \\in \\mathbb{R}^{m}$ and $\\max$ is the elementwise maximum. \n",
    "\n",
    "> The form of this function is no accident.\n",
    "> It represents a (very simple) neural network.\n",
    "\n",
    "We can break this function up as following:\n",
    "\n",
    "$\\vec{c} = W\\vec{x}$\n",
    "\n",
    "$\\vec{d} = \\vec{c} + \\vec{b}$\n",
    "\n",
    "$\\vec{f} = \\max(\\vec{d}, \\vec{0})$\n",
    "\n",
    "The visual representation looks like this:\n",
    "\n",
    "<img src=\"images/compgraph2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Generally, the values that will flow through our computational graph can be _arbitrary tensors_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d388c9",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd6a93",
   "metadata": {},
   "source": [
    "There are different definitions of tensors depending on the mathematical branch you're in.\n",
    "For the purposes of deep learning, a **tensor** is nothing else than a *multidimensional array* (if you've ever used numpy arrays, this is very similar).\n",
    "\n",
    "For example, a zero-dimensional tensor is just a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d477d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "tensor0 = torch.tensor(1)\n",
    "print(tensor0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b8fc2",
   "metadata": {},
   "source": [
    "A one-dimensional tensor is a vector (or an array or a list, depending on what language you like more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152d6a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3, 4])\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8f031",
   "metadata": {},
   "source": [
    "A two-dimensional tensor is a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca168072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "tensor2 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fc51c",
   "metadata": {},
   "source": [
    "You can also construct tensors of three, four or any arbitrary number of dimensions. Here is how a three-dimensional tensor could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e7a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [4, 5]],\n",
      "\n",
      "        [[6, 7],\n",
      "         [8, 9]]])\n"
     ]
    }
   ],
   "source": [
    "tensor3 = torch.tensor([[[1, 2], [4, 5]], [[6, 7], [8, 9]]])\n",
    "print(tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e37bd",
   "metadata": {},
   "source": [
    "You can get the dimension of a tensor using the *dim* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8fd903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(tensor0.dim(), tensor1.dim(), tensor2.dim(), tensor3.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493cbe86",
   "metadata": {},
   "source": [
    "Why would we need such high-dimensional tensors?\n",
    "Consider an NLP task where each *word is represented by a high-dimensional vector* (this is the *first* dimension).\n",
    "A *text consists of multiple words* (this is the *second* dimension).\n",
    "To decrease training time, we often pass *multiple texts at the same time* through a neural network (this is the *third* dimension).\n",
    "I want to reiterate, that this is *not* some theoretical construction, but in fact a very common setup.\n",
    "So the earlier you get used to high-dimensional tensors, the better.\n",
    "\n",
    "This is not meant to intimidate you (of course we *would* say that, wouldn't we).\n",
    "High-dimensional tensors sound scary (mostly because people associate them with quantum mechanics and the like), but for our purposes we don't care about most of that scariness.\n",
    "In fact, a lot of concepts concerning vectors and matrices generalize quite nicely to tensors.\n",
    "\n",
    "For example **tensor addition** and **tensor-constant multiplication** are done componentwise (just like with vectors and matrices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0907f1-23e7-41a1-99df-94ff1a2e8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3, 5],\n",
      "        [7, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[2, 1, 2], [3, 1, 0]])\n",
    "print(t1 + t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb1ceda-252b-46ee-9a3f-ed9eae78ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6,  9],\n",
      "        [12, 15, 18]])\n"
     ]
    }
   ],
   "source": [
    "print(3 * t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde44e26-3c1d-45fe-90dd-8c0c3ade6c32",
   "metadata": {},
   "source": [
    "Consider another operation we will commonly use with tensors - their **product** (which we denote by $\\otimes$).\n",
    "Do note that we will introduce a very particular tensor product (which is sometimes called the *matrix product of tensors*).\n",
    "Depending on your background, you may be familiar with other tensor products.\n",
    "\n",
    "> This is nothing unusual.\n",
    "> Consider matrix products for example.\n",
    "> There is the matrix multiplication and the Hadamard product (which is the element-wise multiplication of two matrices).\n",
    "> In fact, PyTorch has a multitude of functions for computing tensor products (for example matmul and tensordot, which do different things). > I want to stress - we introduce the tensor product the way we do because that is what is useful for us in deep learning - no more, no less.\n",
    "\n",
    "The product of two one-dimensional tensors is essentially the dot product.\n",
    "For example if $\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$ and $\\vec{w} = \\begin{bmatrix} 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{bmatrix}$, then $\\vec{v} \\otimes \\vec{w} = 1 \\cdot 5 + 2 \\cdot 6 + 3 \\cdot 7 + 4 \\cdot 8 = 70$.\n",
    "This is where you might have a first clash with the tensor product as it is known in other areas (where $\\vec{v} \\otimes \\vec{w}$ may be the *outer* product of $\\vec{v}$ and $\\vec{w}$).\n",
    "\n",
    "To compute this in pytorch, we could use the dot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fe72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(70)\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1, 2, 3, 4])\n",
    "w = torch.tensor([5, 6, 7, 8])\n",
    "print(torch.dot(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830f47c",
   "metadata": {},
   "source": [
    "We could also use the matmul function, which also generalizes to higher-dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2583c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(70)\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1, 2, 3, 4])\n",
    "w = torch.tensor([5, 6, 7, 8])\n",
    "print(torch.matmul(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ab7a4",
   "metadata": {},
   "source": [
    "The product of a one-dimensional tensor and a two-dimensional tensor is just regular matrix-vector multiplication. For example if $W = \\begin{bmatrix} 1 & 3 \\\\ 5 & 7 \\\\ 9 & 11 \\end{bmatrix}$ and $\\vec{x} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$, then $W \\otimes \\vec{x} = W \\cdot \\vec{x} = \\begin{bmatrix} 1 \\cdot 2 + 3 \\cdot 4 \\\\ 5 \\cdot 2 + 7 \\cdot 4 \\\\ 9 \\cdot 2 + 11 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ 38 \\\\ 62 \\end{bmatrix}$.\n",
    "\n",
    "Let us reproduce this with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af56c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 38, 62])\n"
     ]
    }
   ],
   "source": [
    "W = torch.tensor([[1, 3], [5, 7], [9, 11]])\n",
    "x = torch.tensor([2, 4])\n",
    "print(torch.matmul(W, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce9c38",
   "metadata": {},
   "source": [
    "Similarly the product of two two-dimensional tensors is just regular matrix-matrix multiplication. For example if $U = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}$ and $V = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}$, then $U \\otimes V = U \\cdot V = \\begin{bmatrix} 1 \\cdot 2 + 2 \\cdot 1 & 1 \\cdot 1 + 2 \\cdot 3 \\\\ 3 \\cdot 2 + 4 \\cdot 1 & 3 \\cdot 1 + 4 \\cdot 3 \\\\ 5 \\cdot 2 + 6 \\cdot 1 & 5 \\cdot 1 + 6 \\cdot 3 \\end{bmatrix} = \\begin{bmatrix} 4 & 7 \\\\ 10 & 15 \\\\ 16 & 23 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2886d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  7],\n",
      "        [10, 15],\n",
      "        [16, 23]])\n"
     ]
    }
   ],
   "source": [
    "U = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "V = torch.tensor([[2, 1], [1, 3]])\n",
    "print(torch.matmul(U, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f699c",
   "metadata": {},
   "source": [
    "But what about products of tensors with more than two dimensions?\n",
    "The tensor product generalizes in a straighforward fashion.\n",
    "Consider the multiplication of a three-dimensional tensor $Q$ of dimension $m \\times n \\times d$ (i.e. $Q \\in \\mathbb{R}^{m \\times n \\times d}$) with a one-dimensional tensor $\\vec{x}$ of dimension $d$ (i.e. $\\vec{x} \\in \\mathbb{R}^d$).\n",
    "We can treat $Q$ as a list of $m$ matrices of dimension $n \\times d$ each.\n",
    "Then we simply compute $m$ vector-matrix products.\n",
    "Each such product is a product of a matrix $Q_i \\in \\mathbb{R}^{n \\times d}$ and the vector $\\vec{x} \\in \\mathbb{R}^d$.\n",
    "Therefore each product results in vector of dimension $n$.\n",
    "We can stack these vectors back into a matrix resulting in a matrix of dimension $\\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "Consider the tensor $Q = \\begin{bmatrix} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 1\\end{bmatrix}, \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix} \\end{bmatrix}$ and the vector $\\vec{x} = \\begin{bmatrix} 5 \\\\ 6 \\\\ 7 \\end{bmatrix}$.\n",
    "We can think of $Q$ as a list of two matrices $Q = \\begin{bmatrix} Q_1 & Q_2 \\end{bmatrix}$ where $Q_1 = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ and $Q_2 = \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 0 & 0 \\end{bmatrix}$.\n",
    "\n",
    "Now we have $Q \\otimes \\vec{x} = \\begin{bmatrix} (Q_1 \\cdot \\vec{x})^\\intercal \\\\ (Q_2 \\cdot \\vec{x})^\\intercal \\end{bmatrix}$ (note that we need to transpose the individual dot products). Now we easily obtain $Q_1 \\cdot \\vec{x} = \\begin{bmatrix} 12 \\\\ 6 \\\\ 12 \\\\ 18 \\end{bmatrix}$ and $Q_2 \\cdot \\vec{x} = \\begin{bmatrix} 7 \\\\ 13 \\\\ 18 \\\\ 5 \\end{bmatrix}$. Stacking these vectors together, we get that $Q \\otimes \\vec{x} = \\begin{bmatrix} 12 & 6 & 12 & 18 \\\\ 7 & 13 & 18 & 5 \\end{bmatrix}$.\n",
    "\n",
    "Let's confirm this in code.\n",
    "First, we initialize the tensors `Q` and `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd367e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.tensor([\n",
    "    [[1, 0, 1], [0, 1, 0], [1, 0, 1], [1, 1, 1]],\n",
    "    [[0, 0, 1], [0, 1, 1], [1, 1, 1], [1, 0, 0]]\n",
    "])\n",
    "print(Q.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9120b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5, 6, 7])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2b63b-2594-486b-a793-aa96adce80dd",
   "metadata": {},
   "source": [
    "Now, we use the `matmul` function to compute the (matrix) product of `Q` and `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde8ed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "y = torch.matmul(Q, x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b52092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12,  6, 12, 18],\n",
      "        [ 7, 13, 18,  5]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f43d3",
   "metadata": {},
   "source": [
    "## Our First Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a21dce",
   "metadata": {},
   "source": [
    "Armed with the concepts we just learned, we can now construct our first very simple neural network. \n",
    "\n",
    "We will attempt to fit a toy classification dataset.\n",
    "Recall from the high-level overview chapter that the neural network *fits* the data if it labels all points from a dataset (more or less) correctly.\n",
    "Note that will not worry about concepts like validation or testing in this chapter.\n",
    "The only thing we *will* worry about is the training (fitting) process.\n",
    "\n",
    "Let's create a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbae8f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=0.0, x2=0.0, t=0\n",
      "x1=0.0, x2=1.0, t=0\n",
      "x1=1.0, x2=0.0, t=0\n",
      "x1=1.0, x2=1.0, t=1\n"
     ]
    }
   ],
   "source": [
    "x1s = torch.tensor([0.0, 0.0, 1.0, 1.0])\n",
    "x2s = torch.tensor([0.0, 1.0, 0.0, 1.0])\n",
    "ts = torch.tensor([0, 0, 0, 1])\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"x1={x1s[i]}, x2={x2s[i]}, t={ts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88e6cf",
   "metadata": {},
   "source": [
    "If you are familiar with basic logic, this is just the truth table for the *AND* function.\n",
    "\n",
    "Of course we can fit such a trivial dataset without fancy deep learning techniques.\n",
    "Nevertheless this is a very nice example that will allow us to showcase a few important concepts.\n",
    "\n",
    "Let's think about the simplest meaningful computational graph we could construct here.\n",
    "This would obviously be some kind of affine function, i.e. something that has the form $f(x) = x_1 \\cdot w_1 + x_2 \\cdot w_2 + b$.\n",
    "However, the output of an affine function can be arbitrary, but our classes can only take the values 0 and 1 (i.e. we are dealing with binary classification here).\n",
    "We would therefore like to squash the outputs of our function to the range $[0, 1]$.\n",
    "Then we can interpret the squashed value as the probability of the class represented by 1.\n",
    "For example if the squashed output is $0.7$, then we would say that the data point has the class 1 with probability $0.7$.\n",
    "This has the added benefit that we can assign a *confidence* to our predictions.\n",
    "If the probability is very high (or very low), we have more confidence regarding our prediction than if the probability is e.g. $0.5$.\n",
    "\n",
    "A commonly used squashing function is the so called *sigmoid* function defined by $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n",
    "It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96287cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7d54903b8a00>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sigmoid_xs = np.arange(-10, 10, 0.01)\n",
    "sigmoid_ys = 1 / (1 + np.exp(-sigmoid_xs))\n",
    "plt.plot(sigmoid_xs, sigmoid_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72f171",
   "metadata": {},
   "source": [
    "We can see the very small values essentially become 0, very large values essentially become 1 and everything in between is mapped to an appropriate value between $0$ and $1$ in a monotonous way (with $\\sigma(0) = 0.5$).\n",
    "This looks like a function that accomplishes what we intended.\n",
    "\n",
    "Therefore we define our first neural network by *applying a sigmoid function to an affine function*.\n",
    "Yes, that's really all we are going to do!\n",
    "Note that while this is the simplest meaningful setup for a classification task, it is still a *pratically useful* setup that is used for certain tasks. \n",
    "\n",
    "Formally speaking, our network is defined as $y = f(\\vec{x}) = f(x_1, x_2) = \\sigma(x_1 \\cdot w_1 + x_2 \\cdot w_2 + b)$.\n",
    "\n",
    "Let us represent this function using a computational graph.\n",
    "\n",
    "First we split $f$ into its respective computations:\n",
    "\n",
    "$h_1 = x_1 \\cdot w_1$\n",
    "\n",
    "$h_2 = x_2 \\cdot w_2$\n",
    "\n",
    "$h = h_1 + h_2$\n",
    "\n",
    "$z = h + b$\n",
    "\n",
    "$y = \\sigma(z)$\n",
    "\n",
    "Let's explicitly draw the graph:\n",
    "\n",
    "<img src=\"images/simple_net.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Note that this computational graph is indeed *differentiable* since every function has a derivative.\n",
    "\n",
    "Recall that the basic idea behind the training of (any) machine learning model is quite simple.\n",
    "We start with randomly initialized parameters (we will discuss initialization schemes later).\n",
    "Then we iterate through all the examples and update the parameters in such a way that we improve our performance on those examples.\n",
    "We often do multiple iterations on the dataset.\n",
    "Each iteration is called an **epoch**.\n",
    "In its simplest form, the training loop therefore looks like this:\n",
    "\n",
    "```python\n",
    "model = NeuralNetwork()\n",
    "for epoch in range(epochs):\n",
    "    for x, t in dataset:\n",
    "        model.update(x, t)\n",
    "```\n",
    "\n",
    "There are a few pratical considerations missing here - we will talk about them later.\n",
    "\n",
    "The big question we will discuss next is how the update should be performed.\n",
    "Intuitively, we would like to update the model parameters in such a way that the network \"performs\" a bit better on the example $x$ labeled with the target value $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89c97c",
   "metadata": {},
   "source": [
    "## The Parameter Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7bd8a",
   "metadata": {},
   "source": [
    "How could we accomplish that?\n",
    "\n",
    "First of all, we need to specify what we mean by \"better performance\".\n",
    "That is we need to quantify how  well a neural network performs on an example $x$ labeled with the target value $t$.\n",
    "\n",
    "To achieve that, we define a **loss function** which is the measure of how far the output of the network is from the correct target.\n",
    "We then simply update our parameters in such a way that the *loss decreases*.\n",
    "Put differently, we want to *minimize the loss*.\n",
    "\n",
    "In this particular case we will use a loss function called **binary crossentropy** which is defined as $BCE(y, t) = -t \\cdot log(y) - (1 - t) \\cdot log(1 - y)$. Here $y$ is the value produced by our network and $t$ is the target value.\n",
    "\n",
    "Binary crossentropy is a concept deep learning shamelessly stole (a.k.a. *borrowed*) from information theory, but we will not go into all that and instead motivate it with very simple intuition.\n",
    "Basically if $t = 1$, then we want the loss to be proportional to $-\\log(y)$.\n",
    "This is because a lower $y$ in our interpretation means a higher probability of $t = 0$ which is bad if in reality $t = 1$.\n",
    "Similarly, if $t = 0$, we want the loss to be proportional to $-\\log(1 - y)$.\n",
    "Now we simply combine this into one expression which gets us the formula from above.\n",
    "\n",
    "How does the loss fit into the computational graph?\n",
    "Even deep learning practicioners sometimes get confused about that, but the simple truth is that the loss is just another node put on top of the network:\n",
    "\n",
    "<img src=\"images/simple_net_loss.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Let us take our simple neural net and set $w_1 = w_2 = 0.5$, $b = 0$.\n",
    "A quick calculation yields an output value of $y = 0.6225$ for $x_1 = 0$, $x_2 = 1$.\n",
    "We see that the output value is not quite right.\n",
    "It should be $t = 0$ and not $y = 0.6225$.\n",
    "Therefore we want to update the parameters of the network (i.e. $w_1, w_2$ and $b$) in such a way that the output $y$ becomes a bit closer to the target $t$.\n",
    "\n",
    "What this really means is that we have to update our weights in such a way that the loss (let us call it $L$) becomes smaller.\n",
    "How could we approach this?\n",
    "\n",
    "If you have some basic knowledge of optimization theory, you are probably smirking right now.\n",
    "This is what gradient descent (and a bunch of other optimization algorithms) are for!\n",
    "However, even if you do not know any numerical optimization, you can still easily develop an idea for what we should do.\n",
    "\n",
    "Remember from high school that the derivative $\\frac{\\partial L}{\\partial b}$ tells you the way $L$ changes if we change $b$.\n",
    "For example, if $\\frac{\\partial L}{\\partial b} > 0$ then increasing $b$ would lead to an increase in $L$.\n",
    "If $\\frac{\\partial L}{\\partial b} < 0$ then an increase in $b$ would lead to a decrease in $L$.\n",
    "\n",
    "Now assume that we somehow get our hands on $\\frac{\\partial L}{\\partial b}$.\n",
    "We want to change $b$ in such a way that $L$ decreases.\n",
    "After all, we want to *minimize* the loss.\n",
    "Therefore we should update $b$ by a value proportionate to $-\\frac{\\partial L}{\\partial b}$.\n",
    "Put differently, we want to update $b$ by $-\\alpha \\cdot \\frac{\\partial L}{\\partial b}$ where $\\alpha$ is some parameter that controls how big our update should be.\n",
    "More formally, the update step looks as follows:\n",
    "\n",
    "$b^{(t+1)} = b^{(t)} - \\alpha \\cdot \\frac{\\partial L}{\\partial b^{(t)}}$\n",
    "\n",
    "That is, at step $t$ we update $b$ by subtracting $\\alpha \\cdot \\frac{\\partial L}{\\partial b^{(t)}}$ from it.\n",
    "\n",
    "Similar logic applies to the other parameters, i.e.\n",
    "\n",
    "$w_1^{(t+1)} = w_1^{(t)} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_1^{(t)}}$\n",
    "\n",
    "$w_2^{(t+1)} = w_2^{(t)} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_2^{(t)}}$\n",
    "\n",
    "After such an update the loss would become smaller.\n",
    "But how could we could calculate the derivative $\\frac{\\partial L}{\\partial b}$?\n",
    "Calculating e.g. the derivative $\\frac{\\partial L}{\\partial y}$ would be easy - we would simply need to take the derivative of the loss function.\n",
    "But $b$ does not influence the loss directly, it influences the loss through $y$.\n",
    "\n",
    "There is no reason to despair here, as we can use the *chain rule*.\n",
    "We know that $\\frac{\\partial L}{\\partial b}$ can be calculated as $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial b}$.\n",
    "But now we have the same problem.\n",
    "Again $b$ is not a direct input to $y$.\n",
    "How do we get $\\frac{\\partial y}{\\partial b}$?\n",
    "\n",
    "You can probably already see where this is going.\n",
    "We see that $y$ depends on $b$ through $z$.\n",
    "By the chain rule we have $\\frac{\\partial y}{\\partial b} = \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial b}$.\n",
    "\n",
    "We therefore have: $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial b}$.\n",
    "Now we only need to calculate the individual derivatives using our knowledge of basic differentiation and we are all set.\n",
    "\n",
    "Let's calculate the individual derivatives:\n",
    "\n",
    "1. We begin with $\\frac{\\partial L}{\\partial y}$. Since $L = -t \\cdot log(y) - (1 - t) \\cdot log(1 - y)$ we have $\\frac{\\partial L}{\\partial y} = \\frac{y - t}{(1 - y)y}$.\n",
    "\n",
    "2. Similarly we get $\\frac{\\partial y}{\\partial z} = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$.\n",
    "\n",
    "3. Finally, we have $\\frac{\\partial z}{\\partial b} = 1$.\n",
    "\n",
    "Therefore $\\frac{\\partial L}{\\partial b} = \\frac{y - t}{(1 - y)y} \\cdot \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$\n",
    "\n",
    "The same logic holds for $w_1$ and $w_2$.\n",
    "\n",
    "Here we have $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial h} \\frac{\\partial h}{\\partial h_1} \\frac{\\partial h_1}{\\partial w_1}$.\n",
    "\n",
    "Since $\\frac{\\partial L}{\\partial y} = -\\frac{t}{y} + \\frac{1 - t}{1 - y}$, $\\frac{\\partial y}{\\partial z} = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$, $\\frac{\\partial z}{\\partial h} = 1$, $\\frac{\\partial h}{\\partial h_1} = 1$ and $\\frac{\\partial h_1}{\\partial w_1} = x_1$ we have $\\frac{\\partial L}{\\partial w_1} = \\frac{y - t}{(1 - y)y} \\cdot \\frac{\\exp(-z)}{(1 + \\exp(-z))^2} \\cdot x_1$.\n",
    "\n",
    "You *should* verify all of this.\n",
    "It's a nice and simple, but *very relevant* exercise in calculating derivatives.\n",
    "\n",
    "In a similar fashion we obtain $\\frac{\\partial L}{\\partial w_2} = \\frac{y - t}{(1 - y)y} \\cdot \\frac{\\exp(-z)}{(1 + \\exp(-z))^2} \\cdot x_2$\n",
    "\n",
    "Let's write all of this down in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29cb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w1 = 0.5\n",
    "        self.w2 = 0.5\n",
    "        self.b = 0.0\n",
    "        \n",
    "        self.alpha = 1.0\n",
    "        \n",
    "    def forward(self, x1, x2, t):\n",
    "        h1 = x1 * self.w1\n",
    "        h2 = x2 * self.w2\n",
    "        h = h1 + h2\n",
    "        z = h + self.b\n",
    "        y = 1 / (1 + exp(-z))\n",
    "        L = -t * log(y) - (1 - t) * log(1 - y)\n",
    "        return { \"x1\": x1, \"x2\": x2, \"h1\": h1, \"h2\": h2, \"h\": h, \"z\": z, \"y\": y, \"L\": L }\n",
    "        \n",
    "    def update(self, x1, x2, t):\n",
    "        # Forward pass\n",
    "        forward_res = self.forward(x1, x2, t)\n",
    "        z, y = forward_res[\"z\"], forward_res[\"y\"]\n",
    "        \n",
    "        # Derivatives\n",
    "        dLdy = (y - t) / ((1 - y) * y)\n",
    "        dydz = exp(-z) / (1 + exp(-z)) ** 2\n",
    "        dLdw1 = dLdy * dydz * x1\n",
    "        dLdw2 = dLdy * dydz * x2\n",
    "        dLdb = dLdy * dydz\n",
    "        \n",
    "        # Update\n",
    "        self.w1 -= self.alpha * dLdw1\n",
    "        self.w2 -= self.alpha * dLdw2\n",
    "        self.b -= self.alpha * dLdb\n",
    "        return { \"dLdw1\": dLdw1, \"dLdw2\": dLdw2, \"dLdb\": dLdb }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92593a58",
   "metadata": {},
   "source": [
    "While this is pretty long, it's really just our above formulas written in Python.\n",
    "\n",
    "Let's take the first example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe2bd460",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, t = 0.0, 0.0, 0.0\n",
    "\n",
    "net = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47723be9",
   "metadata": {},
   "source": [
    "We run a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97400598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': 0.0,\n",
       " 'x2': 0.0,\n",
       " 'h1': 0.0,\n",
       " 'h2': 0.0,\n",
       " 'h': 0.0,\n",
       " 'z': 0.0,\n",
       " 'y': 0.5,\n",
       " 'L': 0.6931471805599453}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb06c4",
   "metadata": {},
   "source": [
    "Now we execute an update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d3216a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dLdw1': 0.0, 'dLdw2': 0.0, 'dLdb': 0.5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.update(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb59e42",
   "metadata": {},
   "source": [
    "We can see that $w_1$, $w_2$ and $b$ changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b9aa35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, -0.5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.w1, net.w2, net.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4561d",
   "metadata": {},
   "source": [
    "But did that change make sense? We could check this by running a forward pass and seeing if the loss decreased, i.e. if $y$ is closer to $t$ now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffc9aa5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': 0.0,\n",
       " 'x2': 0.0,\n",
       " 'h1': 0.0,\n",
       " 'h2': 0.0,\n",
       " 'h': 0.0,\n",
       " 'z': -0.5,\n",
       " 'y': 0.3775406687981454,\n",
       " 'L': 0.47407698418010663}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8796634",
   "metadata": {},
   "source": [
    "This looks good! Now all we have to do is to repeatedly iterate over the dataset and do updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb7df73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNet()\n",
    "for epoch in range(100):\n",
    "    for i in range(4):\n",
    "        x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "        net.update(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433de47",
   "metadata": {},
   "source": [
    "That's it!\n",
    "We've successfully trained our first neural network.\n",
    "If you understood the preceding section, you've come a *tremendously long* way to understanding how neural networks operate.\n",
    "The rest of this chapter is really just about coming up with a more efficient algorithm for doing this (as calculating all the derivatives by hand is very tedious).\n",
    "\n",
    "Before we move on, let us verify that we indeed get useful predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3184afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': tensor(0.), 'x2': tensor(0.), 'h1': tensor(0.), 'h2': tensor(0.), 'h': tensor(0.), 'z': tensor(-8.9435), 'y': 0.00013056159878900093, 'L': tensor(0.0001)}\n",
      "{'x1': tensor(0.), 'x2': tensor(1.), 'h1': tensor(0.), 'h2': tensor(5.8847), 'h': tensor(5.8847), 'z': tensor(-3.0589), 'y': 0.04483687464928194, 'L': tensor(0.0459)}\n",
      "{'x1': tensor(1.), 'x2': tensor(0.), 'h1': tensor(5.9416), 'h2': tensor(0.), 'h': tensor(5.9416), 'z': tensor(-3.0019), 'y': 0.0473383390833865, 'L': tensor(0.0485)}\n",
      "{'x1': tensor(1.), 'x2': tensor(1.), 'h1': tensor(5.9416), 'h2': tensor(5.8847), 'h': tensor(11.8263), 'z': tensor(2.8827), 'y': 0.9469867809360202, 'L': tensor(0.0545)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    result = net.forward(x1s[i], x2s[i], ts[i])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10e047",
   "metadata": {},
   "source": [
    "Incredible!\n",
    "We can see that loss has become very small for every point.\n",
    "Let us have a look at the learned parameters and try to gain an intuition for what the network learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85defcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.9416), tensor(5.8847), tensor(-8.9435))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.w1, net.w2, net.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fcdf5",
   "metadata": {},
   "source": [
    "Basically we learned the function $y = f(x_1, x_2) = \\sigma(5.94 \\cdot x_1 + 5.88 \\cdot x_2 - 8.94)$.\n",
    "This makes sense.\n",
    "Essentially the network arrived at the scheme \"$x_1$ and $x_2$ both need to make a contribution to produce a large value, otherwise the bias will pull the total value down\".\n",
    "This is just a (slightly convoluted) description for the *AND* function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51f73f",
   "metadata": {},
   "source": [
    "## The Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3153f2",
   "metadata": {},
   "source": [
    "Manual updates work fine, but involve calculating lots of derivatives.\n",
    "This would mean that for every new model we would need to recalculate all the derivatives by hand which is going to get quite annoying for _Large_ Language Models.\n",
    "We want to be able to do this more effectively. \n",
    "\n",
    "Let's closely examine the flow of derivatives through the network:\n",
    "\n",
    "<img src=\"images/backprop.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "As you can see the derivatives flow _backwards_. \n",
    "\n",
    "Additionally, the flow is very well structured.\n",
    "\n",
    "Consider the sigmoid node.\n",
    "The derivative that _flows out_ of the node ($\\frac{\\partial L}{\\partial z}$) only depends on the derivative that _flows in_ ($\\frac{\\partial L}{\\partial y}$) and the _local derivative at the node_ $\\frac{\\partial y}{\\partial z}$.\n",
    "After all, the chain rule tells us that $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z}$.\n",
    "\n",
    "The same holds true for all the other nodes!\n",
    "_Given the derivative flowing in each node can calculate the derivative flowing out._\n",
    "\n",
    "This has an interesting consequence.\n",
    "Instead of calculating the derivatives for every network from scratch, we can specify the nodes and give every node a `forward` and a `backward` function.\n",
    "\n",
    "The `forward` function takes inputs to the node and produces outputs from the node.\n",
    "The `backward` function takes the _derivative_ flowing in and produces the _derivative_ flowing out.\n",
    "We can then execute approximately the following **backpropagation** algorithm :\n",
    "\n",
    "```python\n",
    "forward_sorted_nodes = sort_forward(nodes)\n",
    "for node in forward_sorted_nodes:\n",
    "    input_values = node.get_input_values()\n",
    "    node.output_values = node.calc_output_values(input_values)\n",
    "    \n",
    "backward_sorted_nodes = sort_backward(nodes)\n",
    "for node in backward_sorted_nodes:\n",
    "    output_node = get_output_node(node)\n",
    "    local_grad = node.get_local_grad()\n",
    "    node.grad = local_grad * output_node.grad\n",
    "```\n",
    "\n",
    "Let us now implement a bunch of nodes in pytorch.\n",
    "Note that we usually do not have to do that (the PyTorch authors already implemented most of the important nodes).\n",
    "Nevertheless, this is an extremely useful exercise that will teach you a great deal about how neural networks *really work*.\n",
    "\n",
    "We begin by implementing the addition node.\n",
    "The forward function would simply be $z = x + y$.\n",
    "For the backward function we would need to return two gradients - one for $x$ and one for $y$.\n",
    "\n",
    "They are pretty simple.\n",
    "We see that $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}$.\n",
    "Since $\\frac{\\partial z}{\\partial x} = 1$ we have $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}$.\n",
    "Similarly $\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial z}$.\n",
    "\n",
    "Let's put this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69661310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        return x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b754d5",
   "metadata": {},
   "source": [
    "Now we turn to multiplication.\n",
    "The forward function is $z = x \\cdot y$.\n",
    "The backward pass again needs to calculate $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}$. \n",
    "\n",
    "However, now the local gradients are different.\n",
    "We have $\\frac{\\partial z}{\\partial x} = y$ and $\\frac{\\partial z}{\\partial y} = x$.\n",
    "\n",
    "This means that the backward pass is dependent on the forward pass.\n",
    "This is no problem however, as we can store the values from the forward pass using the `ctx` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53a40664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        ctx.x, ctx.y = x, y\n",
    "        return x * y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, y = ctx.x, ctx.y\n",
    "        return grad_output * y, grad_output * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b0866",
   "metadata": {},
   "source": [
    "Using a similar approach, we can define the `Sigmoid` node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "746b0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.x = x\n",
    "        return torch.tensor(1 / (1 + exp(-x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.x\n",
    "        \n",
    "        # Note that here we need to convert the local gradient to a tensor\n",
    "        grad_local = torch.tensor(exp(-x) / (1 + exp(-x)) ** 2)\n",
    "        return grad_output * grad_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d84ac8",
   "metadata": {},
   "source": [
    "And finally, we define the node for the BCE loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd5d78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, y, t):\n",
    "        ctx.y, ctx.t = y, t          \n",
    "        return -t * log(y) - (1 - t) * log(1 - y)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y, t = ctx.y, ctx.t\n",
    "        y_grad_local = (y - t) / ((1 - y) * y)\n",
    "        t_grad_local = -log(y) + log(1 - y)\n",
    "        return grad_output * y_grad_local, grad_output * t_grad_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c3e67",
   "metadata": {},
   "source": [
    "Let us sanity check this. We pass the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b74c9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(0.0)\n",
    "x2 = torch.tensor(0.0)\n",
    "w1 = torch.tensor(0.5, requires_grad=True)\n",
    "w2 = torch.tensor(0.5, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253c2b8",
   "metadata": {},
   "source": [
    "Now we execute the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9358a2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h1': tensor(0., grad_fn=<MulBackward>),\n",
       " 'h2': tensor(0., grad_fn=<MulBackward>),\n",
       " 'h': tensor(0., grad_fn=<AddBackward>),\n",
       " 'z': tensor(0., grad_fn=<AddBackward>),\n",
       " 'y': tensor(0.5000, grad_fn=<SigmoidBackward>),\n",
       " 'L': tensor(0.6931, grad_fn=<BCELossBackward>)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = Mul.apply(w1, x1)\n",
    "h2 = Mul.apply(w2, x2)\n",
    "h = Add.apply(h1, h2)\n",
    "z = Add.apply(h, b)\n",
    "y = Sigmoid.apply(z)\n",
    "t = torch.tensor(0.0)\n",
    "L = BCELoss.apply(y, t)\n",
    "{ \"h1\": h1, \"h2\": h2, \"h\": h, \"z\": z, \"y\": y, \"L\": L }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e6610",
   "metadata": {},
   "source": [
    "Looks good so far.\n",
    "Now if we call the `backward` function on the loss node, PyTorch will automatically calculate the gradients using the algorithm from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87c9189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "befe78b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.), tensor(0.5000))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad, w2.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98548788",
   "metadata": {},
   "source": [
    "This is the same as the result we got in the manual version from above.\n",
    "Let us define the forward and update functions, but now using our nodes instead of doing manual derivative calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42df180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoSimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w1 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.w2 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.b = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        self.alpha = 1.0\n",
    "        \n",
    "    def forward(self, x1, x2, t):\n",
    "        h1 = Mul.apply(self.w1, x1)\n",
    "        h2 = Mul.apply(self.w2, x2)\n",
    "        h = Add.apply(h1, h2)\n",
    "        z = Add.apply(h, self.b)\n",
    "        y = Sigmoid.apply(z)\n",
    "        L = BCELoss.apply(y, t)\n",
    "        return { \"x1\": x1, \"x2\": x2, \"y\": y, \"L\": L }\n",
    "        \n",
    "    def update(self, x1, x2, t):\n",
    "        # Forward pass\n",
    "        forward_res = self.forward(x1, x2, t)\n",
    "        L = forward_res[\"L\"]\n",
    "        \n",
    "        # Backward pass\n",
    "        L.backward()\n",
    "        \n",
    "        # Update\n",
    "        with torch.no_grad():\n",
    "            dLdw1 = float(self.w1.grad)\n",
    "            dLdw2 = float(self.w2.grad)\n",
    "            dLdb = float(self.b.grad)\n",
    "            self.w1 -= self.alpha * dLdw1\n",
    "            self.w2 -= self.alpha * dLdw2\n",
    "            self.b -= self.alpha * dLdb\n",
    "            \n",
    "            self.w1.grad.zero_()\n",
    "            self.w2.grad.zero_()\n",
    "            self.b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f2f60",
   "metadata": {},
   "source": [
    "The training loop doesn't change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "858f9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AutoSimpleNet()\n",
    "for epoch in range(100):\n",
    "    for i in range(4):\n",
    "        x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "        net.update(x1, x2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05de96",
   "metadata": {},
   "source": [
    "And now for the moment of truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f216c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': tensor(0.), 'x2': tensor(0.), 'y': tensor(0.0001, grad_fn=<SigmoidBackward>), 'L': tensor(0.0001, grad_fn=<BCELossBackward>)}\n",
      "{'x1': tensor(0.), 'x2': tensor(1.), 'y': tensor(0.0448, grad_fn=<SigmoidBackward>), 'L': tensor(0.0459, grad_fn=<BCELossBackward>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(0.), 'y': tensor(0.0473, grad_fn=<SigmoidBackward>), 'L': tensor(0.0485, grad_fn=<BCELossBackward>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(1.), 'y': tensor(0.9470, grad_fn=<SigmoidBackward>), 'L': tensor(0.0545, grad_fn=<BCELossBackward>)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    result = net.forward(x1s[i], x2s[i], ts[i])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1182a1",
   "metadata": {},
   "source": [
    "That looks great! Let us perform an additional sanity check by confirming that $w_1$, $w_2$ and $b$ have the same values as in the manual version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "651ce3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9416, requires_grad=True) tensor(5.8847, requires_grad=True) tensor(-8.9435, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.w1, net.w2, net.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2718807",
   "metadata": {},
   "source": [
    "Incredibly enough, there is not much more than this to backpropagation.\n",
    "\n",
    "You now understand the algorithm which underpins basically all of deep learning - from the simplest perceptrons to the fanciest state of the art models out there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a9407",
   "metadata": {},
   "source": [
    "## Backpropagation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf8803",
   "metadata": {},
   "source": [
    "Of course in reality we do not actually go around implementing all the backward functions from scratch.\n",
    "As we already mentioned, the PyTorch team has already done that for *a lot* of functions.\n",
    "We can therefore simply declare the computational graph using those builtin functions, call `backward` and then do the gradient updates.\n",
    "\n",
    "Along the way, we will also simplify the `forward` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e878c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w1 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.w2 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.b = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        self.loss_fun = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, x1, x2, t):\n",
    "        \"\"\"\n",
    "        h1 = torch.mul(self.w1, x1)\n",
    "        h2 = torch.mul(self.w2, x2)\n",
    "        h = torch.add(h1, h2)\n",
    "        z = torch.add(h, self.b)\n",
    "        y = torch.sigmoid(z)\n",
    "        z.retain_grad()\n",
    "        y.retain_grad()\n",
    "        \"\"\"\n",
    "        z = x1 * self.w1 + x2 * self.w2 + self.b\n",
    "        y = torch.sigmoid(z)\n",
    "        \n",
    "        L = self.loss_fun(y, t.float())\n",
    "        return { \"x1\": x1, \"x2\": x2, \"y\": y, \"L\": L }\n",
    "\n",
    "    def update(self, x1, x2, t):\n",
    "        forward_res = self.forward(x1, x2, t)\n",
    "        L = forward_res[\"L\"]\n",
    "        \n",
    "        # Backward\n",
    "        L.backward()\n",
    "        \n",
    "        # Update\n",
    "        with torch.no_grad():\n",
    "            dLdw1 = float(self.w1.grad)\n",
    "            dLdw2 = float(self.w2.grad)\n",
    "            dLdb = float(self.b.grad)\n",
    "            self.w1 -= self.alpha * dLdw1\n",
    "            self.w2 -= self.alpha * dLdw2\n",
    "            self.b -= self.alpha * dLdb\n",
    "            \n",
    "            self.w1.grad.zero_()\n",
    "            self.w2.grad.zero_()\n",
    "            self.b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635fb3f",
   "metadata": {},
   "source": [
    "Again the training loop doesn't change at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84379246",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TorchSimpleNet()\n",
    "for epoch in range(100):\n",
    "    for i in range(4):\n",
    "        x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "        net.update(x1, x2, t) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4495e0",
   "metadata": {},
   "source": [
    "We do the forward passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d667fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': tensor(0.), 'x2': tensor(0.), 'y': tensor(0.0001, grad_fn=<SigmoidBackward0>), 'L': tensor(0.0001, grad_fn=<BinaryCrossEntropyBackward0>)}\n",
      "{'x1': tensor(0.), 'x2': tensor(1.), 'y': tensor(0.0448, grad_fn=<SigmoidBackward0>), 'L': tensor(0.0459, grad_fn=<BinaryCrossEntropyBackward0>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(0.), 'y': tensor(0.0473, grad_fn=<SigmoidBackward0>), 'L': tensor(0.0485, grad_fn=<BinaryCrossEntropyBackward0>)}\n",
      "{'x1': tensor(1.), 'x2': tensor(1.), 'y': tensor(0.9470, grad_fn=<SigmoidBackward0>), 'L': tensor(0.0545, grad_fn=<BinaryCrossEntropyBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "    print(net.forward(x1, x2, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083124c3",
   "metadata": {},
   "source": [
    "Let us also sanity check the resulting values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea56037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9416, requires_grad=True) tensor(5.8847, requires_grad=True) tensor(-8.9435, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.w1, net.w2, net.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d06e1",
   "metadata": {},
   "source": [
    "This is a very basic and simple, but *complete* example of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a11c2-59ac-41f5-acc3-d6049509e8dd",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2615932-f1b9-438c-acec-dfb1ec8803ec",
   "metadata": {},
   "source": [
    "There are various optimization algorithms apart from gradient descent.\n",
    "Additionally, implementing gradient descent by hand every time we create a neural network feels kind of unnecessary.\n",
    "\n",
    "Luckily, PyTorch provides a dedicated package for optimization algorithms.\n",
    "The `torch.optim` package allows us to construct an optimizer object that will hold the current state and update the parameters based on the computed gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d3a7a41-1660-45b7-a3aa-1cfc74fa4e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.0448, grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.0473, grad_fn=<SigmoidBackward0>)\n",
      "tensor(0.9470, grad_fn=<SigmoidBackward0>)\n",
      "tensor(5.9416, requires_grad=True) tensor(5.8847, requires_grad=True) tensor(-8.9435, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class TorchSimpleNetV2:\n",
    "    def __init__(self):\n",
    "        self.w1 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.w2 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.b = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        self.alpha = 1.0\n",
    "        \n",
    "        self.loss_fun = nn.BCELoss()\n",
    "\n",
    "        # To construct the optimizer, we need to give it an iterable\n",
    "        # containing the parameters to optimize.\n",
    "        self.optimizer = optim.SGD([self.w1, self.w2, self.b], lr=self.alpha)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z = x1 * self.w1 + x2 * self.w2 + self.b\n",
    "        y = torch.sigmoid(z)\n",
    "        return y\n",
    "\n",
    "    def compute_loss(self, y, t):\n",
    "        return self.loss_fun(y, t.float())\n",
    "\n",
    "    def update(self, x1, x2, t):\n",
    "        # Forward pass\n",
    "        y = self.forward(x1, x2)\n",
    "        \n",
    "        # Compute loss\n",
    "        L = self.compute_loss(y, t)\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        L.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "net = TorchSimpleNetV2()\n",
    "for epoch in range(100):\n",
    "    for i in range(4):\n",
    "        x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "        net.update(x1, x2, t)\n",
    "\n",
    "for i in range(4):\n",
    "    x1, x2, t = x1s[i], x2s[i], ts[i]\n",
    "    print(net.forward(x1, x2))\n",
    "\n",
    "print(net.w1, net.w2, net.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fabd7-469a-42f3-bbb7-0374ab657d09",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3894c6-950f-4d79-864d-5533e846f477",
   "metadata": {},
   "source": [
    "If we look at the backpropagation algorithm we can quickly see that if our network is very deep and we chain a lot of multiplications two problems can occur.\n",
    "\n",
    "**Vanishing gradients** occur when the gradients of the loss function with respect to the network's parameters become very small as they propagate backward through the layers.\n",
    "Indeed, if we multiply many small numbers with each other, we will get an increasingly smaller (vanishing) number resulting in very small gradients and therefore very small updates.\n",
    "As a result, the layers will learn extremely slowly (if at all).\n",
    "\n",
    "**Exploding gradients**, on the other hand, occur when the gradients become excessively large during backpropagation.\n",
    "Indeed, if we multiply many lage numbers with each other, we will get an increasingly large (exploding) number resulting in very large updates and therefore unstable updates.\n",
    "\n",
    "There are various strategies to mitigate this (which we will discuss in the later chapters).\n",
    "Most importantly, we have to carefully initialize the model weights (luckily, PyTorch already implements that out of the box).\n",
    "\n",
    "However, you should be aware that this is a problem and this problem occurs because of the way backpropagation works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca8adf-64f4-4a33-9dd5-7b8c6ff09505",
   "metadata": {},
   "source": [
    "## Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c0a52-533e-427d-95df-e09fa2f4cae4",
   "metadata": {},
   "source": [
    "Overfitting is a common challenge in machine learning where a model \"fits\" the data too closely.\n",
    "Basically, instead of generalizing from underlying patterns, the models tries to just memorize the data set.\n",
    "As a result, the model performs exceptionally well on the training data, but terribly at new, unseen data (because the model has essentially memorized the training data rather than learning the broader patterns).\n",
    "\n",
    "Regularization is a set of techniques used to prevent overfitting by imposing constraints on the complexity of the model.\n",
    "Some forms of regularization add a penalty to the loss function based on the magnitude of the model's weights.\n",
    "However, there are other forms of regularization as well (which we will look at in later chapters)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
