{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef73e18-c7cc-4f75-ae87-6ce2513d77cc",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1a220-e8b5-490e-85c4-ed84aa63f777",
   "metadata": {},
   "source": [
    "## The Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f4634-04c7-4f1c-8c69-039d25a25b38",
   "metadata": {},
   "source": [
    "The job of the tokenizer is to split the input text into individual units - called **tokens** - for further processing by the LLM. A token is simply any string - tokens can be characters, they can be subwords, words and (theoretically) even something larger.\n",
    "\n",
    "We will use the TinyShakespare dataset as our example dataset for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6228feee-57e9-4c78-9e6a-93e0f144a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "res = requests.get(url)\n",
    "content = res.content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca0cac-80c8-471a-b9b2-f7220dba3d9f",
   "metadata": {},
   "source": [
    "Let's have a brief look at the size of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc10fa3-7b5a-473f-b413-a586e201393c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3baaaa-2e3f-4bcd-a2dc-dd58e6bb8175",
   "metadata": {},
   "source": [
    "Let's also have a brief look at some sample text from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9a39f6-12e7-483a-9749-c603711d2527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e9aae-8104-4bef-9b32-660c3fbdf542",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77caf3f3-24a2-44a4-970d-e1e49555a450",
   "metadata": {},
   "source": [
    "There are many different strategies to split a text into tokens.\n",
    "\n",
    "One very simple way would be to use character tokenization - simply split the text into it's individual characters.\n",
    "The problem with this is that the model would have to learn everything from the ground up (including what a word is) which seems vaguely unncessary.\n",
    "\n",
    "One better way is to use word tokenization - just split the text into words.\n",
    "However, this implicitly treats all words as equally important and leads to a very large vocabulary size (i.e. the number of possible tokens).\n",
    "\n",
    "Current approaches therefore have settled somewhere in between at **subword tokenization**.\n",
    "\n",
    "One very popular scheme is **Byte Pair Encoding (BPE)**, which builds vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words.\n",
    "Put simply, first all individual characters are added to the vocabulary.\n",
    "Then, the most common 2-character combinations are added, followed by the most common 3-character combinations etc.\n",
    "\n",
    "The exact details of BPE are not extremely important right now, just keep in mind that BPE does subword tokenization.\n",
    "\n",
    "We will use the `tiktoken` library to show an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd823a84-6fa0-41ec-8cbf-de0aa3462d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca81c3-d6cd-4212-826e-6238f230b34b",
   "metadata": {},
   "source": [
    "We can use the `encode` method to tokenize a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2ecd62-dc07-48bb-bd6d-6d943845cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(content, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a244893-2e94-4248-88b4-9b8c1d260b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c40657-6f7e-4691-9044-c463d53c8707",
   "metadata": {},
   "source": [
    "Note that the tokenizer doesn't return a list of strings. Instead, it returns a list of integers, where every integer is an ID representing a certain token from the tokenizer vocabulary.\n",
    "\n",
    "We can view the tokens themselves like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b9c564-22cd-473f-9e9f-6b7f67e0f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First'\n",
      "' Citizen'\n",
      "':'\n",
      "'\\n'\n",
      "'Before'\n",
      "' we'\n",
      "' proceed'\n",
      "' any'\n",
      "' further'\n",
      "','\n"
     ]
    }
   ],
   "source": [
    "for encoded_id in encoded[:10]:\n",
    "    print(repr(tokenizer.decode([encoded_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e4800-fcab-4883-8f75-3257f8dd0f5d",
   "metadata": {},
   "source": [
    "We can also decode multiple tokens like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c441b13e-db4a-4717-81b5-b7c65f27c39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further,'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf1650-6956-4ac0-95fb-864a95f89307",
   "metadata": {},
   "source": [
    "The BPE tokenizer can handle any unknown word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a878778-fc24-45e0-9152-fc3bc7c69508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdasdaf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_text = \"asdasdaf\"\n",
    "tokenizer.decode(tokenizer.encode(unknown_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250dda3-dd6d-40ee-b0f3-a1a3002a4e6c",
   "metadata": {},
   "source": [
    "This is because BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf396d5-c977-47d3-bbaf-9d0ad9eee89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'as'\n",
      "'d'\n",
      "'as'\n",
      "'d'\n",
      "'af'\n"
     ]
    }
   ],
   "source": [
    "for encoded_id in tokenizer.encode(unknown_text):\n",
    "    print(repr(tokenizer.decode([encoded_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeea054-0f98-45f3-9ceb-e36c7342214e",
   "metadata": {},
   "source": [
    "## Tokenizers in `transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c852b9-c830-486f-8b01-a4c846536595",
   "metadata": {},
   "source": [
    "The `transformers` library also allows us to work with tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afc5906-b6a5-4352-b71e-899747067fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53da0af3fea4ceca9e126fe873ae8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5ada03561a4aafb0de3ad6efd07f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900cfb94774e4cbab906701ce9b26f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf6843572204d3a841fa793ccc94008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1212,  318,  257, 6827]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "text = \"This is a sentence\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f56382-b020-402f-a7d7-fa748264a188",
   "metadata": {},
   "source": [
    "The `input_ids` value represents the token IDs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67355460-d910-46c4-8965-70cc43fc81f9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
