{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58a6447-6e2d-4b16-a7d1-f26749670255",
   "metadata": {},
   "source": [
    "# Basic Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ce672-2339-48d2-be65-e08aa8a1700e",
   "metadata": {},
   "source": [
    "Apart from the basic nodes we encountered in the previous chapter, PyTorch also provides us more complex *layers* which encapsulate groups of nodes.\n",
    "From the perspective of the computational graph, a layer is basically a subgraph of the computational graph.\n",
    "\n",
    "However, it is usually more helpful to treat layers as mathematical functions that take input tensors and produce output tensors.\n",
    "\n",
    "Layers are located in the `torch.nn` package, so we need to import it along with a few other packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c037c-db03-4667-ab85-01390a516ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b6b01-ac07-46f8-9b7c-0447420790af",
   "metadata": {},
   "source": [
    "We will also set the seed for generating random numbers to improve reproducibility of our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf175b9-b026-4e3d-a79b-62179f6ab353",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f200cc-5a91-4135-81c6-3089508aada4",
   "metadata": {},
   "source": [
    "We will also disable printing in scientific mode to simplify how our tensors are output to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5f91d-bb47-4df7-bbea-23506eebb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b1e4b-38ea-45d7-b396-c76fce92553f",
   "metadata": {},
   "source": [
    "## The Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147278f-9ce8-4e2d-b83e-0fc18b6d958c",
   "metadata": {},
   "source": [
    "One of the simplest and most important layers is the **linear layer**.\n",
    "\n",
    "The linear layer represents the function $f(\\vec{x}) = W\\vec{x} + \\vec{b}$.\n",
    "Put differently, the linear layer takes an input tensor $\\vec{x}$ and produces an output tensor $\\vec{y} = W\\vec{x} + \\vec{b}$.\n",
    "\n",
    "Usually the matrix `W` is called the _weight matrix_ and the vector `b` the _bias_.\n",
    "\n",
    "Whenever you are thinking about a layer, it is very helpful to think about the dimensions of the tensors involved. \n",
    "Consider an example, where $\\vec{x}$ has $3$ features, i.e. $\\vec{x} \\in \\mathbb{R}^{3}$.\n",
    "Let $\\vec{y}$ represent a probability distribution over $4$ classes, i.e. $\\vec{y} \\in \\mathbb{R}^{4}$.\n",
    "Then we have $W \\in \\mathbb{R}^{4 \\times 3}$ and $\\vec{b} \\in \\mathbb{R}^{4}$ (otherwise the dimensions won't match).\n",
    "\n",
    "This is how the above example would look in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cf848-88aa-4944-bd05-ce75e0f165a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 3\n",
    "d_out = 4\n",
    "\n",
    "x = torch.randn(d_in)\n",
    "\n",
    "layer = nn.Linear(in_features=d_in, out_features=d_out)\n",
    "\n",
    "y = layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0077bcb-c6fc-4fd1-b847-acf223b5c08b",
   "metadata": {},
   "source": [
    "Observe how the usage of the layer further emphasizes its nature as a mathematical function.\n",
    "Indeed, the layer can be called as a function that takes an input argument `x` and simply produces an output argument `y`.\n",
    "\n",
    "Let's now verify that the dimensions and values are what we expect.\n",
    "\n",
    "First, let's check that `x` is a one-dimensional tensor (i.e. a vector) of size `3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93bb61-5537-44f3-a339-3ede1ce8fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3a4e0-c0bb-4e14-aaa3-07fb7ef7b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a702007-db9a-45d8-95f1-d9ed2a230ce3",
   "metadata": {},
   "source": [
    "Next, let's verify that `y` is a one-dimensional tensor (i.e. a vector) of size `4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057846c5-1757-4b65-aacb-736fff9d5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b2f8e-d252-4730-b362-b784a458d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2ee0f-2288-4a1f-9d51-b7c0871c3aab",
   "metadata": {},
   "source": [
    "Next, let's verify that `W` is a two-dimensional tensor (i.e. a matrix) of size `4x3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db23ab4-f354-4da1-b3f3-b2deced4e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c091fa8-5972-480c-ac3d-a31030a89e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665df1d-3982-4912-906b-783183abf55c",
   "metadata": {},
   "source": [
    "And finally, we check that `b` is a one-dimensional tensor (i.e. a vector) of size `4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3745f-dc12-462c-bf41-b44430fccc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6676b-265c-4c4a-9159-8c495b0af98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd872467-3974-4262-8e51-6471f913447b",
   "metadata": {},
   "source": [
    "Put more generally if we have an input vector $\\vec{x} \\in \\mathbb{R}^d$, the linear layer with parameters $W \\in \\mathbb{R}^{k \\times d}$ and $\\vec{b} \\in \\mathbb{R}^k$ will compute an output vector $\\vec{y} \\in \\mathbb{R}^k$.\n",
    "Always keep thinking about tensor dimensionalities - this is really helpful when you are trying to understand a layer.\n",
    "\n",
    "Let's also check that the linear layer indeed performs the calculation $f(\\vec{x}) = W\\vec{x} + \\vec{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee29cc-39c5-4fb6-bba7-70ab244175f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_y = torch.matmul(layer.weight, x) + layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6feb6-a82f-4aec-b200-dcbad381ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manual_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b49ae8-3513-42b9-9736-88357e7f97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y == manual_y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68c4e9-66ea-493c-b1b9-6e57aa572775",
   "metadata": {},
   "source": [
    "Redoing the calculations of a layer manually is not something you will do in production, but it's a really helpful way to make sure that your understanding is correct.\n",
    "\n",
    "Note that we can pass a batch through the linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6ba17-31b0-4aab-9a7c-33969948b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 2\n",
    "\n",
    "X = torch.randn(batch_dim, d_in)\n",
    "Y = layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54c50e-0174-489d-a79d-29c40f1d635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b033b94-276d-4870-a4a3-4e669cff01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6e639-2806-4ee0-866e-5cb7b80a4628",
   "metadata": {},
   "source": [
    "Compared to the vectors `x` and `y` from before, the tensors `X` and `Y` now have an additional dimension - the batch dimension.\n",
    "Basically, we can perform the calculation of the linear layer for multiple vectors _at the same time_ greatly speeding up the computation.\n",
    "\n",
    "We can verify that `Y` just contains the result of applying the linear function $f(\\vec{x}) = W\\vec{x} + \\vec{b}$ to every row of `X` at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee270d-aa8a-450f-9367-f5e1bb827b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_y_0 = torch.matmul(layer.weight, X[0]) + layer.bias\n",
    "manual_y_1 = torch.matmul(layer.weight, X[1]) + layer.bias\n",
    "manual_Y = torch.stack((manual_y_0, manual_y_1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bbc8c-16bd-4759-bab1-64b5f5ff12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36852e-0116-4011-8cdf-af553d790d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manual_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867118a2-e0ab-415e-8b97-8525d3a67f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((Y == manual_Y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e408c4c-f0ff-47c5-b7c9-220a6fa21bf7",
   "metadata": {},
   "source": [
    "As discussed in the chapter on computational graphs, when we train a neural network we really _update its parameters_.\n",
    "Now that we have layers, we update the parameters of the layers.\n",
    "In the case of the linear layer these are `W` and `b`.\n",
    "\n",
    "We can view all learnable parameters of a layer using the `parameters()` and `named_parameters()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da8356-e6f1-41d2-b7af-1f4b94cf4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in layer.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b4ccf-c01c-42fc-88cc-05bd3fd433af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in layer.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938dd4a-9040-49e2-8901-bee87d612d92",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f37211-bf83-4375-a6c4-5f55a9b2a9ad",
   "metadata": {},
   "source": [
    "So far our learning hasn't been really _deep_ which is kind of a shame given that the field this book is about is called _deep learning_.\n",
    "\n",
    "Instead of having a single layer, with modern neural networks we have multiple layers on top of each other.\n",
    "The idea here is that each layer learns progressively more complex features.\n",
    "For example, the first layer might learn how individual words interact with each other, the second layer might look at word groups and the third layer might look at sentences.\n",
    "Note that this just an example - in practice the layers learn much more complicated features that can't be easily described (this is also one of their drawbacks).\n",
    "\n",
    "How would we go about constructing such a _deep_ network?\n",
    "\n",
    "Unfortunately, we can't just put linear layers on top of each other.\n",
    "Consider two linear layers which compute $\\vec{h} = W\\vec{x} + \\vec{b}$ and $\\vec{y} = V\\vec{h} + \\vec{c}$ respectively.\n",
    "Then we would have $\\vec{y} = V\\vec{h} + \\vec{c} = V(W\\vec{x} + \\vec{b} + \\vec{c}) = VW\\vec{x} + V\\vec{b} + \\vec{c}$.\n",
    "Let $U = VW$ and $\\vec{d} = V\\vec{b} + \\vec{c}$ and we can see that $\\vec{y} = U\\vec{x} + \\vec{c}$.\n",
    "Therefore the composition of two linear layers is again a linear layer!\n",
    "\n",
    "If you know your linear algebra, the above computation was a trivial exercise, but it reveals an important truth.\n",
    "_We need to go beyond linearities to obtain models with rich capabilities._\n",
    "\n",
    "To solve this problem, we introduce a _nonlinear activation function_ between the layers to break the linearity.\n",
    "That is, the first layer computes $\\vec{h} = f(W\\vec{x} + \\vec{b})$ where $f$ is some nonlinear function and is usually applied elementwise to the vector $W\\vec{x} + \\vec{b}$.\n",
    "\n",
    "If we have two linear layers separated by a nonlinear activation function, the resulting function is _no longer linear_ and we can build impressively capable models.\n",
    "With the idea clear, we need to actually _define_ the activation function.\n",
    "\n",
    "We could use the sigmoid from the chapter on computational graphs.\n",
    "However, this function has a problem which we can see if we look at the graph of its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724afc2-b707-42a2-ae39-3a2bacce2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_derivative_xs = np.arange(-10, 10, 0.01)\n",
    "sigmoid_derivative_ys = np.exp(-sigmoid_derivative_xs) / (1 + np.exp(-sigmoid_derivative_xs) ** 2)\n",
    "plt.plot(sigmoid_derivative_xs, sigmoid_derivative_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f6f73-6a95-4fae-bb57-78455d728dc4",
   "metadata": {},
   "source": [
    "The derivative of the sigmoid function is very close to $0$ outside of a relatively small input range.\n",
    "Why is this a problem?\n",
    "\n",
    "Let's say that during backpropagation the sigmoid node receives a derivative $\\frac{\\partial L}{\\partial y}$ (where $y$ is the output of the sigmoid node).\n",
    "It will then backpropagate the derivative $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z}$.\n",
    "But if $\\frac{\\partial y}{\\partial z}$ is approximately $0$, the derivative $\\frac{\\partial L}{\\partial z}$ will be approximately $0$ as well.\n",
    "The derivative (and therefore the error signal) is now lost.\n",
    "\n",
    "Just as a side note, the above explanation is an example for why it is so important to actually understand what backpropagation does on a technical level even though you will probably never need to implement it.\n",
    "It would be very hard to grasp why sigmoids are a bad idea for activation functions unless you are _actually capable of doing the math_ (luckily the math _to do_ is really not that complicated).\n",
    "This is not the last time we will encounter a problem that has its roots in the way backpropagation operates.\n",
    "\n",
    "Executive summary - we need a different activation function that does not have such a small \"good\" range of inputs. A very simple way to address this is to simply take the identity function and cut off half the input range resulting in $f(x) = \\max(0, x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e616a7-7ae1-4833-81e0-b36741b11766",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_xs = np.arange(-10, 10, 0.01)\n",
    "relu_ys = np.maximum(0, relu_xs)\n",
    "plt.plot(relu_xs, relu_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c75a6a-8994-4d3e-bc10-8e05413bdab6",
   "metadata": {},
   "source": [
    "The derivative of this function is $0$ if $x < 0$ and $1$ if $x > 0$ (we also usually set it to $0$ if $x = 0$ to ensure that the derivative is defined everywhere).\n",
    "We still lose half the input range, but the crucial point is that as long as $x > 0$ we are fine.\n",
    "\n",
    "Interestingly, this function is not called the \"most lazy activation possible\" but has the fancy name **Rectified Linear Unit** instead (ReLU for short).\n",
    "Not only is this function very ~~lazy~~ simple, but it works extremely well in practice.\n",
    "So well in fact, that ReLU and its variants are the default activation functions people go with when creating neural networks.\n",
    "\n",
    "Note that the ReLU function is also directly present in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a0d0b-ff22-4a1a-b223-d9a7f1523722",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "\n",
    "relu_xs = torch.tensor([-0.5, 0.0, 0.5])\n",
    "print(relu(relu_xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709f977-754f-4c55-903b-28880c772429",
   "metadata": {},
   "source": [
    "The most well-known variant of ReLU commonly used in NLP is the GELU activation function.\n",
    "The formula for GELU is $f(x) = x \\Phi(x)$ where $\\Phi$ is the Cumulative Distribution Function for the Gaussian Distribution.\n",
    "\n",
    "This sounds relatively complicated - but if we look at a graph, we see that GELU is simply a variant of ReLU where we have a nonzero derivate for $x < 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c69451-59b3-4ad3-87bf-900e2a20617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu = torch.nn.GELU()\n",
    "\n",
    "gelu_xs = np.arange(-10, 10, 0.01)\n",
    "gelu_ys = gelu(torch.tensor(gelu_xs))\n",
    "plt.plot(gelu_xs, gelu_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4df1f-f7d9-4f86-a52b-b86091eba178",
   "metadata": {},
   "source": [
    "## The Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0bf320-aa55-4b28-949b-e2b4069c8b39",
   "metadata": {},
   "source": [
    "The embedding layer is basically just a simple lookup table that can map IDs to high-dimensional, dense vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f98e2-3065-4703-89f9-4c1df60a000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57908be3-1a3f-4996-93fa-7e764eb55e4f",
   "metadata": {},
   "source": [
    "Let's say that we would like to retrieve the embeddings for IDs `3`, `2` and `5`.\n",
    "Then we could simply pass the tensor containing these IDs to the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21bca1-60fc-464d-b4d1-f8793180eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x = torch.tensor([3, 2, 5])\n",
    "\n",
    "emb_y = embedding_layer(emb_x)\n",
    "print(emb_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a0675-43fb-4dcb-91a2-a48c1701e9c5",
   "metadata": {},
   "source": [
    "As you can see the output contains the rows `3`, `2` and `5` from the lookup table.\n",
    "\n",
    "Note that the lookup table is a learnable parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a275063-a68b-4be7-9773-1135e5f3e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in embedding_layer.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9334b-c2cb-447a-92d6-9d5a088e8fc1",
   "metadata": {},
   "source": [
    "Embeddings are extremely useful in LLMs because they allow us to map discrete units (like words or word positions) to high-dimensional, continuous vectors such that the vectors can be learned by the LLM.\n",
    "For example, we could create _word embeddings_ in such a way that semantically similar words result in embeddings which are spatially close to each other.\n",
    "We could then pass these word embeddings to other layer that would learn further features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b890a1f-3a19-4900-84f0-52284f025817",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5827a-ee2b-4799-bee0-117513a1238f",
   "metadata": {},
   "source": [
    "As we already explained in the chapter on computational graphs, one of the biggest challenges when training neural networks is to avoid overfitting.\n",
    "We briefly talked about a few methods that can be used to accomplish this - however, we have not mentioned one of the most important methods which is called **dropout**.\n",
    "\n",
    "The idea here is to randomly drop some neurons during training to avoid neurons becoming too dependent on other specific neurons.\n",
    "This way we force neurons to learn robust features.\n",
    "\n",
    "Technically speaking, the dropout layer randomly zeroes some of the elements of the input tensor using sampling from a Bernoulli distribution with probability `p`.\n",
    "Furthermore, the outputs are normalized via scaling by a factor of `1/(1-p)`. \n",
    "\n",
    "Let's have a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca039316-adb9-4c37-90b4-8793c658fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer = nn.Dropout(p=0.5)\n",
    "\n",
    "drop_x = torch.randn(5)\n",
    "print(drop_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11541f-f692-44e7-961c-69c653952221",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_y = dropout_layer(drop_x)\n",
    "print(drop_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f55126-aa37-4a8c-b2ea-0bca7f178666",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X = torch.randn(3, 5)\n",
    "print(drop_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efb67f-09e1-4fb8-9ba4-61acec313ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_Y = dropout_layer(X)\n",
    "print(drop_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98223359-e065-42fb-ad89-4eead5d5dbd1",
   "metadata": {},
   "source": [
    "Note that the dropout layer only drops neurons during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422c2db-1ebc-44dd-8b24-7e9d50d71591",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dropout_layer.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa73b6-90da-4dc4-bf62-83b4f67f3685",
   "metadata": {},
   "source": [
    "If we set the dropout layer to evaluation mode using the `eval()` function, we will see that it no longer drops neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc7267-1b4f-4a35-a10a-0d36fa4cfb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer.eval()\n",
    "print(dropout_layer.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e76c9-f301-4a49-8032-bb8b51233196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f846707-880e-4cbb-b240-e898e8841ee5",
   "metadata": {},
   "source": [
    "If we want to set it back to training mode, we can use the `train()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796c0a2-ccdd-4938-abfd-02724cc2e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer.train()\n",
    "print(dropout_layer.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbb970-8db0-4e0a-9b1a-1a86bbc80ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dropout_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ecf3e-df70-47f5-aca5-b1131041217c",
   "metadata": {},
   "source": [
    "Note that the `train()` function does not actually _do any training_, it only _sets_ a layer or a model _to training mode_.\n",
    "\n",
    "> A common source of bugs when training models is to forget to set the model to training mode during training or to evaluation mode during evaluation, so watch out for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f7fa1-c18b-44bb-8ecb-d7bec558b944",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9093539-3f5f-4a54-b2e8-05af3982035f",
   "metadata": {},
   "source": [
    "Another important concept for stabilizing training is called **layer normalization**.\n",
    "\n",
    "This is a technique that is used to normalize inputs across features of individual samples in a batch.\n",
    "The idea is to ensure that each input to the next layer has a mean of zero and a variance of one.\n",
    "\n",
    "To accomplish this, we compute the mean and the variance across features for each sample and then normalize them.\n",
    "\n",
    "Consider the following tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f85e7c-c460-445a-b77a-e00a721d1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(batch_dim, d_in)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1eeb12-9776-40cb-bbdb-c42827b96547",
   "metadata": {},
   "source": [
    "Let's compute the mean and the variance of that tensor for every feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d9c4d-4588-48c9-be43-fadacfc359ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X.mean(dim=-1, keepdim=True)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba0522-926a-4908-b58e-5cc1b8b8a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = X.var(dim=-1, keepdim=True)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa73dfe-0898-417c-a216-60b5a8545aee",
   "metadata": {},
   "source": [
    "Note that `mean` contains the mean for every sample in the batch and `var` contains the variance for every sample in the batch.\n",
    "\n",
    "We can now normalize the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8e850-4988-467a-929a-53f650af5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = (X - mean) / torch.sqrt(var)\n",
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cdd784-be37-42ee-8fb2-c8b76a9536e0",
   "metadata": {},
   "source": [
    "If we recompute the mean and the variance of the new tensor, we will see that the mean is now zero and the variance is now one for every sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18064339-dc53-491c-a50f-9a8994c3db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.mean(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149eef2-a9d7-4920-85b4-5dd926e6ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.var(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f1e04-e7f9-49fc-8fc2-56fba72bf494",
   "metadata": {},
   "source": [
    "Commonly we also scale and shift each normalized feature using learnable parameters `scale` and `shift`.\n",
    "If we would implement a full `LayerNorm` layer from scratch it would look as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b96d89-f1ff-4d1f-9c14-278f4da1056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b627d36-e3c2-43d7-a47a-7eba4c7ea41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(d_in)\n",
    "print(layer_norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8bc88-7c35-4425-9a30-452523e6ecab",
   "metadata": {},
   "source": [
    "Let's compare our manual layer to the builtin `nn.LayerNorm` layer to see if our calculations match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82193f-8358-468f-a01d-323d43305b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm_torch = nn.LayerNorm(d_in)\n",
    "print(layer_norm_torch(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
