{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03009267-70b8-4092-b18b-3c5b646ebcd0",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac2bc5-051a-492d-be07-a402b73177c2",
   "metadata": {},
   "source": [
    "At its core, attention is a mechanism that allows a token vector to include information about the context of that token (i.e. the surrounding token vectors).\n",
    "\n",
    "As an example consider the token sequence $T_0, T_1, T_2$ (like \"an\", \"example\", \"sequence\").\n",
    "It seems sensible that token $T_2$ should have some information about $T_0$ and $T_1$ if we want to model the sequence successfully.\n",
    "\n",
    "As usual, we will need a few imports from `torch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed77978-938e-48cb-ac2d-51f9da359ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74ac46143770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37203064-ab22-4e67-81f4-9dad94b4f898",
   "metadata": {},
   "source": [
    "## Linear Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69830aee-ca82-418d-9599-dd084976e54b",
   "metadata": {},
   "source": [
    "We will continue working with our example sequence $T_0, T_1, T_2$.\n",
    "We will call the vectors that represent the tokens $\\mathbf{x_0}, \\mathbf{x_1}$ and $\\mathbf{x_2}$ respectively.\n",
    "\n",
    "Let's say that every token is represented by a vector of dimension $5$, i.e. the entire sequence is represented by a tensor of dimension $3\\times 5$.\n",
    "\n",
    "We will use a random tensor throughout this section, in reality this would be the result of the layer that precedes the attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da53e99-5f3c-4148-a688-1d82b517544d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
       "        [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674],\n",
       "        [ 0.5349,  0.8094,  1.1103, -1.6898, -0.9890]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(3, 5)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037751b8-d20d-486d-ae6b-aedd026ed286",
   "metadata": {},
   "source": [
    "Now let's say that we would like the token vectors to be able to \"look\" at each other.\n",
    "The simplest way would be to calculate averages.\n",
    "\n",
    "For example, if we would like to get the information contained in $T_0$ and $T_1$, we might compute the average of $\\mathbf{x_0}$ and $\\mathbf{x_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece16362-aa68-4668-b43b-ef470b71371a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0752,  1.1685, -0.2018,  0.3460, -0.4278])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / 2 * X[0] + 1 / 2 * X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1215d-db9f-40c4-8586-95f9d6a4b765",
   "metadata": {},
   "source": [
    "Similarly, if we would like to combine the information in $T_0, T_1$ and $T_2$ we might compute the average of $\\mathbf{x_0}, \\mathbf{x_1}$ and $\\mathbf{x_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46383781-3447-4e3d-a141-da700babc7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2284,  1.0488,  0.2356, -0.3326, -0.6148])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / 3 * X[0] + 1 / 3 * X[1] + 1 / 3 * X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0eedc5-51f1-45a9-8e90-80636adcdbb1",
   "metadata": {},
   "source": [
    "This doesn't look like a great idea, primarily because not every token is equally important for every token.\n",
    "Instead we want to have some kind of weights in our linear combinations which should be data-driven, i.e. we would like to compute arbitrary linear combinations:\n",
    "\n",
    "$w_0 \\cdot \\mathbf{x_0} + w_1 \\cdot \\mathbf{x_1} + w_2 \\cdot \\mathbf{x_2}$\n",
    "\n",
    "where the weights $w_0, w_1$ and $w_2$ should be data-driven parameters.\n",
    "\n",
    "That is, the input of a hypothetical attention layer would be a tensor containing the vectors $\\mathbf{x_0}, \\mathbf{x_1}$ and $\\mathbf{x_2}$, while the output would be another tensor containing new vectors of $\\mathbf{y_0}, \\mathbf{y_1}$ and $\\mathbf{y_2}$ that are certain linear combinations of the input vectors:\n",
    "\n",
    "$w_{00} \\cdot \\mathbf{x_0} + w_{01} \\cdot \\mathbf{x_1} + w_{02} \\cdot \\mathbf{x_2} = \\mathbf{y_0}$\n",
    "\n",
    "$w_{10} \\cdot \\mathbf{x_0} + w_{11} \\cdot \\mathbf{x_1} + w_{12} \\cdot \\mathbf{x_2} = \\mathbf{y_1}$\n",
    "\n",
    "$w_{20} \\cdot \\mathbf{x_0} + w_{21} \\cdot \\mathbf{x_1} + w_{22} \\cdot \\mathbf{x_2} = \\mathbf{y_2}$\n",
    "\n",
    "We could represent this in terms of a matrix-vector \n",
    "\n",
    "Put differently, attention \"mixes\" the input vectors together and gives us new output vectors that were able to \"communicate\" with each other in some sense.\n",
    "\n",
    "The big question is now how to compute the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494d019-fbf7-4799-b3e8-651ed244708a",
   "metadata": {},
   "source": [
    "## Naive Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc3f5a-b60c-46cb-9241-4afb125b23de",
   "metadata": {},
   "source": [
    "Let's take a stab at a very naive attention mechanism.\n",
    "The idea would be to calculate the similarity of every token with every other token.\n",
    "We could use the dot product for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2cc6ad-22a6-441c-997c-e2f386f569b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention(0, 0) = 1.4987846612930298\n",
      "attention(0, 1) = -0.12174583971500397\n",
      "attention(0, 2) = 1.265914797782898\n",
      "attention(1, 0) = -0.12174582481384277\n",
      "attention(1, 1) = 5.602515697479248\n",
      "attention(1, 2) = -0.06531322002410889\n",
      "attention(2, 0) = 1.2659146785736084\n",
      "attention(2, 1) = -0.06531322002410889\n",
      "attention(2, 2) = 6.007389068603516\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        dot_product = torch.dot(X[i], X[j])\n",
    "        print(f\"attention({i}, {j}) = {dot_product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b77401-c3e2-4dc1-8bb0-b9dba2692ac7",
   "metadata": {},
   "source": [
    "This can of course be done much more efficiently via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2bcb48-9d07-409b-a01c-3c0103536a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4988, -0.1217,  1.2659],\n",
       "        [-0.1217,  5.6025, -0.0653],\n",
       "        [ 1.2659, -0.0653,  6.0074]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = torch.matmul(X, X.transpose(0, 1))\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170feb8b-ccd7-4098-91e1-442126da2c86",
   "metadata": {},
   "source": [
    "This yields a matrix of \"attention weights\".\n",
    "We now normalize this matrix using the softmax function to get a matrix of attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a363e82-d564-4865-af9e-e170b49b6750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5025, 0.0994, 0.3981],\n",
       "        [0.0032, 0.9933, 0.0034],\n",
       "        [0.0086, 0.0023, 0.9891]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.softmax(W, dim=1)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43c4ee-89d1-4a0a-a1b3-6e1f5f31eb21",
   "metadata": {},
   "source": [
    "Now we have a matrix of \"attention scores\" indicating how much attention vector $\\mathbf{x_i}$ should pay to vector $\\mathbf{x_j}$.\n",
    "We can now compute a linear combination using data-driven weights.\n",
    "\n",
    "$w_{00} \\cdot \\mathbf{x_0} + w_{01} \\cdot \\mathbf{x_1} + w_{02} \\cdot \\mathbf{x_2} = \\mathbf{y_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c09161-5194-4e2a-86ba-7e060b78ccda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3636,  0.6064,  0.4964, -0.5111, -0.9314])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0, 0] * X[0] + W[0, 1] * X[1] + W[0, 2] * X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe682bf-6181-45d6-8986-fb5976183bc4",
   "metadata": {},
   "source": [
    "Next: $w_{10} \\cdot \\mathbf{x_0} + w_{11} \\cdot \\mathbf{x_1} + w_{12} \\cdot \\mathbf{x_2} = \\mathbf{y_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05b1a2c8-fe93-46db-a331-605733a8332a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1822,  2.1967, -0.6292,  0.4535,  0.2585])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[1, 0] * X[0] + W[1, 1] * X[1] + W[1, 2] * X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e78a68a-3c17-439c-8860-b3b409e4ca03",
   "metadata": {},
   "source": [
    "Next: $w_{20} \\cdot \\mathbf{x_0} + w_{21} \\cdot \\mathbf{x_1} + w_{22} \\cdot \\mathbf{x_2} = \\mathbf{y_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c6e25d7-f206-4f45-afa5-8c30e6deee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5315,  0.8067,  1.0987, -1.6683, -0.9873])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[2, 0] * X[0] + W[2, 1] * X[1] + W[2, 2] * X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7643d-1eaf-4382-9881-560efc28a97b",
   "metadata": {},
   "source": [
    "Again we can realize this much more efficiently via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72bcfddb-78cf-42da-a24d-5b5fee372464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3636,  0.6064,  0.4964, -0.5111, -0.9314],\n",
       "        [-0.1822,  2.1967, -0.6292,  0.4535,  0.2585],\n",
       "        [ 0.5315,  0.8067,  1.0987, -1.6683, -0.9873]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(W, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d7940-b4b4-4774-af69-6998c10ce74a",
   "metadata": {},
   "source": [
    "Unfortunately this naive attention will not work well in practice.\n",
    "The reason is that we need to be able to differentiate between \"information that a token vector represents\" and \"information that a token vector is interested in\".\n",
    "\n",
    "For example, if a token vector encodes that it is the subject of a sentence it will currently pay high attention to other subjects of the sentence (mostly to itself).\n",
    "Instead, it should probably pay attention to e.g. token vectors that encode predicates of a sentence, articles etc.\n",
    "\n",
    "We note that this a hand-wavy intuition and token vectors represent mostly inscrutable high-dimensional concepts that often have no real analogy in linguistics.\n",
    "Despite this, the overall idea still works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15c454-86d0-4157-afbb-f030ecf2cec9",
   "metadata": {},
   "source": [
    "## Key and Query Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93a89e-7fce-47cc-9100-85a67147365e",
   "metadata": {},
   "source": [
    "We now introduce the first important component of the real self-attention mechanism - the **key vectors** and **query vectors**.\n",
    "\n",
    "Every token vector generates a key vector and a query vector.\n",
    "The key vector indicates the information that the token represents and the query vector contains the information the token is interested in.\n",
    "\n",
    "To continue our informal example, a token might have the key vector \"I am the subject of the sentence\" and the query vector \"I am interested in the predicate of the sentence\".\n",
    "Of course in reality key and query vectors will not be this interpretable and will represent some instructable high-dimensional concepts that the language model learned during training.\n",
    "\n",
    "To borrow from databases:\n",
    "\n",
    "A \"query\" is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
    "\n",
    "The \"key\" is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match with the query.\n",
    "\n",
    "The key vectors and query vectors are computed from the token vectors via simple linear layers.\n",
    "\n",
    "For our example, we will create random linear layers - in reality this would be parameters that our neural network would have to learn.\n",
    "Let's call the matrix that will produce the key vectors $W_K$ and the matrix that will produce the query vectors $W_Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3cecfd8-8ddc-4e88-918b-9bc4ef64fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = torch.randn(5, 4)\n",
    "W_Q = torch.randn(5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d95fb-5995-4d48-8f4e-036a947e5746",
   "metadata": {},
   "source": [
    "We can now compute the key vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a18d2cca-54f9-4f67-993e-505fe0e3d9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6023, -0.7260,  1.1799,  0.2383])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(X[0], W_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "327fea18-94c3-41f4-b52e-cc740a4023d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6023, -0.7260,  1.1799,  0.2383],\n",
       "        [-0.6521,  4.4224, -3.7460, -1.2657],\n",
       "        [-0.7106, -4.3429,  4.2984, -2.3664]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.matmul(X, W_K)\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca984a-554a-47ba-90c8-84d5f4b5043e",
   "metadata": {},
   "source": [
    "We can also compute the query vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b5fd610-0068-4b92-ace4-ca11a9f3410b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6964,  1.3355, -0.5133,  0.0674],\n",
       "        [ 1.6595, -0.4445, -0.1917,  1.7729],\n",
       "        [-0.1650, -2.9899, -3.8893,  1.2756]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.matmul(X, W_Q)\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c0c54-6dd5-4d73-909d-b9cb2b943e03",
   "metadata": {},
   "source": [
    "Now we will compute the attention scores in a similar way as before.\n",
    "The big difference is that instead of computing the similarity of the token vectors with each other, we will _compute the similarity between the key vectors and the query vectors_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fa0e842-efc1-4ce9-a254-2b85f390c201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5809,  8.8498, -6.9600],\n",
       "        [ 1.5185, -4.5739, -4.2686],\n",
       "        [-2.2135, -0.1601, -6.6347]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = torch.matmul(Q, K.transpose(0, 1))\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede41f10-199d-4b27-b497-cd140fa757e6",
   "metadata": {},
   "source": [
    "A minor, but important technical detail is that we will need to scale the attention scores to avoid numerical instability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0db4caf7-50b0-4656-a8b8-36dc455efcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2905,  4.4249, -3.4800],\n",
       "        [ 0.7593, -2.2869, -2.1343],\n",
       "        [-1.1067, -0.0801, -3.3173]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = S / (4**0.5)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fed7b41d-5678-42c2-9d20-f94e74ac85b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2830e-03, 9.9635e-01, 3.6758e-04],\n",
       "        [9.0669e-01, 4.3103e-02, 5.0212e-02],\n",
       "        [2.5632e-01, 7.1558e-01, 2.8102e-02]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.softmax(S, dim=1)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cec72e-78f4-4664-8b87-1d6ebf523347",
   "metadata": {},
   "source": [
    "We still have one problem - right now our tokens can \"look into the future\".\n",
    "For example token $T_0$ can \"see\" $T_1$ and $T_2$.\n",
    "\n",
    "But during inference time, this will not be possible - we can't take into account tokens that haven't been generated yet.\n",
    "Therefore we should disable this during training as well.\n",
    "\n",
    "We will \"mask\" the attention scores of future tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dcf1a8b-91c9-4fe0-adf4-6e44a247bd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(S.shape[0], S.shape[0]))\n",
    "mask\n",
    "#S = S.masked_fill(trimat == 0, float(\"-inf\"))\n",
    "#S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "621281eb-b511-430e-b84a-71343193da87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2905,    -inf,    -inf],\n",
       "        [ 0.7593, -2.2869,    -inf],\n",
       "        [-1.1067, -0.0801, -3.3173]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_S = S.masked_fill(mask == 0, float(\"-inf\"))\n",
    "masked_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b70448c-3c90-4100-a2ee-a9e8559519d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6452,    -inf,    -inf],\n",
       "        [ 0.3796, -1.1435,    -inf],\n",
       "        [-0.5534, -0.0400, -1.6587]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_S = masked_S / (4 ** 0.5)\n",
    "masked_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "912519b1-c8f9-4bc4-8da6-7ef3b89cf4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.8210, 0.1790, 0.0000],\n",
       "        [0.3331, 0.5566, 0.1103]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_W = torch.softmax(masked_S, dim=1)\n",
    "masked_W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd6125-1af4-47fc-8fd1-6085b43746cc",
   "metadata": {},
   "source": [
    "Additionally, we can apply Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "015622d6-6ded-4e50-b8a3-5ff3c1e9136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb4ae0c7-4f5c-4044-b849-27b731402349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.1132, 0.2206]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(masked_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4b5a4-f270-4388-bbfb-bb42c14e5764",
   "metadata": {},
   "source": [
    "## Value Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ca22c-8e0c-4292-ac17-3eb8bf2c19e7",
   "metadata": {},
   "source": [
    "Right now, we would apply the scores to the token vectors directly.\n",
    "\n",
    "It turns out that the attention mechanism performs even better if we introduce one more indirection and calculate **value vectors** from the token vectors and only then apply the scores.\n",
    "\n",
    "To borrow from databases:\n",
    "\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.\n",
    "\n",
    "The computation of the value vectors works exactly like the computation of the key and query vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b2da458-19a1-4044-bf6a-a1257ec0888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V = torch.randn(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cda1bd97-ec1d-427d-8d0a-ca7af37509e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9285,  0.3301,  1.8359, -1.3448],\n",
       "        [ 0.4676, -0.1512, -0.5678,  0.8648],\n",
       "        [ 0.6143,  2.6772, -1.3256, -3.2423]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.matmul(X, W_V)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98cec7b8-84ea-4033-ba46-28308c5ad7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4630, -0.1485, -0.5602,  0.8561],\n",
       "        [-0.7909,  0.4272,  1.5735, -1.3448],\n",
       "        [ 0.1138,  0.0517,  0.0270,  0.1831]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = torch.matmul(W, V)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e809dad-4ee8-4ee4-8649-3342edaeac20",
   "metadata": {},
   "source": [
    "## Implementing a Self-Attention Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618288d-7829-4f69-809e-baab2063a2ba",
   "metadata": {},
   "source": [
    "Note that we need to take care because we will also have a batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c21f4006-f8c6-4ee6-87f4-d6e7b3033054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4249,  0.9442, -0.1849,  1.0608,  0.2083],\n",
       "         [-0.5778,  0.3255, -0.8146, -0.7599, -2.0461],\n",
       "         [-1.5295,  0.4049,  0.6319,  0.3125,  1.9892]],\n",
       "\n",
       "        [[-0.4611, -0.0639, -1.3667,  0.3298, -0.9827],\n",
       "         [ 0.3018,  0.1787,  0.4097, -1.5754,  2.2508],\n",
       "         [ 1.0012,  1.3642,  0.6333,  0.4050,  0.3416]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(2, 3, 5)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bf436ff6-7b42-49e6-b3e2-cf57b74bedb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([2, 3, 4]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q = nn.Linear(5, 4, bias=False)\n",
    "W_K = nn.Linear(5, 4, bias=False)\n",
    "W_V = nn.Linear(5, 4, bias=False)\n",
    "K = W_K(X)\n",
    "Q = W_Q(X)\n",
    "V = W_V(X)\n",
    "\n",
    "K.shape, Q.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27987a5a-174a-4ca0-88d8-2e66d41b1fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1268,  0.1442,  0.5098],\n",
       "         [-0.1395, -0.4737, -0.1583],\n",
       "         [-0.0595,  0.8114,  0.3408]],\n",
       "\n",
       "        [[ 0.0802,  0.3535,  0.3662],\n",
       "         [-0.1526,  0.1022, -0.3185],\n",
       "         [ 0.0630,  0.1827, -0.6134]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = torch.matmul(Q, K.transpose(1, 2))\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1eac66c5-9460-4e19-a26b-c6c379c87297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3, 3))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04e9fa15-82e9-4f6b-a96d-b7a8b568fb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1268,    -inf,    -inf],\n",
       "         [-0.1395, -0.4737,    -inf],\n",
       "         [-0.0595,  0.8114,  0.3408]],\n",
       "\n",
       "        [[ 0.0802,    -inf,    -inf],\n",
       "         [-0.1526,  0.1022,    -inf],\n",
       "         [ 0.0630,  0.1827, -0.6134]]], grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_S = S.masked_fill(mask == 0, float(\"-inf\"))\n",
    "masked_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1791d4b7-aa5b-4e8d-9c7c-86f44b2927e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8666f-464e-49f3-90f0-3d0a04ea5c5d",
   "metadata": {},
   "source": [
    "Also softmax along dim -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac698f3-fc88-4521-9069-cbed9b9eadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, c_len, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(c_len, c_len)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, num_tokens, d_in = X.shape\n",
    "        \n",
    "        Q = self.W_Q(X)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "\n",
    "        S = torch.matmul(Q, K.transpose(1, 2))\n",
    "        masked_S = S.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        masked_S = masked_S / K.shape[-1] ** 0.5\n",
    "        W = torch.softmax(masked_S, dim=-1)\n",
    "\n",
    "        W = self.dropout(W)\n",
    "        \n",
    "        R = torch.matmul(W, V)\n",
    "\n",
    "        return R\n",
    "\n",
    "layer = SelfAttention(d_in=5, d_out=4, c_len=3, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb537cff-c034-4b9b-8d1c-24069cab0c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(2, 3, 5)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6703f16-b43b-4686-a0f8-350a89d4a16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = layer(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2f599-70cf-4323-be74-cdce3b235c40",
   "metadata": {},
   "source": [
    "Note that in practice the number of input dimensions and output dimensions will usually be the same, i.e. the self-attention layer doesn't change the dimensionality of the tensor, it only \"mixes\" the elements of the tensor together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e34000-febd-4a74-bf89-8fa8ec460373",
   "metadata": {},
   "source": [
    "## Multi - Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa2dc1-85c6-4e80-a646-bce583604d67",
   "metadata": {},
   "source": [
    "The final piece we are missing is the multi-head attention.\n",
    "\n",
    "Basically instead of having a single self-attention layer, we use multiple self-attention layers, each with its own weights and combine their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f3dc4c6-d04a-4f18-bcbf-b2d550e70364",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 5\n",
    "d_out = 4\n",
    "c_len = 3\n",
    "dropout = 0.5\n",
    "\n",
    "n_heads = 2\n",
    "heads = [SelfAttention(d_in, d_out, c_len, dropout) for _ in range(n_heads)]\n",
    "\n",
    "result = [head(X) for head in heads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "819e7135-5998-4134-ab95-a1068b10f5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4]), torch.Size([2, 3, 4])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[head_out.shape for head_out in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ac04c-3923-4da2-8c7c-bea8d887ebcb",
   "metadata": {},
   "source": [
    "Next we combine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d36c1d1-cc24-4306-b2a6-9ef25ea4211a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_out_combined = torch.cat(result, dim=-1)\n",
    "head_out_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39719fa7-d528-412b-b651-01a29a9102c0",
   "metadata": {},
   "source": [
    "Note that in practice we don't want the multi-head attention layer to blow up the size of the tensor.\n",
    "\n",
    "Therefore we reduce the value of $d_{out}$ and set it to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a55a581-485c-492d-867c-f4f66cdf0e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_out = 2\n",
    "heads = [SelfAttention(d_in, d_out, c_len, dropout) for _ in range(n_heads)]\n",
    "\n",
    "head_out_combined = torch.cat([head(X) for head in heads], dim=-1)\n",
    "head_out_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf042e-96c8-4313-a278-906433568855",
   "metadata": {},
   "source": [
    "While this technically already works, it is computationally expensive since we process the heads sequentially.\n",
    "Instead we can process them in parallel by computing the outputs for all attention heads at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec852e49-0aaa-4d6e-9a77-3e3d0925e3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d432e70-d68f-4a27-aacf-9227784f61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = d_out // n_heads\n",
    "\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, n_tokens, d_in = X.shape\n",
    "\n",
    "        K = self.W_K(X)\n",
    "        Q = self.W_Q(X)\n",
    "        V = self.W_V(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "575db38b-04a2-486f-9290-e0c335dbfcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 3\n",
    "d_in = 4\n",
    "d_out = 4\n",
    "n_heads = 2\n",
    "head_dim = d_out // n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff06f855-b36b-4bb3-9377-87e2f042c935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(2, n_tokens, d_in)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ada7fe4f-a9d7-4287-bcaa-a6b656f26961",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = torch.randn(d_out,  d_in)\n",
    "W_Q = torch.randn(d_out, d_in)\n",
    "W_V = torch.randn(d_out, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41af461e-8164-478c-b986-84d41a2a2dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.matmul(X, W_K)\n",
    "Q = torch.matmul(X, W_Q)\n",
    "K.shape, Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c695577f-f094-4c62-9fbe-b1c72e33529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_view = K.view(2, n_tokens, n_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a525653-0c04-4cf8-8bdf-741ed500aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ccb9e2b-4ef9-4ff4-8a0e-92ba70868897",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_view = Q.view(2, n_tokens, n_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e344a48-73a5-456e-8f9a-ec0f0677beed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abefcb8d-c29c-4b46-a9ae-5bac4b8a9fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_view = K_view.transpose(1, 2)\n",
    "K_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "732bf14a-0dd4-4777-94e9-d6cef902952e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_view = Q_view.transpose(1, 2)\n",
    "Q_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e64e871-cc70-4779-a28e-4ca4d3db107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = torch.matmul(Q_view, K_view.transpose(2, 3))\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2eaa7503-94e9-4e63-9b0c-48ff02a9309e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.7366,  -1.1301,  -2.7070],\n",
       "         [  2.1529,   0.7935,   1.6183],\n",
       "         [ -6.5526,  -2.3705,  -4.6866]],\n",
       "\n",
       "        [[ -0.3216,   3.0144, -13.2677],\n",
       "         [ -0.6322,  -1.1477,  -0.0831],\n",
       "         [ -0.2841,   1.0993,  -5.9737]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fb116f6-fc52-4bbb-b02b-e58c62e9c3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(n_tokens, n_tokens))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35ca05c7-dfe5-464a-9f40-71e5c34a98cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.7366,    -inf,    -inf],\n",
       "         [ 2.1529,  0.7935,    -inf],\n",
       "         [-6.5526, -2.3705, -4.6866]],\n",
       "\n",
       "        [[-0.3216,    -inf,    -inf],\n",
       "         [-0.6322, -1.1477,    -inf],\n",
       "         [-0.2841,  1.0993, -5.9737]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_S = S.masked_fill(mask == 0, float(\"-inf\"))\n",
    "masked_S[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6207c5c4-e8ec-47e1-8a55-dd3912a64d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6841,    -inf,    -inf],\n",
       "         [ 0.5382,  0.1984,    -inf],\n",
       "         [-1.6381, -0.5926, -1.1716]],\n",
       "\n",
       "        [[-0.0804,    -inf,    -inf],\n",
       "         [-0.1581, -0.2869,    -inf],\n",
       "         [-0.0710,  0.2748, -1.4934]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_S = masked_S / K.shape[-1] ** 0.5\n",
    "masked_S[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdff5562-8623-4bfa-8135-a350049bb3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000],\n",
       "         [0.5842, 0.4158, 0.0000],\n",
       "         [0.1838, 0.5230, 0.2931]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000],\n",
       "         [0.5322, 0.4678, 0.0000],\n",
       "         [0.3767, 0.5324, 0.0908]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.softmax(masked_S, dim=-1)\n",
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f46678c-3547-49b8-b544-3e130b11a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply droput normally\n",
    "\n",
    "V = torch.matmul(X, W_V) \n",
    "V_view = V.view(2, n_tokens, n_heads, head_dim)\n",
    "V_view = V_view.transpose(1, 2)\n",
    "R = torch.matmul(W, V_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8218f5d3-ae89-496f-bce3-f420b3c2caad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 3, 2]), torch.Size([2, 2, 3, 3]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_view.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f069a494-2cb3-40fa-bbbf-979a3dc3c786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cf441b65-f74b-4ce5-8878-bfa2efde39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b46aad78-3957-490d-aca8-c5c360131692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f99cb60d-d3d9-4ffe-874b-f386bcebe894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_combined = R.contiguous().view(2, n_tokens, d_out)\n",
    "R_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f876d-8071-4eab-8154-cf64841f47dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
