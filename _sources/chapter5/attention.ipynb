{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03009267-70b8-4092-b18b-3c5b646ebcd0",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac2bc5-051a-492d-be07-a402b73177c2",
   "metadata": {},
   "source": [
    "At its core, attention is a mechanism that allows a token vector to include information about the context of that token (i.e. the surrounding token vectors).\n",
    "This way the token vector can \"pay attention\" to the surrounding token vectors.\n",
    "\n",
    "As an example consider the token sequence $T_0, T_1, T_2$ (like \"an\", \"example\", \"sequence\").\n",
    "It seems sensible that token $T_2$ should have some information about $T_0$ and $T_1$ if we want to model the sequence successfully.\n",
    "\n",
    "As usual, we will need a few imports from `torch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed77978-938e-48cb-ac2d-51f9da359ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x753d9d5810d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37203064-ab22-4e67-81f4-9dad94b4f898",
   "metadata": {},
   "source": [
    "## Linear Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69830aee-ca82-418d-9599-dd084976e54b",
   "metadata": {},
   "source": [
    "We will continue working with our example sequence $T_0, T_1, T_2$.\n",
    "We will call the vectors that represent the tokens $\\mathbf{x_0}, \\mathbf{x_1}$ and $\\mathbf{x_2}$ respectively.\n",
    "\n",
    "Let's say that every token is represented by a vector of dimension $5$, i.e. the entire sequence is represented by a tensor of dimension $3\\times 5$.\n",
    "\n",
    "We will use a random tensor throughout this section, in reality the tensor would be the result of the layer that comes before the attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da53e99-5f3c-4148-a688-1d82b517544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
      "        [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674],\n",
      "        [ 0.5349,  0.8094,  1.1103, -1.6898, -0.9890]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(3, 5)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037751b8-d20d-486d-ae6b-aedd026ed286",
   "metadata": {},
   "source": [
    "Now let's say that we would like the token vectors to be able to \"pay attention\" to each other.\n",
    "The simplest way to accomplish this would be to calculate averages of the token vectors.\n",
    "\n",
    "For example, if we would like to get the information contained in $T_0$ and $T_1$, we might compute the average of $\\mathbf{x_0}$ and $\\mathbf{x_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece16362-aa68-4668-b43b-ef470b71371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0752,  1.1685, -0.2018,  0.3460, -0.4278])\n"
     ]
    }
   ],
   "source": [
    "avg = 1 / 2 * X[0] + 1 / 2 * X[1]\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1215d-db9f-40c4-8586-95f9d6a4b765",
   "metadata": {},
   "source": [
    "Similarly, if we would like to combine the information in $T_0, T_1$ and $T_2$ we might compute the average of $\\mathbf{x_0}, \\mathbf{x_1}$ and $\\mathbf{x_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46383781-3447-4e3d-a141-da700babc7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2284,  1.0488,  0.2356, -0.3326, -0.6148])\n"
     ]
    }
   ],
   "source": [
    "avg = 1 / 3 * X[0] + 1 / 3 * X[1] + 1 / 3 * X[2]\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0eedc5-51f1-45a9-8e90-80636adcdbb1",
   "metadata": {},
   "source": [
    "This doesn't look like a great idea, primarily because not every token is equally important for every token.\n",
    "For example, if we have the sentence \"The bat flew out of the cave\", then \"bat\" should probably pay more attention to \"cave\" than to \"of\".\n",
    "\n",
    "Therefore, we want to have _data-driven_ weights in our linear combination, i.e. we would like to compute arbitrary linear combinations:\n",
    "\n",
    "$w_0 \\cdot \\mathbf{x_0} + w_1 \\cdot \\mathbf{x_1} + w_2 \\cdot \\mathbf{x_2}$\n",
    "\n",
    "where the weights $w_0, w_1$ and $w_2$ are data-driven parameters.\n",
    "\n",
    "That is, the input of a hypothetical attention layer would be a tensor containing the vectors $\\mathbf{x_0}, \\mathbf{x_1}$ and $\\mathbf{x_2}$, while the output would be another tensor containing new vectors of $\\mathbf{y_0}, \\mathbf{y_1}$ and $\\mathbf{y_2}$ that are certain linear combinations of the input vectors:\n",
    "\n",
    "$w_{00} \\cdot \\mathbf{x_0} + w_{01} \\cdot \\mathbf{x_1} + w_{02} \\cdot \\mathbf{x_2} = \\mathbf{y_0}$\n",
    "\n",
    "$w_{10} \\cdot \\mathbf{x_0} + w_{11} \\cdot \\mathbf{x_1} + w_{12} \\cdot \\mathbf{x_2} = \\mathbf{y_1}$\n",
    "\n",
    "$w_{20} \\cdot \\mathbf{x_0} + w_{21} \\cdot \\mathbf{x_1} + w_{22} \\cdot \\mathbf{x_2} = \\mathbf{y_2}$\n",
    "\n",
    "Note that both $\\mathbf{x_0}, \\mathbf{x_1}, \\mathbf{x_2}$ and $\\mathbf{y_0}, \\mathbf{y_1}, \\mathbf{y_2}$ are representations of the token sequence $T_0, T_1, T_2$, but $\\mathbf{y_0}, \\mathbf{y_1}, \\mathbf{y_2}$ is in some sense better than $\\mathbf{x_0}, \\mathbf{x_1}, \\mathbf{x_2}$ because the token vectors have more _context_.\n",
    "\n",
    "Put differently, attention \"mixes\" the input vectors together and gives us new output vectors that were able to \"communicate\" with each other in some sense.\n",
    "\n",
    "The big question is now how to compute the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494d019-fbf7-4799-b3e8-651ed244708a",
   "metadata": {},
   "source": [
    "## Naive Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc3f5a-b60c-46cb-9241-4afb125b23de",
   "metadata": {},
   "source": [
    "Let's take a stab at a very naive attention mechanism.\n",
    "The idea would be to calculate the similarity of every token with every other token.\n",
    "We could use the dot product for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2cc6ad-22a6-441c-997c-e2f386f569b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention(0, 0) = 1.5\n",
      "attention(0, 1) = -0.12\n",
      "attention(0, 2) = 1.27\n",
      "attention(1, 0) = -0.12\n",
      "attention(1, 1) = 5.6\n",
      "attention(1, 2) = -0.07\n",
      "attention(2, 0) = 1.27\n",
      "attention(2, 1) = -0.07\n",
      "attention(2, 2) = 6.01\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        dot_product = torch.dot(X[i], X[j])\n",
    "        print(f\"attention({i}, {j}) = {round(dot_product.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b77401-c3e2-4dc1-8bb0-b9dba2692ac7",
   "metadata": {},
   "source": [
    "This can of course be done much more efficiently via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b2bcb48-9d07-409b-a01c-3c0103536a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4988, -0.1217,  1.2659],\n",
      "        [-0.1217,  5.6025, -0.0653],\n",
      "        [ 1.2659, -0.0653,  6.0074]])\n"
     ]
    }
   ],
   "source": [
    "S = torch.matmul(X, X.transpose(0, 1))\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170feb8b-ccd7-4098-91e1-442126da2c86",
   "metadata": {},
   "source": [
    "This yields a matrix of **attention scores**.\n",
    "It would clearly be benefical if the attention scores would be between $0$ and $1$ and they would sum to $1$.\n",
    "\n",
    "We can accomplish this using the softmax function, yielding a matrix of **attention weights**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a363e82-d564-4865-af9e-e170b49b6750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5025, 0.0994, 0.3981],\n",
      "        [0.0032, 0.9933, 0.0034],\n",
      "        [0.0086, 0.0023, 0.9891]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.softmax(S, dim=1)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43c4ee-89d1-4a0a-a1b3-6e1f5f31eb21",
   "metadata": {},
   "source": [
    "Now we have a matrix of \"attention weights\" indicating how much attention vector $\\mathbf{x_i}$ should pay to vector $\\mathbf{x_j}$.\n",
    "These are our data-driven parameters - we can now compute the linear combination using these attention weights.\n",
    "\n",
    "Let's start with $\\mathbf{y_0}$.\n",
    "Remember that $\\mathbf{y_0} = w_{00} \\cdot \\mathbf{x_0} + w_{01} \\cdot \\mathbf{x_1} + w_{02} \\cdot \\mathbf{x_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c09161-5194-4e2a-86ba-7e060b78ccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3636,  0.6064,  0.4964, -0.5111, -0.9314])\n"
     ]
    }
   ],
   "source": [
    "y_0 = W[0, 0] * X[0] + W[0, 1] * X[1] + W[0, 2] * X[2]\n",
    "print(y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe682bf-6181-45d6-8986-fb5976183bc4",
   "metadata": {},
   "source": [
    "Next we compute $\\mathbf{y_1}$.\n",
    "Remember that $\\mathbf{y_1} = w_{10} \\cdot \\mathbf{x_0} + w_{11} \\cdot \\mathbf{x_1} + w_{12} \\cdot \\mathbf{x_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b1a2c8-fe93-46db-a331-605733a8332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1822,  2.1967, -0.6292,  0.4535,  0.2585])\n"
     ]
    }
   ],
   "source": [
    "y_1 = W[1, 0] * X[0] + W[1, 1] * X[1] + W[1, 2] * X[2]\n",
    "print(y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e78a68a-3c17-439c-8860-b3b409e4ca03",
   "metadata": {},
   "source": [
    "Finally, we compute $\\mathbf{y_2}$.\n",
    "Remember that $\\mathbf{y_2} = w_{20} \\cdot \\mathbf{x_0} + w_{21} \\cdot \\mathbf{x_1} + w_{22} \\cdot \\mathbf{x_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6e25d7-f206-4f45-afa5-8c30e6deee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5315,  0.8067,  1.0987, -1.6683, -0.9873])\n"
     ]
    }
   ],
   "source": [
    "y_2 = W[2, 0] * X[0] + W[2, 1] * X[1] + W[2, 2] * X[2]\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7643d-1eaf-4382-9881-560efc28a97b",
   "metadata": {},
   "source": [
    "Again we can realize this much more efficiently via matrix multiplication.\n",
    "For this, we define the matrix $W$ to be the matrix containing the weights, the $X$ the matrix where the row $i$ is the vector $\\mathbf{x_i}$ and $Y$ the matrix where the row $i$ is the vector $\\mathbf{y_i}$.\n",
    "\n",
    "Then we have $Y = WX$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72bcfddb-78cf-42da-a24d-5b5fee372464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3636,  0.6064,  0.4964, -0.5111, -0.9314],\n",
      "        [-0.1822,  2.1967, -0.6292,  0.4535,  0.2585],\n",
      "        [ 0.5315,  0.8067,  1.0987, -1.6683, -0.9873]])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.matmul(W, X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d7940-b4b4-4774-af69-6998c10ce74a",
   "metadata": {},
   "source": [
    "We made some good progress!\n",
    "Theoretically, we could try to use this attention as is, however, unfortunately this naive attention will not work well in practice.\n",
    "The reason is that we need to be able to differentiate between \"information that a token vector represents\" and \"information that a token vector is interested in\".\n",
    "\n",
    "For example, if a token vector encodes that it is the subject of a sentence it will currently pay high attention to other subjects of the sentence (mostly to itself).\n",
    "Instead, it should probably pay attention to e.g. token vectors that encode predicates of a sentence, articles etc.\n",
    "\n",
    "We note that this a hand-wavy intuition and token vectors represent mostly inscrutable high-dimensional concepts that often have no real analogy in linguistics.\n",
    "Despite this, the overall idea of differentiating etween \"information that a token vector represents\" and \"information that a token vector is interested in\" in practice works better than our current naive attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15c454-86d0-4157-afbb-f030ecf2cec9",
   "metadata": {},
   "source": [
    "## Key and Query Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93a89e-7fce-47cc-9100-85a67147365e",
   "metadata": {},
   "source": [
    "We now introduce the first important component of the real self-attention mechanism - the **key vectors** and **query vectors**.\n",
    "\n",
    "Every token vector generates a key vector and a query vector.\n",
    "The key vector represents the aforementioned \"information that the token represents\" and the query vector represents the aforementioned \"information the token is interested in\".\n",
    "\n",
    "To continue our informal example, a token might have the key vector \"I am the subject of the sentence\" and the query vector \"I am interested in the predicate of the sentence\".\n",
    "Again, in reality, key and query vectors will not be this interpretable and will represent some instructable high-dimensional concepts that the language model learned during training.\n",
    "\n",
    "We can also use a different hand-wavy intuition to understand key and query vectors by borrowing concepts from databases.\n",
    "\n",
    "Here a \"query\" would be analogous to a search query in a database.\n",
    "It represents the current token the model is trying to understand.\n",
    "The query is then to probe (i.e. to \"query\") the other parts of the input sequence to determine how much attention to pay to them.\n",
    "\n",
    "The \"key\" is like a database key.\n",
    "In the attention mechanism, each token then has an associated key which are used to match with the query.\n",
    "\n",
    "Basically, the query is used to look up the keys and determine how important they are for the query.\n",
    "\n",
    "The way we compute the key vectors and query vectors from the token vectors is surprisingly simple - we do this via linear layers.\n",
    "\n",
    "For our example, we will create random linear layers - in reality these would be parameters that our neural network would have to learn.\n",
    "Let's call the matrix that will produce the key vectors $W_K$ and the matrix that will produce the query vectors $W_Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cecfd8-8ddc-4e88-918b-9bc4ef64fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = torch.randn(5, 4)\n",
    "W_Q = torch.randn(5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d95fb-5995-4d48-8f4e-036a947e5746",
   "metadata": {},
   "source": [
    "We can now compute the key vectors.\n",
    "Here is how we would compute key vector $\\mathbf{k_0}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a18d2cca-54f9-4f67-993e-505fe0e3d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6023, -0.7260,  1.1799,  0.2383])\n"
     ]
    }
   ],
   "source": [
    "k_0 = torch.matmul(X[0], W_K)\n",
    "print(k_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209393d-a895-4db5-ab5f-94aabf8ed925",
   "metadata": {},
   "source": [
    "To compute all key vectors at the same time, we can again use matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327fea18-94c3-41f4-b52e-cc740a4023d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6023, -0.7260,  1.1799,  0.2383],\n",
      "        [-0.6521,  4.4224, -3.7460, -1.2657],\n",
      "        [-0.7106, -4.3429,  4.2984, -2.3664]])\n"
     ]
    }
   ],
   "source": [
    "K = torch.matmul(X, W_K)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca984a-554a-47ba-90c8-84d5f4b5043e",
   "metadata": {},
   "source": [
    "We can also compute the query vectors. Here is how we would compute query vector $\\mathbf{q_0}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca4d174-3123-4b35-bf67-b9f7da86691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6964,  1.3355, -0.5133,  0.0674])\n"
     ]
    }
   ],
   "source": [
    "q_0 = torch.matmul(X[0], W_Q)\n",
    "print(q_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61117d87-a4f9-4359-9e0d-3130d08a71d9",
   "metadata": {},
   "source": [
    "To compute all query vectors at the same time, we can use - you guessed it - matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b5fd610-0068-4b92-ace4-ca11a9f3410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6964,  1.3355, -0.5133,  0.0674],\n",
      "        [ 1.6595, -0.4445, -0.1917,  1.7729],\n",
      "        [-0.1650, -2.9899, -3.8893,  1.2756]])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.matmul(X, W_Q)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c0c54-6dd5-4d73-909d-b9cb2b943e03",
   "metadata": {},
   "source": [
    "Now we will compute the attention scores in a similar way as before.\n",
    "The big difference is that instead of computing the similarity of the token vectors with each other, we will _compute the similarity between the key vectors and the query vectors_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fa0e842-efc1-4ce9-a254-2b85f390c201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5809,  8.8498, -6.9600],\n",
      "        [ 1.5185, -4.5739, -4.2686],\n",
      "        [-2.2135, -0.1601, -6.6347]])\n"
     ]
    }
   ],
   "source": [
    "S = torch.matmul(Q, K.transpose(0, 1))\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede41f10-199d-4b27-b497-cd140fa757e6",
   "metadata": {},
   "source": [
    "A minor, but important technical detail is that we will need to scale the attention scores to avoid numerical instability.\n",
    "For boring mathematical reasons that we will not dive into right now, we scale by a factor of $\\sqrt{d}$ where $d$ is the dimension of a key/query vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db4caf7-50b0-4656-a8b8-36dc455efcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2905,  4.4249, -3.4800],\n",
      "        [ 0.7593, -2.2869, -2.1343],\n",
      "        [-1.1067, -0.0801, -3.3173]])\n"
     ]
    }
   ],
   "source": [
    "S_scaled = S / (4**0.5)\n",
    "print(S_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9b80f-9f92-4f02-bbf3-b1b94a24ffaa",
   "metadata": {},
   "source": [
    "Now again we compute the attention weights from the scaled attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fed7b41d-5678-42c2-9d20-f94e74ac85b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0857e-05, 9.9999e-01, 1.3611e-07],\n",
      "        [9.9470e-01, 2.2480e-03, 3.0506e-03],\n",
      "        [1.1356e-01, 8.8508e-01, 1.3650e-03]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.softmax(S, dim=1)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cec72e-78f4-4664-8b87-1d6ebf523347",
   "metadata": {},
   "source": [
    "We still have one problem - right now our tokens can \"look into the future\".\n",
    "For example, token $T_0$ can \"see\" $T_1$ and $T_2$.\n",
    "\n",
    "But during inference time, this will not be possible - we can't take into account tokens that haven't been generated yet.\n",
    "Therefore we should disable this during training as well.\n",
    "\n",
    "We will \"mask\" the attention scores of future tokens.\n",
    "\n",
    "To do this, we define the following mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dcf1a8b-91c9-4fe0-adf4-6e44a247bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(S.shape[0], S.shape[0]))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986289dd-8fbd-4730-94b4-3494b30160fe",
   "metadata": {},
   "source": [
    "Next, we set all the scores where `mask == 0` to `-inf`.\n",
    "That way, when we do the `softmax`, these entries will be set to `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621281eb-b511-430e-b84a-71343193da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5809,    -inf,    -inf],\n",
      "        [ 1.5185, -4.5739,    -inf],\n",
      "        [-2.2135, -0.1601, -6.6347]])\n"
     ]
    }
   ],
   "source": [
    "S_masked = S.masked_fill(mask == 0, float(\"-inf\"))\n",
    "print(S_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54108f9d-21aa-405b-bf09-204a422aac74",
   "metadata": {},
   "source": [
    "Again, we need to normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b70448c-3c90-4100-a2ee-a9e8559519d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2905,    -inf,    -inf],\n",
      "        [ 0.7593, -2.2869,    -inf],\n",
      "        [-1.1067, -0.0801, -3.3173]])\n"
     ]
    }
   ],
   "source": [
    "S_masked_scaled = S_masked / (4 ** 0.5)\n",
    "print(S_masked_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "912519b1-c8f9-4bc4-8da6-7ef3b89cf4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.9546, 0.0454, 0.0000],\n",
      "        [0.2563, 0.7156, 0.0281]])\n"
     ]
    }
   ],
   "source": [
    "W_masked = torch.softmax(S_masked_scaled, dim=1)\n",
    "print(W_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd6125-1af4-47fc-8fd1-6085b43746cc",
   "metadata": {},
   "source": [
    "Additionally, it is common to apply dropout at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "015622d6-6ded-4e50-b8a3-5ff3c1e9136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb4ae0c7-4f5c-4044-b849-27b731402349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.5126, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(W_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4b5a4-f270-4388-bbfb-bb42c14e5764",
   "metadata": {},
   "source": [
    "## Value Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ca22c-8e0c-4292-ac17-3eb8bf2c19e7",
   "metadata": {},
   "source": [
    "Right now, we would apply the weights to the token vectors directly.\n",
    "\n",
    "It turns out that the attention mechanism performs even better if we introduce one more indirection and calculate **value vectors** from the token vectors and apply the weights to the value vectors.\n",
    "\n",
    "To borrow from databases one more time:\n",
    "\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a database (representing the actual content of the items).\n",
    "Once we determine which keys (and thus which parts of the input) are most relevant to the query (the item we are currently looking at), we retrieve the corresponding values.\n",
    "\n",
    "The computation of the value vectors works exactly like the computation of the key and query vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b2da458-19a1-4044-bf6a-a1257ec0888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V = torch.randn(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cda1bd97-ec1d-427d-8d0a-ca7af37509e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3301,  1.8359, -1.3448,  0.7947],\n",
      "        [-0.1512, -0.5678,  0.8648,  4.8368],\n",
      "        [ 2.6772, -1.3256, -3.2423, -0.3151]])\n"
     ]
    }
   ],
   "source": [
    "V = torch.matmul(X, W_V)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e918f-d406-4a23-8d38-ccb9df7af13e",
   "metadata": {},
   "source": [
    "We can now apply the attention weights to the values to get the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98cec7b8-84ea-4033-ba46-28308c5ad7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3301,  1.8359, -1.3448,  0.7947],\n",
      "        [ 0.3082,  1.7268, -1.2445,  0.9781],\n",
      "        [ 0.0517,  0.0270,  0.1831,  3.6559]])\n"
     ]
    }
   ],
   "source": [
    "R = torch.matmul(W_masked, V)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ccc25-c46d-41c8-a092-27931ec85ee2",
   "metadata": {},
   "source": [
    "This particular attention mechanism is called **scaled dot product attention** and is the by far most common attention these days.\n",
    "\n",
    "Let's verify that our understanding of scaled dot product attention is correct by using the `scaled_dot_product_attention` function from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3b1e4a8-2f15-4420-833d-ea30905bb088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3301,  1.8359, -1.3448,  0.7947],\n",
      "        [ 0.3082,  1.7268, -1.2445,  0.9781],\n",
      "        [ 0.0517,  0.0270,  0.1831,  3.6559]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "R_torch = scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "print(R_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfe858-dbab-4b86-816a-e4b1a18ccd45",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e809dad-4ee8-4ee4-8649-3342edaeac20",
   "metadata": {},
   "source": [
    "## The Batch Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618288d-7829-4f69-809e-baab2063a2ba",
   "metadata": {},
   "source": [
    "There is one more detail that we are missing right now.\n",
    "Namely, we need to incorporate the batch dimension in our calculations.\n",
    "\n",
    "While not particularly exiting, we will still go through the calculations, since this is a nice example that the batch dimension doesn't complicate things conceptually - you just need to get used to the fact that you always have an additional dimension in front of your tensors.\n",
    "\n",
    "Let's initialize a random tensor that represents a situation where we have a batch size of `2`, each batch element has `3` tokens and each token is represented by a vector of size `5`.\n",
    "\n",
    "Note that in this section we will postfix all variables with `_b` to emphasize that we are dealing with an additional batch dimension here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c21f4006-f8c6-4ee6-87f4-d6e7b3033054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3449, -1.4241, -0.1163, -0.9727,  0.9585],\n",
      "         [ 1.6192,  1.4506,  0.2695,  0.2625, -1.4391],\n",
      "         [ 0.5214,  0.3488,  0.9676, -0.4657,  1.1179]],\n",
      "\n",
      "        [[-1.2956,  0.0503, -0.5855, -0.3900,  0.0358],\n",
      "         [ 0.1206, -0.8057,  0.2080, -1.1586, -0.9637],\n",
      "         [-0.3750,  0.8033, -0.5188, -1.5013, -1.9267]]])\n"
     ]
    }
   ],
   "source": [
    "X_b = torch.randn(2, 3, 5)\n",
    "print(X_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79fa07-3299-4260-9058-981cb7e7d940",
   "metadata": {},
   "source": [
    "Next, we need to create the weights for the queries, keys and values.\n",
    "Instead of initializing matrices and then doing the calculations manually, lets initialize _linear layers_ and then call the layers (since this will be closer to what we will do in the end):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf436ff6-7b42-49e6-b3e2-cf57b74bedb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "W_Q_b = nn.Linear(5, 4, bias=False)\n",
    "W_K_b = nn.Linear(5, 4, bias=False)\n",
    "W_V_b = nn.Linear(5, 4, bias=False)\n",
    "K_b = W_K_b(X_b)\n",
    "Q_b = W_Q_b(X_b)\n",
    "V_b = W_V_b(X_b)\n",
    "\n",
    "print(K_b.shape, Q_b.shape, V_b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4cd97-f687-4c77-bb15-2ba38a8df7c7",
   "metadata": {},
   "source": [
    "Finally, we calculate the attention scores.\n",
    "Note that we need to be careful when we transpose the key matrix, since our actual token dimensions are now the dimensions `1` and `2` (as `0` is the batch dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27987a5a-174a-4ca0-88d8-2e66d41b1fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0351,  0.3447,  0.2260],\n",
      "         [ 0.1568, -2.1882, -0.7753],\n",
      "         [ 0.0279, -0.2793, -0.1115]],\n",
      "\n",
      "        [[-0.1849,  0.3218,  0.4802],\n",
      "         [ 0.4504, -0.5067, -0.4989],\n",
      "         [ 0.9851, -1.1174, -1.2743]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "S_b = torch.matmul(Q_b, K_b.transpose(1, 2))\n",
    "print(S_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2405f0-5bf3-4e6f-9ed1-23ce9e9abf14",
   "metadata": {},
   "source": [
    "We initialize the mask as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1eac66c5-9460-4e19-a26b-c6c379c87297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask_b = torch.tril(torch.ones(3, 3))\n",
    "print(mask_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70543eb1-72c5-4c1e-bf0b-f9af847544b1",
   "metadata": {},
   "source": [
    "We mask the respective scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04e9fa15-82e9-4f6b-a96d-b7a8b568fb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0351,    -inf,    -inf],\n",
      "         [ 0.1568, -2.1882,    -inf],\n",
      "         [ 0.0279, -0.2793, -0.1115]],\n",
      "\n",
      "        [[-0.1849,    -inf,    -inf],\n",
      "         [ 0.4504, -0.5067,    -inf],\n",
      "         [ 0.9851, -1.1174, -1.2743]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "S_masked_b = S_b.masked_fill(mask_b == 0, float(\"-inf\"))\n",
    "print(S_masked_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c657a6-df65-4297-aa3b-52dec718f590",
   "metadata": {},
   "source": [
    "Finally, we normalize the scores and apply softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67d1fe96-9066-4aac-bbd8-222b2724636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.7636, 0.2364, 0.0000],\n",
      "         [0.3584, 0.3074, 0.3343]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.6174, 0.3826, 0.0000],\n",
      "         [0.5979, 0.2090, 0.1932]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "S_masked_scaled_b = S_masked_b / K_b.shape[-1] ** 0.5\n",
    "W_b = torch.softmax(S_masked_scaled_b, dim=-1)\n",
    "print(W_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386325bd-18c7-42cf-94c6-f9010acc5bc4",
   "metadata": {},
   "source": [
    "And the output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee1c5bba-79f4-4a5b-a117-2fdd42908d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "R_b = torch.matmul(W_b, V_b)\n",
    "print(R_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc7312-f752-4222-9d4e-5bd39e521af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f8666f-464e-49f3-90f0-3d0a04ea5c5d",
   "metadata": {},
   "source": [
    "## Implementing a `SelfAttention` Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092109a5-46e6-4948-91a8-5b5fdc390458",
   "metadata": {},
   "source": [
    "Let's now package our calculations into a nice self-contained `SelfAttention` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ac698f3-fc88-4521-9069-cbed9b9eadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, c_len, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(c_len, c_len)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, num_tokens, d_in = X.shape\n",
    "        \n",
    "        Q = self.W_Q(X)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "\n",
    "        S = torch.matmul(Q, K.transpose(1, 2))\n",
    "        S_masked = S.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        S_masked_scaled = S_masked / K.shape[-1] ** 0.5\n",
    "        W = torch.softmax(S_masked_scaled, dim=-1)\n",
    "\n",
    "        W = self.dropout(W)\n",
    "        \n",
    "        R = torch.matmul(W, V)\n",
    "\n",
    "        return R\n",
    "\n",
    "layer = SelfAttention(d_in=5, d_out=4, c_len=3, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1f240-59fa-4c47-904a-4bfc65e6e3bb",
   "metadata": {},
   "source": [
    "Let's test that the layer works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb537cff-c034-4b9b-8d1c-24069cab0c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(2, 3, 5)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6703f16-b43b-4686-a0f8-350a89d4a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "Y = layer(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09cf7c-9444-4845-87a4-a3753e98f426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b48ac5-0eca-4851-b953-a1914315060c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af2f599-70cf-4323-be74-cdce3b235c40",
   "metadata": {},
   "source": [
    "Note that in practice the number of input dimensions and output dimensions will usually be the same, i.e. the self-attention layer doesn't change the dimensionality of the tensor, it only \"mixes\" the elements of the tensor together.\n",
    "\n",
    "This has the nice benefit that we can stack lots of self-attention on top of each other without needing to think about shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e34000-febd-4a74-bf89-8fa8ec460373",
   "metadata": {},
   "source": [
    "## Multi - Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa2dc1-85c6-4e80-a646-bce583604d67",
   "metadata": {},
   "source": [
    "The final piece we are missing is the multi-head attention.\n",
    "\n",
    "Basically instead of having a single self-attention layer, we use multiple self-attention layers, each with its own weights and combine their outputs.\n",
    "This allows the model to learn different features in different heads, allowing for richer capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f3dc4c6-d04a-4f18-bcbf-b2d550e70364",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(2, 3, 5)\n",
    "\n",
    "d_in = 5\n",
    "d_out = 4\n",
    "c_len = 3\n",
    "n_heads = 2\n",
    "\n",
    "heads = [SelfAttention(d_in, d_out, c_len, 0.0) for _ in range(n_heads)]\n",
    "\n",
    "result = [head(X) for head in heads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "819e7135-5998-4134-ab95-a1068b10f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 3, 4]), torch.Size([2, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "print([head_out.shape for head_out in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ac04c-3923-4da2-8c7c-bea8d887ebcb",
   "metadata": {},
   "source": [
    "Next we combine the head results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d36c1d1-cc24-4306-b2a6-9ef25ea4211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "head_out_combined = torch.cat(result, dim=-1)\n",
    "print(head_out_combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39719fa7-d528-412b-b651-01a29a9102c0",
   "metadata": {},
   "source": [
    "Note that in practice we don't want the multi-head attention layer to blow up the size of the tensor.\n",
    "\n",
    "Therefore we reduce the value of $d_{out}$ and set it to in such a way that the multi-head self-attention layer will not change the dimension of the incoming tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a55a581-485c-492d-867c-f4f66cdf0e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "d_out = 2\n",
    "heads = [SelfAttention(d_in, d_out, c_len, 0.0) for _ in range(n_heads)]\n",
    "\n",
    "head_out_combined = torch.cat([head(X) for head in heads], dim=-1)\n",
    "print(head_out_combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf042e-96c8-4313-a278-906433568855",
   "metadata": {},
   "source": [
    "While this technically already works, it is computationally expensive since we process the heads sequentially.\n",
    "Instead we can process them in parallel by computing the outputs for all attention heads at the same time.\n",
    "\n",
    "Here is how the entire process looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "575db38b-04a2-486f-9290-e0c335dbfcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 3\n",
    "d_in = 4\n",
    "d_out = 4\n",
    "n_heads = 2\n",
    "head_dim = d_out // n_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb67ad7-6741-4158-856f-8d91da5d6d1b",
   "metadata": {},
   "source": [
    "Let `X` be the input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff06f855-b36b-4bb3-9377-87e2f042c935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(2, n_tokens, d_in)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e9a71-db55-4e7d-b979-19926343d915",
   "metadata": {},
   "source": [
    "We initialize weight matrices for `W_K`, `W_Q` and `W_V`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ada7fe4f-a9d7-4287-bcaa-a6b656f26961",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K = torch.randn(d_out, d_in)\n",
    "W_Q = torch.randn(d_out, d_in)\n",
    "W_V = torch.randn(d_out, d_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bf65a-2b61-47e7-a1df-ac343259f806",
   "metadata": {},
   "source": [
    "We compute the key and query matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41af461e-8164-478c-b986-84d41a2a2dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "K = torch.matmul(X, W_K)\n",
    "Q = torch.matmul(X, W_Q)\n",
    "print(K.shape, Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716df1a-4cd2-4b4c-8d4c-78aa7152a7f2",
   "metadata": {},
   "source": [
    "We reshape the key and query matrices in such a way that we get multiple heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c695577f-f094-4c62-9fbe-b1c72e33529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_view = K.view(2, n_tokens, n_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a525653-0c04-4cf8-8bdf-741ed500aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(K_view.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ccb9e2b-4ef9-4ff4-8a0e-92ba70868897",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_view = Q.view(2, n_tokens, n_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e344a48-73a5-456e-8f9a-ec0f0677beed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(Q_view.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abefcb8d-c29c-4b46-a9ae-5bac4b8a9fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "K_view = K_view.transpose(1, 2)\n",
    "print(K_view.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "732bf14a-0dd4-4777-94e9-d6cef902952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "Q_view = Q_view.transpose(1, 2)\n",
    "print(Q_view.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5e985-9bd8-4402-beee-851c65484e2f",
   "metadata": {},
   "source": [
    "We obtain the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e64e871-cc70-4779-a28e-4ca4d3db107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "S = torch.matmul(Q_view, K_view.transpose(2, 3))\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2eaa7503-94e9-4e63-9b0c-48ff02a9309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -2.8630,  -0.1614,   1.2478],\n",
      "         [-10.0620,   0.9408,   3.7090],\n",
      "         [  2.8880,   0.4650,  -1.3942]],\n",
      "\n",
      "        [[ 11.3074,   4.0007,  -6.2850],\n",
      "         [ -4.6397,  -7.2396,   3.7868],\n",
      "         [ -4.1246,  -0.4083,   2.0658]]])\n"
     ]
    }
   ],
   "source": [
    "print(S[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206819e-c580-4110-8cb0-c150fa62adff",
   "metadata": {},
   "source": [
    "We obtain the mask and use it to mask out the future scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fb116f6-fc52-4bbb-b02b-e58c62e9c3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(n_tokens, n_tokens))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35ca05c7-dfe5-464a-9f40-71e5c34a98cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -2.8630,     -inf,     -inf],\n",
      "         [-10.0620,   0.9408,     -inf],\n",
      "         [  2.8880,   0.4650,  -1.3942]],\n",
      "\n",
      "        [[ 11.3074,     -inf,     -inf],\n",
      "         [ -4.6397,  -7.2396,     -inf],\n",
      "         [ -4.1246,  -0.4083,   2.0658]]])\n"
     ]
    }
   ],
   "source": [
    "S_masked = S.masked_fill(mask == 0, float(\"-inf\"))\n",
    "print(S_masked[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f69c8b-0cdf-457c-8bf6-cafbd5a68e7e",
   "metadata": {},
   "source": [
    "We scale the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6207c5c4-e8ec-47e1-8a55-dd3912a64d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.4315,    -inf,    -inf],\n",
      "          [-5.0310,  0.4704,    -inf],\n",
      "          [ 1.4440,  0.2325, -0.6971]],\n",
      "\n",
      "         [[ 5.6537,    -inf,    -inf],\n",
      "          [-2.3198, -3.6198,    -inf],\n",
      "          [-2.0623, -0.2041,  1.0329]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0593,    -inf,    -inf],\n",
      "          [-1.4993,  0.2439,    -inf],\n",
      "          [ 5.9892, -1.2553,  1.5600]],\n",
      "\n",
      "         [[-4.6679,    -inf,    -inf],\n",
      "          [ 0.5341,  0.3049,    -inf],\n",
      "          [-3.2725, -0.4792, -0.8532]]]])\n"
     ]
    }
   ],
   "source": [
    "S_masked_scaled = S_masked / K.shape[-1] ** 0.5\n",
    "print(S_masked_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815cd8e6-915c-408b-a6b6-7381463d4b8b",
   "metadata": {},
   "source": [
    "We apply the softmax to get the normalized attention weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdff5562-8623-4bfa-8135-a350049bb3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.0041, 0.9959, 0.0000],\n",
      "         [0.7066, 0.2104, 0.0830]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.7858, 0.2142, 0.0000],\n",
      "         [0.0339, 0.2173, 0.7488]]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.softmax(S_masked_scaled, dim=-1)\n",
    "print(W[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960ec1e-783b-429c-a62c-5ed9b621f1df",
   "metadata": {},
   "source": [
    "We calculate the values and reshape the value matrix.\n",
    "Finally, we get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f46678c-3547-49b8-b544-3e130b11a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = torch.matmul(X, W_V) \n",
    "V_view = V.view(2, n_tokens, n_heads, head_dim)\n",
    "V_view = V_view.transpose(1, 2)\n",
    "R = torch.matmul(W, V_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8218f5d3-ae89-496f-bce3-f420b3c2caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 2]) torch.Size([2, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(V_view.shape, W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f069a494-2cb3-40fa-bbbf-979a3dc3c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dd703-98ad-48c5-bc77-0e67b517358d",
   "metadata": {},
   "source": [
    "Next, we reshape the result back to three dimensions.\n",
    "The resulting tensor can now be fed into another layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf441b65-f74b-4ce5-8878-bfa2efde39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b46aad78-3957-490d-aca8-c5c360131692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f99cb60d-d3d9-4ffe-874b-f386bcebe894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_combined = R.contiguous().view(2, n_tokens, d_out)\n",
    "R_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f876d-8071-4eab-8154-cf64841f47dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
