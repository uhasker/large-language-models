{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a17b97-5311-4fb4-806f-9290c7d549a8",
   "metadata": {},
   "source": [
    "# Deep Dive Into GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44132849-22a1-4b67-bc00-166626502dd0",
   "metadata": {},
   "source": [
    "In this chapter we take a deep dive into the architecture of one of the first truly _Large_ Language Models - **GPT-2**.\n",
    "GPT-2 is an LLM that was released by OpenAI in 2019 and was followed by widespread public discussion about the potentials and dangers of LLMs.\n",
    "\n",
    "The reason we chose GPT-2 is simple.\n",
    "The model is not too large (\"just\" 1.5 billion parameters), so you will be able to load it into the memory of your local machine without having to provision a GPU instance on some cloud provider.\n",
    "\n",
    "As usual, we need to import a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b8fce5-619c-45ea-b7af-468348a6ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff1e6-abd7-4ec8-ba17-3b60a26cf46e",
   "metadata": {},
   "source": [
    "We will also disable the `huggingface_hub` progress bars as to not pollute the book (you should probably keep them though when following along):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0021e8af-ab64-45fc-ba35-b84215bed550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f2aae-003d-4b58-9cce-8ca694fbed30",
   "metadata": {},
   "source": [
    "Note that we use `torch==2.4.0` and `transformers==4.44.0`.\n",
    "If you want to follow along, you should also probably have these versions installed, otherwise you might need to change some of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293a680a-a1d2-48db-8f19-afc03ecb5bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d40398-78b3-4c6f-b521-7ae6a981c35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598a43a-4159-48cb-920a-45bf60b201fa",
   "metadata": {},
   "source": [
    "The code in this chapter is written in such a way that it closely mimicks the code of the `transformers` codebase.\n",
    "Generally, we highly encourage you to read the `transformers` codebase - it is well-written and easy to understand.\n",
    "\n",
    "The most relevant file for the purposes of this chapter is [`models/gpt2/modeling_gpt2.py`](https://github.com/huggingface/transformers/blob/v4.44-release/src/transformers/models/gpt2/modeling_gpt2.py) (especially the `GPT2LMHeadModel`, `GPT2Block`, `GPT2SdpaAttention` and `GPT2MLP` classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8cf6d-e4e7-472c-9bdd-b3ba2b9294be",
   "metadata": {},
   "source": [
    "## Loading the Model and Performing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f43410-bf58-42a5-baaf-055c63fba0b0",
   "metadata": {},
   "source": [
    "First, let us load the `gpt2` tokenizer and the `gpt2` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138676bf-9343-4dee-8217-2cf69b9e6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac7225b-4536-43ae-ba61-45fb77c02f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f5863d-790c-4b8f-bf92-ebac7c5c6996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88659cea-5415-4399-bf1a-22ee835f6247",
   "metadata": {},
   "source": [
    "Next let us perform some inference using an example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b425f42b-905c-4f2d-ac14-53c3b4e7fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is an example sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b82f10d-16a0-42b8-a0a0-d76dab57f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d3ba86-b335-4901-b0f3-cce3af2f1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1212,  318,  281, 1672, 6827]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8ba4d2-c7a4-4f32-9c62-4b33f1baac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53fc6dde-ee49-47ee-8c6e-61e201b324a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38bba3-27df-4dff-bd1d-70f7197a72f4",
   "metadata": {},
   "source": [
    "The `output` of the model is a `CausalLMOutputWithCrossAttentions` object which has (among other things) a `logits` attribute.\n",
    "This is basically the list of probabilities we discussed in chapter 1 except that it has the logits of the probabilities (for numerical reasons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cddf0e68-211c-4db8-8afe-78f59a04791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f503924-ae9c-4e09-bc91-572fdfaefb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -35.8889,  -35.2048,  -39.1335,  ...,  -42.4868,  -41.8196,\n",
      "           -36.0382],\n",
      "         [-107.7291, -108.0176, -113.2967,  ..., -116.4645, -115.7444,\n",
      "          -110.8654],\n",
      "         [-100.5390,  -99.8515, -103.7539,  ..., -105.0177, -107.3317,\n",
      "          -102.0780],\n",
      "         [ -71.9370,  -72.7244,  -76.2083,  ...,  -82.9281,  -81.7860,\n",
      "           -73.6416],\n",
      "         [-104.6989, -105.5694, -111.0104,  ..., -116.2477, -115.0037,\n",
      "          -106.4377]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a79d7-f3b1-44ca-b5a4-41515185c4eb",
   "metadata": {},
   "source": [
    "Note that `output.logits` is a tensor with three dimensions.\n",
    "\n",
    "The first dimension is the batch size.\n",
    "Since we only have a single text, the batch size is just `1`.\n",
    "\n",
    "The second dimension is the number of tokens in the sequence.\n",
    "We have five tokens, therefore the second dimension has the size `5`.\n",
    "\n",
    "Finally, the third dimension is the size of the vocabulary (since we output a probability for every token).\n",
    "Since the vocabulary size is `50257`, the third dimension has the size  `50257`.\n",
    "\n",
    "We want to predict the token that follows the last token.\n",
    "Therefore care about the probabilities of the tokens after the _last_ token, so let's extract that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac72eeeb-c902-4926-aad1-7ef73180ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_logits = output.logits[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e44732-88d0-4461-9e2b-3569d77e9b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "print(last_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b6bbf3-0cf6-4f2e-95a2-24fb9c468530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-104.6989, -105.5694, -111.0104,  ..., -116.2477, -115.0037,\n",
      "        -106.4377], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(last_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa03fed-c6d1-4bd9-9b2a-a1dcb940cfc7",
   "metadata": {},
   "source": [
    "Let's now get the actual probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9e98476-8acf-4aa9-9625-1534544b6426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0367e-03, 4.3414e-04, 1.8819e-06,  ..., 1.0002e-08, 3.4703e-08,\n",
      "        1.8218e-04], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(last_logits, dim=0)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9f4e5-0086-4779-986f-66d2e1516d0c",
   "metadata": {},
   "source": [
    "Finally, we sample the next token from this probability distribution.\n",
    "There are many different ways to do that - the simplest is to just get the token with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3131e4a2-d961-4ae3-bdf8-0666602950e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(326)\n"
     ]
    }
   ],
   "source": [
    "next_token_id = torch.argmax(probas, dim=-1)\n",
    "print(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4b9a0a6-6ee3-4b3f-b6b1-a83e50a7e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that\n"
     ]
    }
   ],
   "source": [
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bba75-9b3e-47af-bdad-f7bbb28d6f49",
   "metadata": {},
   "source": [
    "Let's also have a look at the `10` most probable tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5e0d9cc-cb11-4bfe-bc04-4ac8fe5681ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  that (ID = 326), Probability: 0.15\n",
      "Token:  from (ID = 422), Probability: 0.12\n",
      "Token: . (ID = 13), Probability: 0.11\n",
      "Token: , (ID = 11), Probability: 0.1\n",
      "Token: : (ID = 25), Probability: 0.09\n",
      "Token:  of (ID = 286), Probability: 0.05\n",
      "Token:  in (ID = 287), Probability: 0.04\n",
      "Token:  with (ID = 351), Probability: 0.02\n",
      "Token:  which (ID = 543), Probability: 0.02\n",
      "Token:  and (ID = 290), Probability: 0.02\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "top_k_probs, top_k_ids = torch.topk(probas, top_k)\n",
    "\n",
    "top_k_tokens = [(token_id, tokenizer.decode(token_id)) for token_id in top_k_ids]\n",
    "\n",
    "for (token_id, token), prob in zip(top_k_tokens, top_k_probs):\n",
    "    print(f\"Token: {token} (ID = {token_id}), Probability: {round(prob.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee55363-36b1-4797-a1ac-750212058ca1",
   "metadata": {},
   "source": [
    "We see that all of these tokens are reasonable next tokens for the sentence `\"This is an example sentence\"`.\n",
    "\n",
    "In fact, instead of simply selecting the most probably next token at every step (which often leads to repetitive and boring texts), we could _sample_ from the probability distribution.\n",
    "Here we would choose a random token weighted by the probabilities of the tokens (i.e. tokens with higher probabilities are more likely to be sampled):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a014762-f731-49d1-b64e-3e3cf51ff86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sampled_token_id = torch.multinomial(probas, num_samples=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d23aa6ce-20d8-41f1-9cc7-966f873f066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25)\n"
     ]
    }
   ],
   "source": [
    "print(sampled_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7996fc2d-b53c-4fe8-91f7-545667c0c6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(sampled_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82700c-af96-4c7b-95b2-62bcfe088fbc",
   "metadata": {},
   "source": [
    "Now that we have seen how to automatically compute the probabilities of the next token, let's redo the calculations manually - layer by layer.\n",
    "Before we do that, we will inspect the architecture of the model first to see what layers are actually present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d29b2-0597-4dad-8df7-456be8f17559",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6498-06af-40bd-95ac-73ecf50c9c8d",
   "metadata": {},
   "source": [
    "The `model` we have loaded is actually a `torch.nn.Module` which is the PyTorch base class for all neural network modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbceae16-9938-4c3a-8190-deb49f717e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(model, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48ece6-878b-4a1d-84ee-51c63418f465",
   "metadata": {},
   "source": [
    "There are many ways to inspect the architecture of a `torch.nn.Module`, here we simply `print` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bad79de-8a0e-48bd-84ca-2149d45f0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd9e39-f1e3-4f84-8888-b11d885c5e07",
   "metadata": {},
   "source": [
    "The model has two parts - a `transformer` and an `lm_head`.\n",
    "Two things should be noted here.\n",
    "\n",
    "First, the tokenizer is not part of the `model` as it is already represented by the `tokenizer` variable.\n",
    "Second, the `transformer` object has both the embedding block and the transformer (in terms of the terminology introduced in chapter 1).\n",
    "\n",
    "Looking closely, we see that the model has two embedding layers at the beginning - `wte` and `wpe`.\n",
    "The `wte` layer is the embedding layer for the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a6560dc-f154-4953-a17f-a45980bbc004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.wte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8188e25-036b-43e4-8151-e8236e53ffa4",
   "metadata": {},
   "source": [
    "The `wpe` layer is the positional embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa1e6059-40bc-4752-b585-962f77f3ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(1024, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.wpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c7704-8145-4220-a036-bf29eb449300",
   "metadata": {},
   "source": [
    "These layers are followed by a dropout layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8de427f-13c2-45ea-ab90-07379e309026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e649f44-b8b3-4e15-8e89-67fdd43286fd",
   "metadata": {},
   "source": [
    "Next, we have a module list consisting of 12 \"GPT blocks\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f6f4333-43ce-46df-a2f4-cb6db0cf6f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.ModuleList'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model.transformer.h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27baf5a6-a8cf-497d-acb4-a4635b02fae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(model.transformer.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831e87a-9f5e-4029-973a-d522742c3805",
   "metadata": {},
   "source": [
    "Each block is a so called `GPT2Block` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec809641-332e-4739-b839-32eb5c754898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model.transformer.h[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac64e5-4e00-4ac4-ade1-ca8f0214f4a4",
   "metadata": {},
   "source": [
    "Looking inside such a `GPT2Block`, we will see a lot of very familiar things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3be03888-b7be-4f08-953d-379177de66d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e55d8e-43b7-4780-a0c2-2b027f8634d3",
   "metadata": {},
   "source": [
    "The module list with the GPT blocks is followed by one last layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3a38b0b-431d-4996-86f2-58cec94e8a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.ln_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82d0b1-ff04-49f1-8227-5d2954fb13c3",
   "metadata": {},
   "source": [
    "The final component of the entire model is a linear layer which is responsible for computing the logits of the probabilities of the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "748d3408-0119-48ec-8e92-8db88917b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=50257, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606b2dc-92df-4a02-8274-3376061a0093",
   "metadata": {},
   "source": [
    "Generally speaking - whenever you want to find out how a particular LLM model works, it is extremely instructive to print out its architecture to get a basic view of the components it has.\n",
    "\n",
    "Now let's see how the tensors actually flow through the model.\n",
    "\n",
    "We will start with the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e604f-373f-4a6a-a93d-9ac108debabd",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a199e-6a4f-4e82-8fb5-40a2f3d7a891",
   "metadata": {},
   "source": [
    "Let's retrieve the token IDs of our example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa499601-7eaa-4da7-a094-27104698ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94223803-7a83-4e14-bdf7-90f132b901b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1212,  318,  281, 1672, 6827]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37ce1e-20ed-4db4-9ffd-053912e2c859",
   "metadata": {},
   "source": [
    "Let's also get the attention mask for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94a9d811-9c38-4bbf-9e2c-b177d828ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed7203f1-7055-42d7-b482-81f050f7dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b2b7b-927e-43c0-99c1-3084d9d6d83e",
   "metadata": {},
   "source": [
    "Let's also generate the position IDs.\n",
    "\n",
    "For every token ID we need a corresponding position ID.\n",
    "The position ID sequence is constructed by simply starting at `0` and then counting up to `len(token_ids) - 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36545f28-0536-4263-a8f7-3c200bc51160",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.tensor([[0, 1, 2, 3, 4]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a96068-ec39-4dbc-9f00-68d2608520b4",
   "metadata": {},
   "source": [
    "Let's now calculate the token embeddings.\n",
    "This is just a matter of applying the token embedding layer (i.e. `wte`) to the tensor containing the token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20346315-9f21-41ed-8975-92252d608559",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds = model.transformer.wte(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09865d-4b9f-4cab-846a-68a9f3f83600",
   "metadata": {},
   "source": [
    "We can do a quick dimensionality check.\n",
    "The `token_ids` is a sequence of dimension `1x5` (batch size `1` and sequence length of `5`).\n",
    "For every token we compute an embedding of dimension `768` - therefore the output tensor should have a dimension of `1x5x768`.\n",
    "This is indeed the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4482c3b-d5c9-4fc1-bf9c-91db62b5924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(token_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee8bfb-7559-4d34-abaa-a195d8414bb0",
   "metadata": {},
   "source": [
    "Let's also calculate the positional embeddings.\n",
    "This calculation is very similar to the calculation of the token embeddings, except that we now apply the _positional_ embedding layer (i.e. `wpe`) to the tensor containing the position IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e010997-ec76-4d3d-bea9-704862e44c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeds = model.transformer.wpe(position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c383ff-7056-4818-aa18-ce0382e914ad",
   "metadata": {},
   "source": [
    "Again, we do a quick dimensionality check.\n",
    "The `position_ids` sequence is a sequence of dimension `1x5` (batch size `1` and `5` position IDs).\n",
    "For every position we compute an embedding of dimension `768` and so the resulting output tensor should have a dimension of `1x5x768`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed089e27-95c3-418d-8a86-4394d143a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(position_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56ceb6-f123-4909-ad8f-245c1ece749c",
   "metadata": {},
   "source": [
    "To get the final embeddings we simply _add_ the token embeddings and the positional embeddings.\n",
    "\n",
    "These final embeddings will also be what we pass to the module block as the first layer input.\n",
    "Since the `transformers` codebase refers to the intermediate tensors in the module block as \"hidden states\", we will stick to that convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15cbff8c-1b4d-4b9d-925a-fe68a7a2bbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = token_embeds + position_embeds\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d2790-975d-419f-8b1c-440c39952ed5",
   "metadata": {},
   "source": [
    "Here is a graphic representation of the tensor flow so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb1a35-595c-4823-a423-da6abc02b067",
   "metadata": {},
   "source": [
    "![Embeddings](images/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fe52a-302b-4dee-9947-9e4918587e0e",
   "metadata": {},
   "source": [
    "## The First GPT2 Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f18f6-7327-446e-a44b-04fcee4f9c41",
   "metadata": {},
   "source": [
    "Let's now have a look at the module list, specifically its first \"GPT block\".\n",
    "\n",
    "First, we will give both the module list and the GPT block more meaningful names than `h` and `h[0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d654011-01fb-49da-96b5-32b6361a09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_blocks = model.transformer.h\n",
    "layer_block = layer_blocks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6493bf-b678-4a47-b8bb-c56ec37d763c",
   "metadata": {},
   "source": [
    "As a reminder, here is how the layer block looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee26f84f-1bec-4cdf-bc73-e508f5dc0cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(layer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061893e8-1dd5-4c27-aff1-c46ebe7d6d34",
   "metadata": {},
   "source": [
    "Basically, a `GPT2Block` has two parts - an attention part and a MLP part. \n",
    "\n",
    "The attention part consists of a `LayerNorm` layer, followed by a `GPT2SdpaAttention` block which contains the attention mechanism.\n",
    "\n",
    "The MLP part consists of a `LayerNorm` layer, followed by a `GPT2MLP` block which contains a simple MLP with two linear layers separated by a non-linear activation function.\n",
    "\n",
    "We will now look at both parts in detail.\n",
    "\n",
    "Let's rename the tensor and also save it in another variable since we will need it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff072217-0124-41b6-ae2b-d672ae854411",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_input = hidden_states\n",
    "attention_residual = attention_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773ba5b-b2cb-4012-917c-ccb2906a68b0",
   "metadata": {},
   "source": [
    "### The Attention Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b0977-b7ef-4bc7-989e-96d77c0d1255",
   "metadata": {},
   "source": [
    "First, we perform layer normalization on our tensor using the `LayerNorm` layer.\n",
    "\n",
    "Remember that this operation does not change the dimension of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b1430d9-e39e-4a81-937a-8b1f544f4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "normalized_attention_input = layer_block.ln_1(attention_input)\n",
    "print(normalized_attention_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671df682-f29e-48a0-8387-9111d09e2020",
   "metadata": {},
   "source": [
    "Next, we want to get the query, key and value tensors.\n",
    "\n",
    "This is what the `c_attn` layer is for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51e5abed-b6ee-4c9e-b9e1-43374da7d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2304])\n"
     ]
    }
   ],
   "source": [
    "query_key_value = layer_block.attn.c_attn(normalized_attention_input)\n",
    "print(query_key_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f82636-0030-46f0-8772-a85a2a1a4faf",
   "metadata": {},
   "source": [
    "Again, the first dimension of the tensor is the batch size (which is `1`) and the second dimension the number of tokens (which is `5`).\n",
    "\n",
    "The third dimension is more complicated.\n",
    "Basically in the `transformer` codebase the calculation of the queries, keys and values are combined in a single operation.\n",
    "Therefore the output tensor contains the queries, keys and value all in one object.\n",
    "This is why the third dimension is `2304 = 768 * 3` (since we store queries _and_ keys _and_ values and each of these has `768` items).\n",
    "\n",
    "Since we want to work with these tensors separately, we need to split them out using the `split` function.\n",
    "We have three dimensions and the queries, keys and values are split across the dimension number 2, so we need to `split` across `dim=2`.\n",
    "\n",
    "The order of the items in the tensor is query first, key second and value third:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d17e8c41-9b93-430e-a762-b00cbdfdf1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, keys, values = query_key_value.split(768, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acaa6fcb-0abe-4ce4-b4ce-e74470ce92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768]) torch.Size([1, 5, 768]) torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(queries.shape, keys.shape, values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073259da-762d-40ae-9fd4-99f1298eefbf",
   "metadata": {},
   "source": [
    "Next, we need to split the attention heads.\n",
    "\n",
    "To accomplish this, we will simply reuse the `_split_heads` helper function from the `transformers` codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a91071f0-6579-4570-8fa6-ffa267e4fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Splits hidden_size dim into attn_head_size and num_heads.\n",
    "    This function is taken directly from the transformers codebase.\n",
    "    \"\"\"\n",
    "    new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "    tensor = tensor.view(new_shape)\n",
    "    return tensor.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c66e7e-cb80-49e5-9146-8586294fdcc9",
   "metadata": {},
   "source": [
    "The GPT-2 model has 12 attention heads and a head dimension of 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86a50683-2120-4047-bc73-c59833014a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 5, 64]) torch.Size([1, 12, 5, 64]) torch.Size([1, 12, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 12\n",
    "head_dim = 64\n",
    "head_queries = _split_heads(queries, num_heads, head_dim)\n",
    "head_keys = _split_heads(keys, num_heads, head_dim)\n",
    "head_values = _split_heads(values, num_heads, head_dim)\n",
    "print(head_queries.shape, head_keys.shape, head_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378360a-fce1-42fb-baf7-9c7df5373be8",
   "metadata": {},
   "source": [
    "Next, we compute the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d189e52-1296-47df-9a77-12540fa728f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdpa_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    head_queries,\n",
    "    head_keys,\n",
    "    head_values,\n",
    "    attn_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    is_causal=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c138a3a9-5c85-496d-9aa1-6ec179a58810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "print(sdpa_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09976b-1e4f-45ab-b6cb-296367e25e5f",
   "metadata": {},
   "source": [
    "Now its time to merge the attention heads back together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e832ee2-e430-476e-b1ae-9aa05021c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdpa_output_transposed = sdpa_output.transpose(1, 2).contiguous()\n",
    "sdpa_output_view = sdpa_output_transposed.view(1, 5, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a61e3d-f35d-4cce-b9c5-c97782693b56",
   "metadata": {},
   "source": [
    "Let's double check the dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3d6012a-fd50-4382-89fa-382c9eb63fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(sdpa_output_view.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424e6c6-5743-4e53-a107-5c839c870e88",
   "metadata": {},
   "source": [
    "Now, we pass the the tensor through the projection layer `c_proj`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "965a7ae1-bed4-439d-9cd8-cec91481983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_output = layer_block.attn.c_proj(sdpa_output_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1154af-cf1d-4787-af2d-861a64c3325c",
   "metadata": {},
   "source": [
    "Again, we verify the dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b706ef6b-5941-49b9-96f1-5c567cac311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(projection_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e4b60-5d70-40fb-87e7-46c538cce9e6",
   "metadata": {},
   "source": [
    "Finally, we add our saved hidden state to the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "437cfd00-4217-417a-abf0-eb2fee0ea79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = projection_output + attention_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b63a0-68ac-4dd9-9ec1-1161e3827f4b",
   "metadata": {},
   "source": [
    "This is called a **residual connection**.\n",
    "\n",
    "Generally, we speak of residual connections if there is some function of the form $y = f(x) + x$ (in this case `x` is the input hidden state).\n",
    "Residual connections essentially helps with propagating the \"signal\" across layers (both in the forward and the backward pass).\n",
    "\n",
    "Especially for the backward pass, such connections can help addressing the vanishing gradient problem (discussed in the chapter on computational graphs).\n",
    "We can see this by comparing the derivative of a hypothetical loss function (with respect to the input $x$) with a residual connection and without a residual connection.\n",
    "\n",
    "Let's compute $\\frac{\\partial L}{\\partial x}$:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} (1 + \\frac{\\partial}{\\partial x} f(x)) = \\frac{\\partial L}{\\partial y} + \\frac{\\partial L}{\\partial y} \\frac{\\partial}{\\partial x} f(x)$\n",
    "\n",
    "Without the residual connection the derivative would simply be $\\frac{\\partial L}{\\partial y} \\frac{\\partial}{\\partial x} f(x)$.\n",
    "With the residual connection however, we directly _add_ the term $\\frac{\\partial L}{\\partial y}$ on top of that.\n",
    "This means that even if the gradient of `f` is vanishingly small, this won't be true for the gradient $\\frac{\\partial L}{\\partial x}$ and there should be a meaningful update when we perform gradient descent.\n",
    "\n",
    "Let's print the shape of `attention_output`.\n",
    "Note that the attention part of the GPT block did not change the _shape_ of the tensor, only its _values_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5773eebe-e65b-4384-9dd7-70c1ba670eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b0415d7-e073-4035-97aa-5395d4c8cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0065, -0.2930,  0.0762,  ...,  0.0184, -0.0275,  0.1638],\n",
      "         [ 0.0142, -0.0437, -0.0393,  ...,  0.1487, -0.0278, -0.0255],\n",
      "         [-0.0828, -0.0964,  0.1232,  ...,  0.0530,  0.0755, -0.1057],\n",
      "         [ 0.0714, -0.2025,  0.1870,  ..., -0.3685, -0.0108, -0.1304],\n",
      "         [-0.0888, -0.0326,  0.1666,  ..., -0.2539, -0.0370, -0.2046]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attention_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03aaeb7f-f32a-4537-8f91-8ba40a8c586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6158, -0.5796,  0.3292,  ...,  0.0484, -0.0504,  0.1853],\n",
      "         [-0.2766, -0.3195,  0.2326,  ...,  0.1510,  0.0209, -0.0308],\n",
      "         [-0.4673, -0.0429,  0.4036,  ...,  0.0257,  0.1020, -0.1085],\n",
      "         [-0.4814, -0.2070,  0.0321,  ..., -0.3712, -0.0445, -0.1020],\n",
      "         [-1.4112,  0.7068, -0.2456,  ..., -0.2627, -0.0697, -0.2511]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780bb478-a813-4ae3-8877-e8b1b54a640e",
   "metadata": {},
   "source": [
    "Here is a visualization of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5250a-91fa-447a-8ad3-fd6abd414ad8",
   "metadata": {},
   "source": [
    "![Attention](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4090c45-0f66-4653-9164-918da560cd6e",
   "metadata": {},
   "source": [
    "### The MLP Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb12d8-a721-477a-9630-3512c4febf36",
   "metadata": {},
   "source": [
    "The second part of the GPT block is the MLP part.\n",
    "\n",
    "Again, we first save the current hidden state tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b4eff2f-2655-472a-b890-2ce2b6ba775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_input = attention_output\n",
    "mlp_residual = mlp_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8017824-681e-40f2-a8ea-216fb9fede0e",
   "metadata": {},
   "source": [
    "And - again - we first pass the tensor through a layer normalization block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e2f4abb-28bb-455d-a926-2a77c46cef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_nlp_input = layer_block.ln_2(mlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d9d60eb-9379-4311-b1a3-a6f300ef8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(normalized_nlp_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d9aef-9429-49e7-b429-b19b3555a48c",
   "metadata": {},
   "source": [
    "Next, we pass the hidden states through the MLP block.\n",
    "\n",
    "The MLP block contains a linear layer, followed by a non-linear activation function, followed by another linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a500405a-9d9a-4b2e-9fff-0a7071a936ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(layer_block.mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e63097-0ef8-4da8-86ef-88a33dc70e5b",
   "metadata": {},
   "source": [
    "Here is how the tensor flow looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8863582b-c951-4572-bb4b-2fc9876abdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fc_output = layer_block.mlp.c_fc(normalized_nlp_input)\n",
    "act_output = layer_block.mlp.act(c_fc_output)\n",
    "c_proj_output = layer_block.mlp.c_proj(act_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82e1bf83-e11d-4155-aa86-89988e6ccf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(c_proj_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebccc0d-717c-42c5-8cee-65b8ad658bff",
   "metadata": {},
   "source": [
    "Finally, we have another residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be335bba-0e0e-4bf9-a768-ba3a02f59f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_output = mlp_residual + c_proj_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97415aaf-fc17-4c5e-859b-27987d93d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(mlp_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21e94cb6-d18c-4001-a578-95bf40a45e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1035, -0.2464,  0.3231,  ..., -1.1987, -0.6627,  1.9803],\n",
       "         [-1.3085, -0.2814, -0.8437,  ...,  0.2028,  0.1010,  0.7039],\n",
       "         [-0.9680, -0.7868,  0.1910,  ..., -0.4938, -0.0832, -1.1468],\n",
       "         [-2.1911, -0.1754, -1.5701,  ..., -2.0236,  0.8131,  1.1406],\n",
       "         [-3.2628,  2.9190, -0.5089,  ...,  0.0336, -0.0497, -0.7092]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf91230-e26b-4478-9e99-a468798d3bfa",
   "metadata": {},
   "source": [
    "Here is a visualization of the MLP part:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f05a6-7904-4c5b-88e4-05e1a60b7ede",
   "metadata": {},
   "source": [
    "![MLP](images/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1230e6d-e0c5-436b-a4a7-5857348a66ad",
   "metadata": {},
   "source": [
    "## The Other GPT Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25fee92-1497-45e9-a24c-b00c08537f31",
   "metadata": {},
   "source": [
    "Now, we simply pass the hidden states through one block after the other, where the output of each block is the input to the next block.\n",
    "\n",
    "Note that we alredy passed the tensor through the first block, so we will only consider the other 11 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60fe9393-2c8b-43a6-9c01-9cdae44b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = mlp_output\n",
    "\n",
    "for block in model.transformer.h[1:]:\n",
    "    hidden_states = block(hidden_states)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d38ed2-148f-4741-b5c1-b6d9bec08495",
   "metadata": {},
   "source": [
    "Since every block only changes the values of the tensor, but not its shape, the shape of the final tensor is _unchanged_ as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0089d5de-d5ec-4026-935f-fcc80f4fa603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ca622-c676-4cbb-92ea-a7c8aa00f559",
   "metadata": {},
   "source": [
    "Finally, we pass the final result through one last layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b971b718-631e-475e-be3b-c9154802b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = model.transformer.ln_f(hidden_states)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271862ca-3c01-4fa1-9c0b-77a1c2da42af",
   "metadata": {},
   "source": [
    "## Calculating the Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374d0ba-3a98-4cbe-96ee-f7ce779b7cb9",
   "metadata": {},
   "source": [
    "At last, we use the `lm_head` layer to calculate the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd596ea8-71a7-4e51-8be5-b41a4242f6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits = model.lm_head(hidden_states)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edf132-7b3e-4ba8-a260-3f94c075c646",
   "metadata": {},
   "source": [
    "Let's verify that our calculations are correct by checking if the `logits` tensor we computed manually is the same as `output.logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9131aa3-6d24-402f-a98b-f4fa7a4b3d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print((output.logits == logits).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
