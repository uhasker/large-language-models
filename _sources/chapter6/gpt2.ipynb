{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a17b97-5311-4fb4-806f-9290c7d549a8",
   "metadata": {},
   "source": [
    "# Deep Dive Into GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44132849-22a1-4b67-bc00-166626502dd0",
   "metadata": {},
   "source": [
    "In this chapter we will deconstruct one of the most known LLMs ever created - **GPT-2**.\n",
    "The reason we chose this model is that most of the models that followed it reused much of its architecture.\n",
    "If you understand GPT-2, you will not have too much trouble understanding later models.\n",
    "Additionally, GPT-2 is not too large (\"just\" 1.5 billion parameters), so you will be able to load it in the memory of your local machine and won't have to provision a GPU instance.\n",
    "\n",
    "As usual, we need to import a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b8fce5-619c-45ea-b7af-468348a6ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff1e6-abd7-4ec8-ba17-3b60a26cf46e",
   "metadata": {},
   "source": [
    "We will also disable the progress bars as to not pollute the book (you should probably keep them though when following along):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0021e8af-ab64-45fc-ba35-b84215bed550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8cf6d-e4e7-472c-9bdd-b3ba2b9294be",
   "metadata": {},
   "source": [
    "## Loading the Model and Performing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f43410-bf58-42a5-baaf-055c63fba0b0",
   "metadata": {},
   "source": [
    "First, we will load the tokenizer and the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138676bf-9343-4dee-8217-2cf69b9e6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac7225b-4536-43ae-ba61-45fb77c02f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6f5863d-790c-4b8f-bf92-ebac7c5c6996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88659cea-5415-4399-bf1a-22ee835f6247",
   "metadata": {},
   "source": [
    "Next perform inference using an example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b425f42b-905c-4f2d-ac14-53c3b4e7fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is an example sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b82f10d-16a0-42b8-a0a0-d76dab57f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d3ba86-b335-4901-b0f3-cce3af2f1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1212,  318,  281, 1672, 6827]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8ba4d2-c7a4-4f32-9c62-4b33f1baac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53fc6dde-ee49-47ee-8c6e-61e201b324a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38bba3-27df-4dff-bd1d-70f7197a72f4",
   "metadata": {},
   "source": [
    "The `output` contains among other things a `last_hidden_state` attribute which has the sequence of all the hidden states at the output of the last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7262fa95-4bfa-4793-8f2c-a5b2a5d22bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a79d7-f3b1-44ca-b5a4-41515185c4eb",
   "metadata": {},
   "source": [
    "Note that the first dimension is the batch size (which is `1` here since we only have a single text).\n",
    "The second dimension is the number of tokens.\n",
    "Finally, the third dimension is the dimension of the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ada2b0c-506f-41f2-99a8-3bbdcfc194ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0530, -0.0137, -0.2393,  ..., -0.1245, -0.1116,  0.0225],\n",
       "         [ 0.2470,  0.2260,  0.0397,  ...,  0.2413,  0.4349,  0.1768],\n",
       "         [ 0.7483, -0.4052, -0.9382,  ...,  0.3646, -0.0287,  0.3722],\n",
       "         [ 0.1990, -0.3695, -1.8210,  ..., -0.1772,  0.0093,  0.1647],\n",
       "         [ 0.0704, -0.0537, -2.5189,  ...,  0.0582, -0.1217, -0.3843]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699bb8d1-c545-4235-a132-4d58893210ee",
   "metadata": {},
   "source": [
    "Since we want to predict the next token, we want only the hidden state at the last token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e98476-8acf-4aa9-9625-1534544b6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = output.last_hidden_state[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d29b2-0597-4dad-8df7-456be8f17559",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6498-06af-40bd-95ac-73ecf50c9c8d",
   "metadata": {},
   "source": [
    "Let's output the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bad79de-8a0e-48bd-84ca-2149d45f0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd9e39-f1e3-4f84-8888-b11d885c5e07",
   "metadata": {},
   "source": [
    "We see that the model has two embedding layers at the beginning - `wte` and `wpe`.\n",
    "The `wte` layer is the embedding layer for the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a6560dc-f154-4953-a17f-a45980bbc004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.wte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8188e25-036b-43e4-8151-e8236e53ffa4",
   "metadata": {},
   "source": [
    "The `wpe` layer is the positional embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1e6059-40bc-4752-b585-962f77f3ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(1024, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.wpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c7704-8145-4220-a036-bf29eb449300",
   "metadata": {},
   "source": [
    "These layers are followed by a dropout layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8de427f-13c2-45ea-ab90-07379e309026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.1, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e649f44-b8b3-4e15-8e89-67fdd43286fd",
   "metadata": {},
   "source": [
    "Next, we have the module list `h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f6f4333-43ce-46df-a2f4-cb6db0cf6f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.ModuleList'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec188142-d3b9-42a6-bde9-699973708aaa",
   "metadata": {},
   "source": [
    "The module list consists of 12 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27baf5a6-a8cf-497d-acb4-a4635b02fae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(model.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831e87a-9f5e-4029-973a-d522742c3805",
   "metadata": {},
   "source": [
    "Each block is a so called `GPT2Block`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec809641-332e-4739-b839-32eb5c754898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model.h[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac64e5-4e00-4ac4-ade1-ca8f0214f4a4",
   "metadata": {},
   "source": [
    "Looking inside such a `GPT2Block`, we will see a lot of very familiar things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3be03888-b7be-4f08-953d-379177de66d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e55d8e-43b7-4780-a0c2-2b027f8634d3",
   "metadata": {},
   "source": [
    "Basically, a `GPT2Block` has a layer normalization, followed a `GPT2Attention` block which contains the attention mechanism.\n",
    "This is in turn followed by another layer normalization, followed by a `GPT2MLP` block.\n",
    "\n",
    "Whenever you want to find out, how an LLM model works, it is extremely instructive to print out its architecture to get a basic view of the components it has.\n",
    "Now let's say how the tensors actually flow through the model.\n",
    "\n",
    "We start with the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e604f-373f-4a6a-a93d-9ac108debabd",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a199e-6a4f-4e82-8fb5-40a2f3d7a891",
   "metadata": {},
   "source": [
    "Let's retrieve the token IDs of our example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa499601-7eaa-4da7-a094-27104698ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94223803-7a83-4e14-bdf7-90f132b901b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1212,  318,  281, 1672, 6827]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37ce1e-20ed-4db4-9ffd-053912e2c859",
   "metadata": {},
   "source": [
    "Let's also get the attention mask for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94a9d811-9c38-4bbf-9e2c-b177d828ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed7203f1-7055-42d7-b482-81f050f7dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b2b7b-927e-43c0-99c1-3084d9d6d83e",
   "metadata": {},
   "source": [
    "Let's also generate the position IDs.\n",
    "\n",
    "For every token ID we need a corresponding position ID.\n",
    "The position ID sequence is constructed by simply starting at `0` and then counting up to `len(token_ids) - 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36545f28-0536-4263-a8f7-3c200bc51160",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.tensor([[0, 1, 2, 3, 4]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a96068-ec39-4dbc-9f00-68d2608520b4",
   "metadata": {},
   "source": [
    "Let's calculate the token embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20346315-9f21-41ed-8975-92252d608559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.wte(token_ids)\n",
    "print(token_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee8bfb-7559-4d34-abaa-a195d8414bb0",
   "metadata": {},
   "source": [
    "Let's also calculate the positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e010997-ec76-4d3d-bea9-704862e44c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "position_embeds = model.wpe(position_ids)\n",
    "print(position_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56ceb6-f123-4909-ad8f-245c1ece749c",
   "metadata": {},
   "source": [
    "To get the final embeddings, we simply *add* the token embeddings and the positional embeddings.\n",
    "\n",
    "These final embeddings will also be what we pass to the model as the first layer input.\n",
    "Since the `transformers` codebase refers to the layer inputs/outputs as \"hidden states\", we will stick to that convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15cbff8c-1b4d-4b9d-925a-fe68a7a2bbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0065, -0.2930,  0.0762,  ...,  0.0184, -0.0275,  0.1638],\n",
      "         [ 0.0142, -0.0437, -0.0393,  ...,  0.1487, -0.0278, -0.0255],\n",
      "         [-0.0828, -0.0964,  0.1232,  ...,  0.0530,  0.0755, -0.1057],\n",
      "         [ 0.0714, -0.2025,  0.1870,  ..., -0.3685, -0.0108, -0.1304],\n",
      "         [-0.0888, -0.0326,  0.1666,  ..., -0.2539, -0.0370, -0.2046]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_states = token_embeds + position_embeds\n",
    "print(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d2790-975d-419f-8b1c-440c39952ed5",
   "metadata": {},
   "source": [
    "Here is a graphic representation of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb1a35-595c-4823-a423-da6abc02b067",
   "metadata": {},
   "source": [
    "![Embeddings](images/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fe52a-302b-4dee-9947-9e4918587e0e",
   "metadata": {},
   "source": [
    "## The First GPT2 Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773ba5b-b2cb-4012-917c-ccb2906a68b0",
   "metadata": {},
   "source": [
    "### The Attention Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f149f-2f87-470c-8d7c-0201c6a617b7",
   "metadata": {},
   "source": [
    "The `hidden_states` tensor is what we will not feed into the first GPT block.\n",
    "\n",
    "Let's save the tensor since we will need it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c591be4e-fd17-4a5c-b5cc-065ad0615cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b24e54-0a1c-499d-9c5e-f4da624ce7ce",
   "metadata": {},
   "source": [
    "Let's also give the module list a more meaningful name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb602121-e300-41c0-b203-2586d395f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_blocks = model.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15b61950-2dd1-4fb2-b046-35cdccbd366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_block = layer_blocks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b5ee-b6fe-4c2c-b155-bf821f0c8172",
   "metadata": {},
   "source": [
    "As a reminder, here is how the layer block looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7e06e5c-53e8-461e-9d19-7f5925845aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(layer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b0977-b7ef-4bc7-989e-96d77c0d1255",
   "metadata": {},
   "source": [
    "First, we pass the hidden states through the attention block.\n",
    "This means, we need to run them through the normalization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b1430d9-e39e-4a81-937a-8b1f544f4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = layer_block.ln_1(hidden_states)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671df682-f29e-48a0-8387-9111d09e2020",
   "metadata": {},
   "source": [
    "Next, we want to get the query, key and value tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51e5abed-b6ee-4c9e-b9e1-43374da7d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2304])\n"
     ]
    }
   ],
   "source": [
    "query_key_value = layer_block.attn.c_attn(hidden_states)\n",
    "print(query_key_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f82636-0030-46f0-8772-a85a2a1a4faf",
   "metadata": {},
   "source": [
    "The queries, keys and values are combined into a single tensor.\n",
    "This is why the third dimension is `2304 = 768 * 3`.\n",
    "\n",
    "Since we want to work with these tensor separately, we need to split them out using the `split` function.\n",
    "We have three dimensions and the queries, keys and values are split across the dimension number 2 (number 0 is the batch size, number 1 is the number tokens), so we need to `split` across `dim=2`.\n",
    "\n",
    "The order of the items in the tensor is query first, key second and value third:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d17e8c41-9b93-430e-a762-b00cbdfdf1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query, key, value = query_key_value.split(768, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acaa6fcb-0abe-4ce4-b4ce-e74470ce92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768]) torch.Size([1, 5, 768]) torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(query.shape, key.shape, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073259da-762d-40ae-9fd4-99f1298eefbf",
   "metadata": {},
   "source": [
    "Next, we need to split the attention heads.\n",
    "\n",
    "The GPT-2 model has 12 attention heads and a head dimension of 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86a50683-2120-4047-bc73-c59833014a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 5, 64]) torch.Size([1, 12, 5, 64]) torch.Size([1, 12, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 12\n",
    "head_dim = 64\n",
    "query = layer_block.attn._split_heads(query, num_heads, head_dim)\n",
    "key = layer_block.attn._split_heads(key, num_heads, head_dim)\n",
    "value = layer_block.attn._split_heads(value, num_heads, head_dim)\n",
    "print(query.shape, key.shape, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378360a-fce1-42fb-baf7-9c7df5373be8",
   "metadata": {},
   "source": [
    "Next, we compute the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d189e52-1296-47df-9a77-12540fa728f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_output, attn_weights = layer_block.attn._attn(query, key, value, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c138a3a9-5c85-496d-9aa1-6ec179a58810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 5, 64]) torch.Size([1, 12, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(attn_output.shape, attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09976b-1e4f-45ab-b6cb-296367e25e5f",
   "metadata": {},
   "source": [
    "Now its time to merge the attention heads back together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54c37fa9-ffd0-4cb7-b0b0-227e7fd44d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output = layer_block.attn._merge_heads(attn_output, num_heads, head_dim)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424e6c6-5743-4e53-a107-5c839c870e88",
   "metadata": {},
   "source": [
    "Now, we pass the the tensor through a final linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "965a7ae1-bed4-439d-9cd8-cec91481983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = layer_block.attn.c_proj(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b706ef6b-5941-49b9-96f1-5c567cac311f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e4b60-5d70-40fb-87e7-46c538cce9e6",
   "metadata": {},
   "source": [
    "Finally, we add our saved hidden state to the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "437cfd00-4217-417a-abf0-eb2fee0ea79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = attn_output + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5773eebe-e65b-4384-9dd7-70c1ba670eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780bb478-a813-4ae3-8877-e8b1b54a640e",
   "metadata": {},
   "source": [
    "Here is again a visualization of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5250a-91fa-447a-8ad3-fd6abd414ad8",
   "metadata": {},
   "source": [
    "![Attention](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4090c45-0f66-4653-9164-918da560cd6e",
   "metadata": {},
   "source": [
    "### The MLP Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb12d8-a721-477a-9630-3512c4febf36",
   "metadata": {},
   "source": [
    "The second part of the GPT block is the MLP part.\n",
    "\n",
    "Again, we first save the current hidden state tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b4eff2f-2655-472a-b890-2ce2b6ba775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8017824-681e-40f2-a8ea-216fb9fede0e",
   "metadata": {},
   "source": [
    "And - again - we first pass the hidden states through a layer normalization block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e2f4abb-28bb-455d-a926-2a77c46cef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = layer_block.ln_2(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d9d60eb-9379-4311-b1a3-a6f300ef8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d9aef-9429-49e7-b429-b19b3555a48c",
   "metadata": {},
   "source": [
    "Next, we pass the hidden states through the linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82e1bf83-e11d-4155-aa86-89988e6ccf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_hidden_states = layer_block.mlp(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9350711c-60b2-455c-81d1-9185d2aac3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(feed_forward_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebccc0d-717c-42c5-8cee-65b8ad658bff",
   "metadata": {},
   "source": [
    "Finally, we add the residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be335bba-0e0e-4bf9-a768-ba3a02f59f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = residual + feed_forward_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97415aaf-fc17-4c5e-859b-27987d93d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21e94cb6-d18c-4001-a578-95bf40a45e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1035, -0.2464,  0.3231,  ..., -1.1987, -0.6627,  1.9803],\n",
       "         [-1.3085, -0.2814, -0.8437,  ...,  0.2028,  0.1010,  0.7039],\n",
       "         [-0.9680, -0.7867,  0.1910,  ..., -0.4938, -0.0832, -1.1468],\n",
       "         [-2.1911, -0.1754, -1.5701,  ..., -2.0236,  0.8131,  1.1406],\n",
       "         [-3.2628,  2.9190, -0.5089,  ...,  0.0336, -0.0497, -0.7092]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1230e6d-e0c5-436b-a4a7-5857348a66ad",
   "metadata": {},
   "source": [
    "## The Other GPT Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25fee92-1497-45e9-a24c-b00c08537f31",
   "metadata": {},
   "source": [
    "Now, we simply pass the hidden states through one block after the other, where the output of each block is the input to the next block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60fe9393-2c8b-43a6-9c01-9cdae44b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in model.h[1:]:\n",
    "    hidden_states = block(hidden_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0089d5de-d5ec-4026-935f-fcc80f4fa603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ca622-c676-4cbb-92ea-a7c8aa00f559",
   "metadata": {},
   "source": [
    "Finally, we pass the final result through one last layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b971b718-631e-475e-be3b-c9154802b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = model.ln_f(hidden_states)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edf132-7b3e-4ba8-a260-3f94c075c646",
   "metadata": {},
   "source": [
    "Let's verify that our calculations are correct by checking if the `hidden_states` tensor we computed manually is the same as `output.last_hidden_state`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9131aa3-6d24-402f-a98b-f4fa7a4b3d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(hidden_states, output.last_hidden_state, rtol=1e-04, atol=1e-06)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
