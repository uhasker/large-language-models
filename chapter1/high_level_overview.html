
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>High-Level Overview &#8212; Large Language Models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter1/high_level_overview';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Computational Graphs and Backpropagation" href="../chapter2/computational_graphs.html" />
    <link rel="prev" title="Preface" href="../preface.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../preface.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Large Language Models - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Large Language Models - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../preface.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">High-Level Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2/computational_graphs.html">Computational Graphs and Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/basic_layers.html">Basic Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter4/tokenization.html">Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter5/attention.html">Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter6/deep_dive_into_gpt_2.html">Deep Dive Into GPT-2</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/uhasker/llm-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/uhasker/llm-book/issues/new?title=Issue%20on%20page%20%2Fchapter1/high_level_overview.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter1/high_level_overview.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>High-Level Overview</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#executive-summary">Executive Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#its-numbers-all-the-way-down">It’s Numbers All The Way Down</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-four-components-of-a-large-language-model">The Four Components of a Large Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-large-language-model">Training a Large Language Model</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="high-level-overview">
<h1>High-Level Overview<a class="headerlink" href="#high-level-overview" title="Link to this heading">#</a></h1>
<p>This chapter contains a very high-level overview of the topics discussed in this book. It serves both as an introduction for this book as well as a standalone text for those who don’t care about technical details, but want to get a general feel for the field.</p>
<section id="executive-summary">
<h2>Executive Summary<a class="headerlink" href="#executive-summary" title="Link to this heading">#</a></h2>
<p>First of all, Large Language Models are <strong>next token predictors</strong>.</p>
<p>What do we mean by that?
At its core, you give an LLM a text and it predicts the next token.
A token is basically just a part of a text (for example a token might be “an” or “123” or “this” or “th”).</p>
<p>For example, the input text could be the text “What is a Large Language”.
Then the LLM might predict that the next token should be “ Model”.
The resulting text would then be “What is a Large Language Model”.</p>
<a class="reference internal image-reference" href="../_images/llm.png"><img alt="LLM" src="../_images/llm.png" style="width: 400px;" /></a>
<p>However, if ever you talked to something like ChatGPT, you know that LLMs don’t just give the next token, but instead respond with complete texts.
How do they do that?</p>
<p>The answer is embarrassingly simple - <em>LLMs generate texts one token at a time</em>.</p>
<p>Again, let’s say that the text to be completed is “What is a Large Language”.</p>
<p>The LLM might predict “ Model” as the next token.
Now you would take the text together with the next token (“What is a Large Language Model”) and feed it into the model again, asking it to predict the next token which might be “?”.
You can now again take the text together with the next token (“What is a Large Language Model?”) and feed it into the model.</p>
<p>This process continues until either a maximum number of tokens has been reached or the predicted token is a special EOS (end of sequence) token which basically stands for “this sequence is over, stop generating”.
For example, if upon feeding the text “What is Large Language Model?” the model predicts the EOS token as the next token, the generation would end.</p>
<a class="reference internal image-reference" href="../_images/llm2.png"><img alt="LLM" src="../_images/llm2.png" style="width: 400px;" /></a>
<p>Ok, so LLMs complete texts by repeatedly predicting the next token.
But how do they predict the next token?
To adequately explain that, we need to introduce a concept called “embeddings” first.</p>
</section>
<section id="its-numbers-all-the-way-down">
<h2>It’s Numbers All The Way Down<a class="headerlink" href="#its-numbers-all-the-way-down" title="Link to this heading">#</a></h2>
<p>LLMs consist of mathematical functions (more on this later).
Mathematical functions tend to work with numbers instead of texts, so to apply functions to texts, we usually need to convert the texts to numbers first.</p>
<p>However, we don’t want to convert texts to arbitrary numbers.
Instead, these numbers should have some meaning.</p>
<p>This is the core insight behind embeddings - an embedding is a point in some high-dimensional space that represents some piece of a text in a meaningful way.</p>
<p>How can we make this more concrete?
Let’s say you would like to represent the words “cat”, “dog”, “pet”, “car”, “automobile”, “vehicle”, “apple”, “banana” and “fruit” using points in two-dimensional space.</p>
<p>It seems that in order for our representation to be meaningful the words “cat”, “dog” and “pet” should be close to each other.
The same should probably be true for “car”, “automobile” and “vehicle”.
Finally, “apple”, “fruit” and “banana” should be close to each other as well.
However, “cat” and “car” should be far apart, because they are semantically different (even though there is only a one-letter difference between them).</p>
<p>Here is how this could look like:</p>
<a class="reference internal image-reference" href="../_images/embeddings.png"><img alt="Embeddings" src="../_images/embeddings.png" style="width: 400px;" /></a>
<p>Mathematically, a point is just a list of numbers (for example, the word “cat” is represented by the point (1, 2) in this example), i.e. a vector.</p>
<p>Therefore, embeddings of texts are vectors with the property that two embeddings are close to each other if the underlying texts are semantically similar.</p>
<p>We can apply this concepts to more than just words.
Embeddings can be created for sentences, paragraphs, articles and any arbitrary pieces of text (i.e. any tokens).</p>
<p>The concept of semantic similarity is usually formalized using the distributional hypothesis:</p>
<p><strong>Words (more generally, tokens) are similar if they appear in similar contexts.</strong></p>
<p>This is why we would say that “cat” and “dog” are semantically similar (even though obviously cats and dogs are very different pets) - because they appear in similar contexts (namely, when we talk about pets).</p>
<p>One final important thing to note is that, in practice, embeddings have much more dimensions than just two - for example, embeddings that are utilized in modern LLMs have anywhere from hundreds to thousands of dimensions.</p>
</section>
<section id="the-four-components-of-a-large-language-model">
<h2>The Four Components of a Large Language Model<a class="headerlink" href="#the-four-components-of-a-large-language-model" title="Link to this heading">#</a></h2>
<p>Armed with this newfound knowledge, we can sketch out the four fundamental components of most Large Language Models.</p>
<p>The first component is the tokenizer.
The tokenizer is responsible for splitting the texts into tokens.</p>
<p>For example, the tokenizer used by GPT-4 would take the text “What is a Large Language Model?” and split it into the following tokens:</p>
<ul class="simple">
<li><p>“What”</p></li>
<li><p>“ is”</p></li>
<li><p>“ a”</p></li>
<li><p>“ Large”</p></li>
<li><p>“ Language”</p></li>
<li><p>“ Model”</p></li>
<li><p>“?”</p></li>
</ul>
<p>Here is how you could visualize this:</p>
<a class="reference internal image-reference" href="../_images/tokenize.png"><img alt="Embeddings" src="../_images/tokenize.png" style="width: 400px;" /></a>
<p>Note that the GPT-4 tokenizer would not always split the text into word tokens.
For example, if you would take the english word “overfitting”, the resulting tokens would be “over”, “fit” and “ting”.</p>
<blockquote>
<div><p>Technical Completeness Note: In reality, the tokenizer produces token IDs which are special numbers that represent the tokens. However, this is not particularly relevant for high-level understanding.</p>
</div></blockquote>
<p>The second component is an <strong>embedding layer</strong>.
The embedding layer takes the list of tokens produced by the tokenizer and produces an embedding for each token.
Consider the list of tokens from our example:</p>
<ul class="simple">
<li><p>“What”</p></li>
<li><p>“ is”</p></li>
<li><p>“ a”</p></li>
<li><p>“ Large”</p></li>
<li><p>“ Language”</p></li>
<li><p>“ Model”</p></li>
<li><p>“?”</p></li>
</ul>
<p>Here we have seven tokens.
This means that the embedding layer would produce seven embeddings (one embedding per token).</p>
<p>The first embedding would be a vector representing the token “What”, the second embedding a vector representing the token “ is” and so on until we arrive at the last embedding representing the token “?”.</p>
<p>Remember that embeddings are lists of numbers (vectors) that represent points in high-dimensional space in a semantically meaningful way.
Therefore the output of the embedding layer would be seven lists of numbers which have some semantic meaning.</p>
<blockquote>
<div><p>Technical Completeness Note: In addition to token embeddings, there are also positional embeddings and more.
We will gloss over them, since these are mostly technical optimizations and - again - not particularly relevant for the high-level understanding.</p>
</div></blockquote>
<p>The third component is the <strong>transformer</strong>.
The transformer is really the core component of modern LLMs.
It basically takes the embeddings and mixes them together to provide context.</p>
<p>Why is this important?
Consider the two sentences “The bat flew out of the cave” and “He hit the ball with his bat”.
In both texts, the token “bat” is present, however it has a completely different meaning in each sentence.
Therefore, the embedding layer would produce the same embeddings for the “bat” token in both cases.</p>
<p>The transformer now introduces context using something called the attention mechanism.
This allows the other tokens to - well - pay attention to other relevant tokens.
For example, “bat” might “pay attention” to “flew” and “cave” in the first sentence and “pay attention” to “hit” and “ball” in the second sentence.</p>
<p>Let’s consider the sentence “The bat flew out of the cave”.
Then attention scores for this sentence might look like this:</p>
<a class="reference internal image-reference" href="../_images/attention1.png"><img alt="Attention" src="../_images/attention1.png" style="width: 400px;" /></a>
<p>Here we have a high attention score between “flew” and “bat”, as well as a high attention score between “bat” and “cave”. This means that the transformer knows that the most relevant tokens for “bat” are “flew” and “cave”, i.e. this is the type of bat that flies out of caves.</p>
<p>Now let’s consider the sentence “He hit the ball with his bat”:</p>
<a class="reference internal image-reference" href="../_images/attention2.png"><img alt="Attention" src="../_images/attention2.png" style="width: 400px;" /></a>
<p>Here we have a high attention score between “bat” and “hit”, as well as a high attention score between “bat” and “ball”.
In this case, the transformer knows that the most relevant tokens for “bat” are “hit” and “ball”, i.e. this is completely different type of bat - the type that can be used to hit balls.</p>
<p>The transformer would now mix the “bat” embedding with the “flew” and “cave” embeddings in the first sentence.
In the second sentence however, the transformer would mix the “bat” embedding with the “hit” and “ball” embeddings.</p>
<p>The embeddings for “bat” would therefore be completely different in each sentence, because they now have context - we call these contextualized embeddings.</p>
<blockquote>
<div><p>Technical Completeness Note: The attention mechanism is relatively complicated and traditionally involves converting the embeddings to key and query vectors, computing attention scores and then using the attention scores to combine value vectors.
Since this a high-level overview, we will not dive into the computations here.</p>
</div></blockquote>
<p>The fourth component is a prediction layer.
This takes the embeddings created by the transformer and produces a huge list of probabilities for the next token.
For example, if the input text was “What is a Large Language” then the token “ Model” would have a high probability, but the tokens “homework” or “fngrl” would probably have a low probability.</p>
<p>The interesting thing here is that you can use this list of probabilities in different ways.
You could of course just always select the most probable token.
However, this often leads to boring and repetitive texts.
Instead, LLMs usually sample from this distribution.
This means that they randomly select a token in such a way that tokens with higher probabilities have a higher chance to be generated.</p>
<p>To sum it up - here is how an LLM predicts the next token given a text:</p>
<p>First, the tokenizer splits the text into tokens.
Second, the embedding layer generates an embedding for every token.
Third, the transformer improves the the embeddings by mixing them together.
Finally, the prediction layer generates a list of probabilities for all possible next tokens and samples the next token.</p>
</section>
<section id="training-a-large-language-model">
<h2>Training a Large Language Model<a class="headerlink" href="#training-a-large-language-model" title="Link to this heading">#</a></h2>
<p>Ok, so far, so good.
But why do companies like OpenAI and Antrophic have to spend tens of millions of dollars on their Large Language Models?</p>
<p>Well, remember all those mathematical functions from the previous paragraphs?
These functions usually have billions upon billions of parameters.</p>
<p>And these parameters have to be learned - we call that “training the model”.</p>
<p>To train the model, we need to collect a giant dataset first.
And by giant I mean giant.
For example, the RedPajama-Data-v2 dataset (one of the most well-known open datasets) has 20 billion documents consisting of 30 trillion tokens.
This is a lot of tokens.</p>
<p>The way the actual training works is conceptually simple:
First, we initialize the model with random parameters.
Next, we split our text corpus into tokens.
Then we iterate over all the tokens and at each token, we ask the model to give the probabilities for the next token (this is the output of the prediction layer, remember?).
Finally, we adjust the parameters of the model in such a way that the actual next token becomes more probable and all other tokens become less probable.</p>
<p>This is really it, although quite often after the initial training is done, models are usually finetuned (i.e. trained further) on special datasets that make the model less likely to produce undesired outputs, allow the model to chat with a user etc.</p>
<blockquote>
<div><p>Technical Completeness Note: We necessarily glossed over a lot of technical details in this section.
For example, we don’t really train models token by token. Instead, the model is fed batches of training data and processes multiple examples at the same time.</p>
</div></blockquote>
<p>While conceptually simple, the mathematical details and technical execution can get quite complicated.
For example, just recently Meta announced two 24k GPU clusters to manage model training.
As you can guess, managing these clusters and running the training requires a high degree of technical sophistication and a lot of resources - not many companies can do something like this.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../preface.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Preface</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter2/computational_graphs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Computational Graphs and Backpropagation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#executive-summary">Executive Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#its-numbers-all-the-way-down">It’s Numbers All The Way Down</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-four-components-of-a-large-language-model">The Four Components of a Large Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-large-language-model">Training a Large Language Model</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mikhail Berkov
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>