
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tokenization &#8212; Large Language Models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter2/tokenization';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Computational Graphs and Backpropagation" href="../chapter3/computational_graphs.html" />
    <link rel="prev" title="High-level overview" href="../chapter1/high_level_overview.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../preface.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Large Language Models - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Large Language Models - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../preface.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter1/high_level_overview.html">High-level overview</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/computational_graphs.html">Computational Graphs and Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter4/layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter5/attention.html">Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter6/deep-dive-into-gpt-2.html">Deep Dive Into GPT-2</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/uhasker/llm-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/uhasker/llm-book/issues/new?title=Issue%20on%20page%20%2Fchapter2/tokenization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter2/tokenization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tokenization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-idea">The Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subword-tokenization">Subword Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizers-in-the-transformers-library">Tokenizers in the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-byte-pair-encoding-algorithm">The Byte-Pair Encoding Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-notes">Final Notes</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tokenization">
<h1>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h1>
<section id="the-idea">
<h2>The Idea<a class="headerlink" href="#the-idea" title="Link to this heading">#</a></h2>
<p>The core of a Large Language Model expects some kind of sequence, so first we need to be able to convert a text into a sequence of some kind.</p>
<p>This is the job of the <strong>tokenizer</strong>.
The tokenizer splits the input text into a seqquence of individual units - called <strong>tokens</strong> - for further processing by the LLM.
A token is simply any string - tokens can be characters, they can be subwords, words and (theoretically) even larger units of text.</p>
<p>There are many different strategies that can be used to split a text into tokens.
These strategies mainly differ in the <strong>vocabulary</strong> they allow for.
The vocabulary is the list of all possible tokens that can be produced by a tokenizer.</p>
<p>One very simple strategy would be to do character tokenization, i.e. to simply split the text into it’s individual characters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_characters</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">tokenize_characters</span><span class="p">(</span><span class="s2">&quot;Example&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;E&#39;, &#39;x&#39;, &#39;a&#39;, &#39;m&#39;, &#39;p&#39;, &#39;l&#39;, &#39;e&#39;]
</pre></div>
</div>
</div>
</div>
<p>The problem with this approach is that the model would have to learn everything from the ground up - including how to form words - which will make
Additionally, this will result in very long sequence lengths (since every character is a separate unit) which will make it harder for our model to “pay attention” to the relevant parts.</p>
<p>One better way is to use word tokenization - just split the text into words.
Here how to simplest word tokenizer might look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_words</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="n">tokenize_words</span><span class="p">(</span><span class="s2">&quot;This is a sentence&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;sentence&#39;]
</pre></div>
</div>
</div>
</div>
<p>There are two objections to this approach.</p>
<p>First, we would need to manually handle a lot of special cases like punctuation and words with apostrophes (should <code class="docutils literal notranslate"><span class="pre">don't</span></code> be a single token or two tokens?).</p>
<p>Second and more important, this approach implicitly treats all words as equally important and leads to an extremely large <strong>vocabulary size</strong> (which is just the number of possible tokens).
In the later chapters we will see that the size of the LLM grows with the vocabulary size, so we want to keep that reasonable.
This becomes especially important once we move beyond the English language and consider - well - the rest of the world.</p>
<p>We’ve now considered two opposite approaches and found flaws with both of them.
Character tokenization results in units that are too small and we end up with very long sequences.
Word tokenization results in units that are too large and we end up with a lot of possible units.</p>
<p>Therefore, most modern tokenizers have settled somewhere in between character and word tokenization and do something called <strong>subword tokenization</strong>.</p>
</section>
<section id="subword-tokenization">
<h2>Subword Tokenization<a class="headerlink" href="#subword-tokenization" title="Link to this heading">#</a></h2>
<p>Let’s use the <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> library to show an example of subword tokenization.
First, we need to load a tokenizer.</p>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> tokenizer since we will use the <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> model throughout this book:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tiktoken</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">encode</span></code> method to tokenize a string:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;This is a sentence&quot;</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;&lt;|endoftext|&gt;&quot;</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s inspect the tokenization result:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1212, 318, 257, 6827]
</pre></div>
</div>
</div>
</div>
<p>Huh?
What are these strange numbers?</p>
<p>This is where we mention, that we have omitted an important technical detail so far.
A tokenizer doesn’t actually return a list of strings.
Instead, it returns a list of integers, where every integer is an ID representing a certain token from the tokenizer vocabulary.</p>
<p>We can view the tokens themselves like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">encoded_id</span> <span class="ow">in</span> <span class="n">encoded</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">encoded_id</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;This&#39;
&#39; is&#39;
&#39; a&#39;
&#39; sentence&#39;
</pre></div>
</div>
</div>
</div>
<p>We can also decode multiple tokens at the same time:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This is a sentence
</pre></div>
</div>
</div>
</div>
<p>Interestingly, the tokenizer can even handle garbage input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">garbage</span> <span class="o">=</span> <span class="s2">&quot;asdasdaf&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">garbage</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>asdasdaf
</pre></div>
</div>
</div>
</div>
<p>If we look at the individual tokens, we will get a glimpse at how subword tokenization operates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">encoded_id</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">garbage</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">encoded_id</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;as&#39;
&#39;d&#39;
&#39;as&#39;
&#39;d&#39;
&#39;af&#39;
</pre></div>
</div>
</div>
</div>
<p>It seems like the <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> tokenizer breaks word that aren’t in its predefined vocabulary into smaller subwords and backs off to individual characters if that’s not possible.
We will cover the specifics of this in a second.
Before we do that, let’s look at tokenizers in the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library, since that’s the library we will use most of the time.</p>
</section>
<section id="tokenizers-in-the-transformers-library">
<h2>Tokenizers in the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> Library<a class="headerlink" href="#tokenizers-in-the-transformers-library" title="Link to this heading">#</a></h2>
<p>Using the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library, we can instantiate a tokenizer like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2TokenizerFast</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading:   0%|          | 0.00/26.0 [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading: 100%|██████████| 26.0/26.0 [00:00&lt;00:00, 151kB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading:   0%|          | 0.00/0.99M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading: 100%|██████████| 0.99M/0.99M [00:00&lt;00:00, 36.2MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading:   0%|          | 0.00/446k [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading: 100%|██████████| 446k/446k [00:00&lt;00:00, 6.47MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading:   0%|          | 0.00/1.29M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading: 100%|██████████| 1.29M/1.29M [00:00&lt;00:00, 33.0MB/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading:   0%|          | 0.00/665 [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading: 100%|██████████| 665/665 [00:00&lt;00:00, 3.54MB/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast&#39;&gt;
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>Alternatively, you can also use the <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> class and let the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library figure out what kind of tokenizer should be instantiated:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at the vocabulary size of the tokenizer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>50257
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> tokenizer has more than 50.000 tokens, which is quite a large number, but certainly not as large as the number of all possible words that can be formed in the English and other languages.</p>
<p>We can map a text to its token IDs using the <code class="docutils literal notranslate"><span class="pre">encode</span></code> method of the <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;This is a sentence&quot;</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1212, 318, 257, 6827]
</pre></div>
</div>
</div>
</div>
<p>Similarly, we can map the token IDs back to the original text using the <code class="docutils literal notranslate"><span class="pre">decode</span></code> method of the <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">original_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">original_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This is a sentence
</pre></div>
</div>
</div>
</div>
<p>If you want to get the tokens from the text as strings, you can use the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;This&#39;, &#39;Ġis&#39;, &#39;Ġa&#39;, &#39;Ġsentence&#39;]
</pre></div>
</div>
</div>
</div>
<p>Note that most tokenizers do some shenanigans when representing tokens internally.
For example, the <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> tokenizer represents a whitespace at the beginning of a word as <code class="docutils literal notranslate"><span class="pre">Ġ</span></code>.
This is not important to us except as a technical detail.</p>
<p>You can also convert IDs to tokens using the <code class="docutils literal notranslate"><span class="pre">convert_ids_to_tokens</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">token_ids</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;This&#39;, &#39;Ġis&#39;, &#39;Ġa&#39;, &#39;Ġsentence&#39;]
</pre></div>
</div>
</div>
</div>
<p>Finally, you can call the <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> object directly to get output that will be usable by the rest of the LLM:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;input_ids&#39;: tensor([[1212,  318,  257, 6827]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1]])}
</pre></div>
</div>
</div>
</div>
<p>This will not only return the token IDs (called <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> in this dictionary), but also an <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>.
We will cover the purpose of this in later chapters.</p>
</section>
<section id="the-byte-pair-encoding-algorithm">
<h2>The Byte-Pair Encoding Algorithm<a class="headerlink" href="#the-byte-pair-encoding-algorithm" title="Link to this heading">#</a></h2>
<p>How exactly are subwords tokenizers created?</p>
<p>We could theoretically manually define all the respective subwords, but this is pretty tedious and becomes quite hard to do for tokenizers with 50.000 possible tokens.
Therefore, subword tokenizers are usually <em>trained</em>.</p>
<p>Let’s consider the following text:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Tokenizers are essential tools in natural language processing.</span>
<span class="s2">They break down text into smaller units called tokens.</span>
<span class="s2">Tokenizers help in transforming raw text into a format that machine learning models can understand.</span>
<span class="s2">There are different types of tokenizers, including word tokenizers, subword tokenizers, and character tokenizers.</span>
<span class="s2">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>First, we convert the text into a byte sequence:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>b&#39;\nTokenizers are essential tools in natural language processing.\nThey break down text into smaller units called tokens.\nTokenizers help in transforming raw text into a format that machine learning models can understand.\nThere are different types of tokenizers, including word tokenizers, subword tokenizers, and character tokenizers.\n&#39;
</pre></div>
</div>
</div>
</div>
<p>Let’s inspect the first ten bytes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">token_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">token_ids</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[10, 84, 111, 107, 101, 110, 105, 122, 101, 114]
</pre></div>
</div>
</div>
</div>
<p>We will now use the <code class="docutils literal notranslate"><span class="pre">token_ids</span></code> list to train our own (very limited) tokenizer.
The most very popular method to train tokenizers is called <strong>Byte Pair Encoding (BPE)</strong>, which builds a vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words.</p>
<p>Basically, we begin with an initial list of token IDs.
At every step, we identify the pair of token IDs that is most common in our sequence and “merge” that pair into a new token.
For example, if the pair <code class="docutils literal notranslate"><span class="pre">(126,</span> <span class="pre">84)</span></code> is the most common pair, we would generate a new token with some ID that doesn’t exist so far, replace all occurrences of <code class="docutils literal notranslate"><span class="pre">(126,</span> <span class="pre">84)</span></code> with this new tokens and continue the process.</p>
<p>To get started, let’s write a helper function that identifies the most common token pair in our sequence:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_token_pair_counts</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">counts</span>

<span class="k">def</span> <span class="nf">get_most_common_token_pair</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
    <span class="n">token_pair_counts</span> <span class="o">=</span> <span class="n">get_token_pair_counts</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">token_pair_counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">token_pair_counts</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_common_token_pair</span> <span class="o">=</span> <span class="n">get_most_common_token_pair</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">most_common_token_pair</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(101, 114)
</pre></div>
</div>
</div>
</div>
<p>This pair corresponds to the characters <code class="docutils literal notranslate"><span class="pre">e</span></code> and <code class="docutils literal notranslate"><span class="pre">r</span></code> which makes sense if we look at the text and observe the frequencies of the character pairs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">chr</span><span class="p">(</span><span class="mi">101</span><span class="p">),</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">114</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>e r
</pre></div>
</div>
</div>
</div>
<p>To merge these characters into a new token, we simply iterate over the sequence and replace each character pair with a new token ID:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">new_token_id</span><span class="p">):</span>
    <span class="n">new_token_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">token_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">token_ids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">new_token_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_token_id</span><span class="p">)</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_token_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">new_token_ids</span>
</pre></div>
</div>
</div>
</div>
<p>Here is how we could use the <code class="docutils literal notranslate"><span class="pre">merge</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">merge</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1, 6, 2, 3, 6, 3, 6]
</pre></div>
</div>
</div>
</div>
<p>Now we simply repeatedly perform a merge of the most common token pairs.
Note that this would include tokens that were the result of a merge, so BPE can merge tokens recursively.</p>
<p>One important question to consider is how many steps we want to perform.
This is basically a hyperparameter - the more tokens we have, the larger our vocabulary size, but the smaller our sequence lengths will be.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_merges</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">old_token_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

<span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
    <span class="n">most_common_token_pair</span> <span class="o">=</span> <span class="n">get_most_common_token_pair</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="n">new_token_id</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">+</span> <span class="n">i</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Merge </span><span class="si">{</span><span class="n">most_common_token_pair</span><span class="si">}</span><span class="s2"> into a new token </span><span class="si">{</span><span class="n">new_token_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">merge</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">most_common_token_pair</span><span class="p">,</span> <span class="n">new_token_id</span><span class="p">)</span>
    <span class="n">merges</span><span class="p">[</span><span class="n">most_common_token_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token_id</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Merge (101, 114) into a new token 256
Merge (32, 116) into a new token 257
Merge (105, 110) into a new token 258
Merge (101, 110) into a new token 259
Merge (111, 107) into a new token 260
Merge (260, 259) into a new token 261
Merge (256, 115) into a new token 262
Merge (261, 105) into a new token 263
Merge (263, 122) into a new token 264
Merge (264, 262) into a new token 265
Merge (101, 32) into a new token 266
Merge (32, 258) into a new token 267
Merge (97, 110) into a new token 268
Merge (10, 84) into a new token 269
Merge (97, 114) into a new token 270
Merge (97, 108) into a new token 271
Merge (258, 103) into a new token 272
Merge (111, 114) into a new token 273
Merge (257, 265) into a new token 274
Merge (101, 115) into a new token 275
Merge (97, 116) into a new token 276
Merge (46, 269) into a new token 277
Merge (32, 99) into a new token 278
Merge (272, 32) into a new token 279
Merge (274, 44) into a new token 280
Merge (265, 32) into a new token 281
Merge (270, 266) into a new token 282
Merge (275, 115) into a new token 283
Merge (259, 116) into a new token 284
Merge (108, 115) into a new token 285
Merge (277, 104) into a new token 286
Merge (257, 101) into a new token 287
Merge (287, 120) into a new token 288
Merge (288, 116) into a new token 289
Merge (289, 267) into a new token 290
Merge (290, 116) into a new token 291
Merge (291, 111) into a new token 292
Merge (292, 32) into a new token 293
Merge (271, 108) into a new token 294
Merge (32, 117) into a new token 295
Merge (295, 110) into a new token 296
Merge (102, 273) into a new token 297
Merge (297, 109) into a new token 298
Merge (97, 99) into a new token 299
Merge (268, 100) into a new token 300
Merge (119, 273) into a new token 301
Merge (301, 100) into a new token 302
Merge (302, 280) into a new token 303
Merge (303, 32) into a new token 304
Merge (269, 281) into a new token 305
</pre></div>
</div>
</div>
</div>
<p>All that’s left is to write the <code class="docutils literal notranslate"><span class="pre">decode</span></code> and <code class="docutils literal notranslate"><span class="pre">encode</span></code> functions.</p>
<p>Writing the <code class="docutils literal notranslate"><span class="pre">decode</span></code> functions is simple - we just need to translate every individual ID to the respective string it encodes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">([</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span> <span class="p">}</span>
<span class="k">for</span> <span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">),</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">merges</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">p0</span><span class="p">]</span> <span class="o">+</span> <span class="n">vocab</span><span class="p">[</span><span class="n">p1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;replace&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">decode</span><span class="p">([</span><span class="mi">104</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">301</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">33</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hello world!
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">encode</span></code> function is a bit harder and happens in an iterative way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="n">get_token_pair_counts</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
        <span class="n">pair</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">merges</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">pair</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">merges</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">merges</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="n">merge</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">pair</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">token_ids</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;hello world!&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[104, 101, 108, 108, 111, 32, 301, 108, 100, 33]
</pre></div>
</div>
</div>
</div>
</section>
<section id="final-notes">
<h2>Final Notes<a class="headerlink" href="#final-notes" title="Link to this heading">#</a></h2>
<p>Note that the tokenizer is conceptually distinct from the rest of the LLM.
It often uses its own training set, which may differ from that of the LLM, to train its vocabulary using the BPE algorithm.</p>
<p>The tokenizer’s role is to translate between text and numbers.
The LLM only processes numbers and never interacts directly with the text.
In theory, once the text is translated into numerical form, the original text could be discarded.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter1/high_level_overview.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">High-level overview</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter3/computational_graphs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Computational Graphs and Backpropagation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-idea">The Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subword-tokenization">Subword Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizers-in-the-transformers-library">Tokenizers in the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-byte-pair-encoding-algorithm">The Byte-Pair Encoding Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-notes">Final Notes</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mikhail Berkov
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>