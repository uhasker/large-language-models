{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef73e18-c7cc-4f75-ae87-6ce2513d77cc",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1a220-e8b5-490e-85c4-ed84aa63f777",
   "metadata": {},
   "source": [
    "## The Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f4634-04c7-4f1c-8c69-039d25a25b38",
   "metadata": {},
   "source": [
    "The core of a Large Language Model expects some kind of sequence, so first we need to be able to convert a text into a sequence of some kind.\n",
    "\n",
    "This is the job of the **tokenizer**.\n",
    "The tokenizer splits the input text into a sequence of individual units - called **tokens** - for further processing by the LLM.\n",
    "A token is simply any string - tokens can be characters, subwords, words and (theoretically) even larger units of text.\n",
    "\n",
    "There are many different strategies that can be used to split a text into tokens.\n",
    "These strategies mainly differ in the **vocabulary** they allow for.\n",
    "The vocabulary is the list of all possible tokens that can be produced by a tokenizer.\n",
    "\n",
    "One very simple strategy would be to do character tokenization, i.e. to split the text into its individual characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a9777-c34c-4dc6-8efa-67b979915dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_characters(text):\n",
    "    return list(text)\n",
    "\n",
    "tokenize_characters(\"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b599a-bf82-4f22-b7d4-07c0ab20520a",
   "metadata": {},
   "source": [
    "The problem with this approach is that the model would have to learn everything from the ground up - including how to form words. \n",
    "Additionally, this will result in very long sequence lengths (since every character is a separate unit), which will make it harder for our model to \"pay attention\" to the relevant parts.\n",
    "\n",
    "One better way is to use word tokenization - just split the text into words.\n",
    "Here, how the simplest word tokenizer might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6b7ea-2ecb-4f37-ae59-4d45d3524131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenize_words(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551ea2c-9fde-4ed8-8570-569b4ee200e8",
   "metadata": {},
   "source": [
    "There are two objections to this approach.\n",
    "\n",
    "First, we would need to manually handle a lot of special cases such as punctuation and words with apostrophes (should `don't` be a single token or two tokens?).\n",
    "\n",
    "Second and more important, this approach implicitly treats all words as equally important and leads to an extremely large **vocabulary size** (which is just the number of possible tokens).\n",
    "In the later chapters, we will see that the size of the LLM grows with the vocabulary size, so we want to keep that reasonable.\n",
    "This becomes especially important once we move beyond the English language and consider - well - the rest of the world.\n",
    "\n",
    "So far, we have considered two opposing approaches and identified flaws in both.\n",
    "Character tokenization results in units that are too small, and we end up with very long sequences.\n",
    "Word tokenization results in units that are too large, leading to a lot of possible units.\n",
    "\n",
    "Therefore, most modern tokenizers have settled somewhere in between character and word tokenization and do something called **subword tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e9aae-8104-4bef-9b32-660c3fbdf542",
   "metadata": {},
   "source": [
    "## Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89089a-9fd2-41ea-8a1c-647348e09c2b",
   "metadata": {},
   "source": [
    "Let's use the `tiktoken` library to show an example of subword tokenization.\n",
    "First, we need to load a tokenizer.\n",
    "\n",
    "We will use the `gpt2` tokenizer since we will use the `gpt2` model throughout this book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd823a84-6fa0-41ec-8cbf-de0aa3462d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca81c3-d6cd-4212-826e-6238f230b34b",
   "metadata": {},
   "source": [
    "We can use the `encode` method to tokenize a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ecd62-dc07-48bb-bd6d-6d943845cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(\"This is a sentence\", allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca6cb2-39c2-483e-a4dc-9eba83c91c4c",
   "metadata": {},
   "source": [
    "Let's inspect the tokenization result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a244893-2e94-4248-88b4-9b8c1d260b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c40657-6f7e-4691-9044-c463d53c8707",
   "metadata": {},
   "source": [
    "Huh?\n",
    "What are these strange numbers?\n",
    "\n",
    "This is where we mention, that we have omitted an important technical detail so far.\n",
    "A tokenizer doesn't actually return a list of strings.\n",
    "Instead, it returns a list of integers, where every integer is an ID representing a certain token from the tokenizer vocabulary.\n",
    "\n",
    "We can view the tokens themselves like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9c564-22cd-473f-9e9f-6b7f67e0f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoded_id in encoded[:10]:\n",
    "    print(repr(tokenizer.decode([encoded_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e4800-fcab-4883-8f75-3257f8dd0f5d",
   "metadata": {},
   "source": [
    "We can also decode multiple tokens at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441b13e-db4a-4717-81b5-b7c65f27c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf1650-6956-4ac0-95fb-864a95f89307",
   "metadata": {},
   "source": [
    "Interestingly, the tokenizer can even handle garbage input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a878778-fc24-45e0-9152-fc3bc7c69508",
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage = \"asdasdaf\"\n",
    "print(tokenizer.decode(tokenizer.encode(garbage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250dda3-dd6d-40ee-b0f3-a1a3002a4e6c",
   "metadata": {},
   "source": [
    "If we look at the individual tokens, we will get a glimpse at how subword tokenization operates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf396d5-c977-47d3-bbaf-9d0ad9eee89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoded_id in tokenizer.encode(garbage):\n",
    "    print(repr(tokenizer.decode([encoded_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c06b7-fa6e-46f7-9233-0551e4fbd731",
   "metadata": {},
   "source": [
    "It seems like the `gpt2` tokenizer breaks words not found in its predefined vocabulary into smaller subwords and backs off to individual characters if necessary.\n",
    "We will cover the specifics of this in a second.\n",
    "Before we do that, let's look at tokenizers in the `transformers` library, since that's the library we will use most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeea054-0f98-45f3-9ceb-e36c7342214e",
   "metadata": {},
   "source": [
    "## Tokenizers in the `transformers` Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c852b9-c830-486f-8b01-a4c846536595",
   "metadata": {},
   "source": [
    "Using the `transformers` library, we can instantiate a tokenizer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc5906-b6a5-4352-b71e-899747067fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5956678-184d-48e3-8a39-2ebe53eb4159",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the `AutoTokenizer` class and let the `transformers` library figure out what kind of tokenizer should be instantiated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5291f8-e681-485e-b8ed-da2d8d20abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91deaa4-03f0-42fe-a783-7db5c4f5cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a4a62-a284-4d0b-8004-09c96fad981f",
   "metadata": {},
   "source": [
    "Let's have a look at the vocabulary size of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6872b4-c33c-469d-b0db-addf096a147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805cda5-a92a-4643-9f70-16a7bf585e59",
   "metadata": {},
   "source": [
    "The `gpt2` tokenizer has more than 50.000 tokens, which is quite a large number, but certainly not as large as the number of all possible words that can be formed in English and other languages.\n",
    "\n",
    "We can map a text to its token IDs using the `encode` method of the `tokenizer` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca126bd-04c1-4229-838a-8cb3e7f08d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sentence\"\n",
    "\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b662d8d-ac15-434f-80d9-70eb2ff4b0e2",
   "metadata": {},
   "source": [
    "Similarly, we can map the token IDs back to the original text using the `decode` method of the `tokenizer` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed8c40-8ee6-46b6-a42b-3f83d726efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = tokenizer.decode(token_ids)\n",
    "\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eef82-2988-484f-8b61-4338dff83229",
   "metadata": {},
   "source": [
    "If you want to get the tokens from the text as strings, you can use the `tokenize` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2b87f-2c2d-4f58-9115-758fa9967799",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f41dbc-7aea-4c1a-8e7c-d401d6256b22",
   "metadata": {},
   "source": [
    "Note that most tokenizers do some shenanigans when representing tokens internally.\n",
    "For example, the `gpt2` tokenizer represents a whitespace at the beginning of a word as `Ġ`.\n",
    "This is not important to us except as a technical detail.\n",
    "\n",
    "You can also convert IDs to tokens using the `convert_ids_to_tokens` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41920d25-5c11-49e2-a06a-fb81a4213549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a556c-4bd0-42f8-b525-4507e747a63d",
   "metadata": {},
   "source": [
    "Finally, you can call the `tokenizer` object directly to get output that will be usable by the rest of the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c551d-5ae0-44cb-a44c-e4ee5d1f2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e1bf4-399b-478e-8792-79bc9f666d48",
   "metadata": {},
   "source": [
    "This will not only return the token IDs (called `input_ids` in this dictionary) but also an `attention_mask`.\n",
    "We will cover the purpose of this in later chapters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67355460-d910-46c4-8965-70cc43fc81f9",
   "metadata": {},
   "source": [
    "## The Byte-Pair Encoding Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a7755-55fc-45d7-9ade-dfa960ee1452",
   "metadata": {},
   "source": [
    "How exactly are subword tokenizers created?\n",
    "\n",
    "We could theoretically manually define all the respective subwords, but this is pretty tedious and becomes quite hard to do for tokenizers with 50.000 possible tokens.\n",
    "Therefore, subword tokenizers are usually _trained_.\n",
    "\n",
    "Let's consider the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98458430-d63b-4b58-a994-eab9860dfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Tokenizers are essential tools in natural language processing.\n",
    "They break down text into smaller units called tokens.\n",
    "Tokenizers help in transforming raw text into a format that machine learning models can understand.\n",
    "There are different types of tokenizers, including word tokenizers, subword tokenizers, and character tokenizers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c82910-1cf3-417c-9f4e-b100307f18a2",
   "metadata": {},
   "source": [
    "First, we convert the text into a byte sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8ae9c-74fd-4be0-8a7c-03399b2e9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.encode(\"utf-8\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6a59c-36c2-4277-aacf-0b2a1cb4a365",
   "metadata": {},
   "source": [
    "Let's inspect the first ten bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5563e-d5f8-468c-8a6a-a354c4fccecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = list(map(int, tokens))\n",
    "print(token_ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff3f53-6e31-486f-8156-111b17f5ff07",
   "metadata": {},
   "source": [
    "We will now use the `token_ids` list to train our own (very limited) tokenizer.\n",
    "The most popular method to train tokenizers is called **Byte Pair Encoding (BPE)** which builds a vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words.\n",
    "\n",
    "Basically, we begin with an initial list of token IDs.\n",
    "At every step, we identify the pair of token IDs that is most common in our sequence and \"merge\" that pair into a new token.\n",
    "For example, if the pair `(126, 84)` is the most common pair, we would generate a new token with some ID that doesn't exist so far, replace all occurrences of `(126, 84)` with these new tokens and continue the process.\n",
    "\n",
    "To get started, let's write a helper function that identifies the most common token pair in our sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565a7cd-0d28-4379-a243-d83abc768aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_pair_counts(token_ids):\n",
    "    counts = {}\n",
    "    for pair in zip(token_ids, token_ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def get_most_common_token_pair(token_ids):\n",
    "    token_pair_counts = get_token_pair_counts(token_ids)\n",
    "    return max(token_pair_counts, key=token_pair_counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00267ebf-09bd-4ad9-a699-20b4613d79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_token_pair = get_most_common_token_pair(token_ids)\n",
    "print(most_common_token_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65f1a6-bbf3-420d-86a5-d66216b9042e",
   "metadata": {},
   "source": [
    "This pair corresponds to the characters `e` and `r`, which makes sense if we look at the text and observe the frequencies of the character pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9b91f-8c24-4211-9971-2dbfa03eaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chr(101), chr(114))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5db11-3ce1-48c0-84e5-c4a056f739d7",
   "metadata": {},
   "source": [
    "To merge these characters into a new token, we simply iterate over the sequence and replace each character pair with a new token ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cfd36-5c03-43bd-aea8-5291b0219565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(token_ids, pair, new_token_id):\n",
    "    new_token_ids = []\n",
    "    i = 0\n",
    "    while i < len(token_ids):\n",
    "        if i < len(token_ids) - 1 and token_ids[i] == pair[0] and token_ids[i+1] == pair[1]:\n",
    "            new_token_ids.append(new_token_id)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_token_ids.append(token_ids[i])\n",
    "            i += 1\n",
    "    return new_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075eb6a-3339-4664-b0c9-a5c633576bc6",
   "metadata": {},
   "source": [
    "Here is how we could use the `merge` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7c53e-1932-4a8c-98eb-cb5ff2b78896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge([1, 4, 5, 2, 3, 4, 5, 3, 4, 5], (4, 5), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d772e1-6412-4ffb-97ed-af1f804fa1e2",
   "metadata": {},
   "source": [
    "Now, we simply repeatedly perform a merge of the most common token pairs.\n",
    "Note that this would include tokens that were the result of a merge, so BPE can merge tokens recursively.\n",
    "\n",
    "One important question to consider is how many steps we want to perform.\n",
    "This is basically a hyperparameter - the more tokens we have, the larger our vocabulary size but the smaller our sequence lengths will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67d876-6740-4ab7-889f-735489a7b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 50\n",
    "\n",
    "old_token_ids = list(token_ids)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    most_common_token_pair = get_most_common_token_pair(token_ids)\n",
    "    new_token_id = 256 + i\n",
    "    print(f\"Merge {most_common_token_pair} into a new token {new_token_id}\")\n",
    "    token_ids = merge(token_ids, most_common_token_pair, new_token_id)\n",
    "    merges[most_common_token_pair] = new_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf26b51-5f85-4ce3-80ed-2b3f31a05360",
   "metadata": {},
   "source": [
    "All that's left is to write the `decode` and `encode` functions.\n",
    "\n",
    "Writing the `decode` functions is simple - we just need to translate every individual ID to the respective string it encodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986a1ae-a4a6-406e-8842-b39445dc4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { idx: bytes([idx]) for idx in range(256) }\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    return tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "print(decode([104, 101, 108, 108, 111, 32, 301, 108, 100, 33]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936ea6c-e227-451b-8215-62a4715e665f",
   "metadata": {},
   "source": [
    "The `encode` function is a bit harder and happens in an iterative way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5ad34-ff44-4bef-9fa3-13e937bc171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    token_ids = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        counts = get_token_pair_counts(token_ids)\n",
    "        pair = min(counts, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        token_ids = merge(token_ids, pair, idx)\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2e49d-b475-4f4d-a354-cd3acc4b6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2537b-c1d2-42fe-bf33-9a10926a4655",
   "metadata": {},
   "source": [
    "## Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f8754-4edf-4164-9057-97a2a8d852d3",
   "metadata": {},
   "source": [
    "Note that the tokenizer is conceptually distinct from the rest of the LLM.\n",
    "It often uses its own training set, which may differ from that of the LLM, to train its vocabulary using the BPE algorithm.\n",
    "\n",
    "The tokenizer's role is to translate between text and numbers.\n",
    "The LLM only processes numbers and never interacts directly with the text.\n",
    "In theory, once the text is translated into numerical form, the original text could be discarded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
