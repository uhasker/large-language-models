{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a17b97-5311-4fb4-806f-9290c7d549a8",
   "metadata": {},
   "source": [
    "# Deep Dive Into GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44132849-22a1-4b67-bc00-166626502dd0",
   "metadata": {},
   "source": [
    "In this chapter, we take a deep dive into the architecture of one of the first truly _Large_ Language Models - **GPT-2**.\n",
    "GPT-2 is an LLM that was released by OpenAI in 2019, which sparked widespread public discussion about the potential benefits and dangers of LLMs.\n",
    "\n",
    "The reason we chose GPT-2 is simple.\n",
    "The model is not too large (\"just\" 1.5 billion parameters), so you will be able to load it into the memory of your local machine without having to provision a GPU instance on some cloud provider.\n",
    "\n",
    "As usual, we need to import a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8fce5-619c-45ea-b7af-468348a6ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ff1e6-abd7-4ec8-ba17-3b60a26cf46e",
   "metadata": {},
   "source": [
    "We will also disable the `huggingface_hub` progress bars as to not pollute the book (you should probably keep them though when following along):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021e8af-ab64-45fc-ba35-b84215bed550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f2aae-003d-4b58-9cce-8ca694fbed30",
   "metadata": {},
   "source": [
    "Note that we use `torch==2.4.0` and `transformers==4.44.0`.\n",
    "If you want to follow along, you should also probably have these versions installed. Otherwise, you might need to change some of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a680a-a1d2-48db-8f19-afc03ecb5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d40398-78b3-4c6f-b521-7ae6a981c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598a43a-4159-48cb-920a-45bf60b201fa",
   "metadata": {},
   "source": [
    "The code in this chapter is written in such a way that it closely mimicks the `transformers` codebase.\n",
    "Generally, we highly encourage you to read the `transformers` codebase - it is well-written and easy to understand.\n",
    "\n",
    "The most relevant file for the purposes of this chapter is [`models/gpt2/modeling_gpt2.py`](https://github.com/huggingface/transformers/blob/v4.44-release/src/transformers/models/gpt2/modeling_gpt2.py) (especially the `GPT2LMHeadModel`, `GPT2Block`, `GPT2SdpaAttention` and `GPT2MLP` classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8cf6d-e4e7-472c-9bdd-b3ba2b9294be",
   "metadata": {},
   "source": [
    "## Loading the Model and Performing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f43410-bf58-42a5-baaf-055c63fba0b0",
   "metadata": {},
   "source": [
    "First, let's load the `gpt2` tokenizer and the `gpt2` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138676bf-9343-4dee-8217-2cf69b9e6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7225b-4536-43ae-ba61-45fb77c02f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5863d-790c-4b8f-bf92-ebac7c5c6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88659cea-5415-4399-bf1a-22ee835f6247",
   "metadata": {},
   "source": [
    "Next, let's perform some inference using an example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425f42b-905c-4f2d-ac14-53c3b4e7fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is an example sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82f10d-16a0-42b8-a0a0-d76dab57f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3ba86-b335-4901-b0f3-cce3af2f1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ba4d2-c7a4-4f32-9c62-4b33f1baac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc6dde-ee49-47ee-8c6e-61e201b324a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38bba3-27df-4dff-bd1d-70f7197a72f4",
   "metadata": {},
   "source": [
    "The `output` of the model is a `CausalLMOutputWithCrossAttentions` object that has (among other things) a `logits` attribute.\n",
    "This is basically the list of probabilities we discussed in Chapter 1, except that it contains the logits of the probabilities (for numerical reasons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf0e68-211c-4db8-8afe-78f59a04791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f503924-ae9c-4e09-bc91-572fdfaefb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a79d7-f3b1-44ca-b5a4-41515185c4eb",
   "metadata": {},
   "source": [
    "Note that `output.logits` is a tensor with three dimensions.\n",
    "\n",
    "The first dimension represents the batch size. \n",
    "Since we only have a single text, the batch size is simply `1`.\n",
    "\n",
    "The second dimension is the number of tokens in the sequence.\n",
    "We have five tokens, and therefore, the second dimension has the size `5`.\n",
    "\n",
    "Finally, the third dimension is the size of the vocabulary (as we output a probability for every token).\n",
    "Since the vocabulary size is `50257`, the third dimension has the size  `50257`.\n",
    "\n",
    "We want to predict the token that follows the last token.\n",
    "Therefore, we are interested in the probabilities of the tokens that come after the _last_ token, so let's extract those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72eeeb-c902-4926-aad1-7ef73180ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_logits = output.logits[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e44732-88d0-4461-9e2b-3569d77e9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6bbf3-0cf6-4f2e-95a2-24fb9c468530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa03fed-c6d1-4bd9-9b2a-a1dcb940cfc7",
   "metadata": {},
   "source": [
    "Let's now get the actual probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e98476-8acf-4aa9-9625-1534544b6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(last_logits, dim=0)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9f4e5-0086-4779-986f-66d2e1516d0c",
   "metadata": {},
   "source": [
    "Finally, we sample the next token from this probability distribution.\n",
    "There are many different ways to accomplish this; the simplest is to select the token with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131e4a2-d961-4ae3-bdf8-0666602950e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_id = torch.argmax(probas, dim=-1)\n",
    "print(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b9a0a6-6ee3-4b3f-b6b1-a83e50a7e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bba75-9b3e-47af-bdad-f7bbb28d6f49",
   "metadata": {},
   "source": [
    "Let's also have a look at the `10` most probable tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0d9cc-cb11-4bfe-bc04-4ac8fe5681ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_k_probs, top_k_ids = torch.topk(probas, top_k)\n",
    "\n",
    "top_k_tokens = [(token_id, tokenizer.decode(token_id)) for token_id in top_k_ids]\n",
    "\n",
    "for (token_id, token), prob in zip(top_k_tokens, top_k_probs):\n",
    "    print(f\"Token: {token} (ID = {token_id}), Probability: {round(prob.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee55363-36b1-4797-a1ac-750212058ca1",
   "metadata": {},
   "source": [
    "We can see that all of these tokens are reasonable candidates for the next token in the sentence `\"This is an example sentence\"`.\n",
    "\n",
    "In fact, instead of simply selecting the most probable next token at every step (which often leads to repetitive and boring texts), we could _sample_ from the probability distribution.\n",
    "Here, we would choose a random token weighted by the probabilities of the tokens (i.e. tokens with higher probabilities are more likely to be sampled):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a014762-f731-49d1-b64e-3e3cf51ff86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sampled_token_id = torch.multinomial(probas, num_samples=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aa6ce-20d8-41f1-9cc7-966f873f066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampled_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996fc2d-b53c-4fe8-91f7-545667c0c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(sampled_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82700c-af96-4c7b-95b2-62bcfe088fbc",
   "metadata": {},
   "source": [
    "Now, that we have seen how to automatically compute the probabilities of the next token, let's redo the calculations manually - layer by layer.\n",
    "Before we do that, we will inspect the architecture of the model first to see what layers are actually present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d29b2-0597-4dad-8df7-456be8f17559",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6498-06af-40bd-95ac-73ecf50c9c8d",
   "metadata": {},
   "source": [
    "The `model` we have loaded is actually a `torch.nn.Module` that is the PyTorch base class for all neural network modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbceae16-9938-4c3a-8190-deb49f717e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(isinstance(model, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48ece6-878b-4a1d-84ee-51c63418f465",
   "metadata": {},
   "source": [
    "There are many ways to inspect the architecture of a `torch.nn.Module`. One straightforward method is to simply `print` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad79de-8a0e-48bd-84ca-2149d45f0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd9e39-f1e3-4f84-8888-b11d885c5e07",
   "metadata": {},
   "source": [
    "The model consists of two parts: a `transformer` and an `lm_head`.\n",
    "Two important points should be noted here:\n",
    "\n",
    "First, the tokenizer is not part of the `model` as it is already represented by the `tokenizer` variable.\n",
    "Second, the `transformer` object includes both the embedding block and the transformer (in terms of the terminology introduced in Chapter 1).\n",
    "\n",
    "Looking closely, we see that the model has two embedding layers at the beginning - `wte` and `wpe`.\n",
    "The `wte` layer is the embedding layer for the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6560dc-f154-4953-a17f-a45980bbc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transformer.wte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8188e25-036b-43e4-8151-e8236e53ffa4",
   "metadata": {},
   "source": [
    "The `wpe` layer is the positional embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e6059-40bc-4752-b585-962f77f3ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transformer.wpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c7704-8145-4220-a036-bf29eb449300",
   "metadata": {},
   "source": [
    "These layers are followed by a dropout layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de427f-13c2-45ea-ab90-07379e309026",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transformer.drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e649f44-b8b3-4e15-8e89-67fdd43286fd",
   "metadata": {},
   "source": [
    "Next, we have a module list consisting of 12 \"GPT blocks\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f4333-43ce-46df-a2f4-cb6db0cf6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.transformer.h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27baf5a6-a8cf-497d-acb4-a4635b02fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.transformer.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831e87a-9f5e-4029-973a-d522742c3805",
   "metadata": {},
   "source": [
    "Each block is a so-called `GPT2Block` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec809641-332e-4739-b839-32eb5c754898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.transformer.h[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac64e5-4e00-4ac4-ade1-ca8f0214f4a4",
   "metadata": {},
   "source": [
    "When we look inside a `GPT2Block`, we will encounter many familiar components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be03888-b7be-4f08-953d-379177de66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transformer.h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e55d8e-43b7-4780-a0c2-2b027f8634d3",
   "metadata": {},
   "source": [
    "The module list with the GPT blocks is followed by one last layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a38b0b-431d-4996-86f2-58cec94e8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transformer.ln_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82d0b1-ff04-49f1-8227-5d2954fb13c3",
   "metadata": {},
   "source": [
    "The final component of the entire model is a linear layer which is responsible for computing the logits of the probabilities of the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d3408-0119-48ec-8e92-8db88917b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606b2dc-92df-4a02-8274-3376061a0093",
   "metadata": {},
   "source": [
    "Generally speaking, whenever you want to understand how a particular LLM model works, printing its architecture is extremely instructive as it provides a basic overview of the components it has.\n",
    "\n",
    "Now let's see how the tensors actually flow through the model.\n",
    "\n",
    "We will start with the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e604f-373f-4a6a-a93d-9ac108debabd",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a199e-6a4f-4e82-8fb5-40a2f3d7a891",
   "metadata": {},
   "source": [
    "Let's retrieve the token IDs of our example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa499601-7eaa-4da7-a094-27104698ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94223803-7a83-4e14-bdf7-90f132b901b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37ce1e-20ed-4db4-9ffd-053912e2c859",
   "metadata": {},
   "source": [
    "We also obtain the attention mask for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9d811-9c38-4bbf-9e2c-b177d828ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7203f1-7055-42d7-b482-81f050f7dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b2b7b-927e-43c0-99c1-3084d9d6d83e",
   "metadata": {},
   "source": [
    "Next, we need to generate the position IDs.\n",
    "\n",
    "For each token ID, we require a corresponding position ID.\n",
    "The position ID sequence is constructed by simply starting at `0` and then counting up to `len(token_ids) - 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36545f28-0536-4263-a8f7-3c200bc51160",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.tensor([[0, 1, 2, 3, 4]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a96068-ec39-4dbc-9f00-68d2608520b4",
   "metadata": {},
   "source": [
    "Let's now calculate the token embeddings.\n",
    "This is just a matter of applying the token embedding layer (i.e. `wte`) to the tensor containing the token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20346315-9f21-41ed-8975-92252d608559",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds = model.transformer.wte(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09865d-4b9f-4cab-846a-68a9f3f83600",
   "metadata": {},
   "source": [
    "We can do a quick dimensionality check.\n",
    "The `token_ids` is a sequence of dimension `1x5` (batch size `1` and sequence length of `5`).\n",
    "For each token, we compute an embedding of dimension `768`, and therefore, the output tensor should have a dimension of `1x5x768`.\n",
    "This is indeed the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4482c3b-d5c9-4fc1-bf9c-91db62b5924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee8bfb-7559-4d34-abaa-a195d8414bb0",
   "metadata": {},
   "source": [
    "Let's also calculate the positional embeddings.\n",
    "This calculation is very similar to the calculation of the token embeddings, except that we now apply the _positional_ embedding layer (i.e. `wpe`) to the tensor containing the position IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e010997-ec76-4d3d-bea9-704862e44c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeds = model.transformer.wpe(position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c383ff-7056-4818-aa18-ce0382e914ad",
   "metadata": {},
   "source": [
    "Again, we perform a quick dimensionality check.\n",
    "The `position_ids` sequence has a dimension of `1x5` (with a batch size `1` and `5` position IDs).\n",
    "For each position, we compute an embedding of dimension `768`, so the resulting output tensor should have a dimension of `1x5x768`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed089e27-95c3-418d-8a86-4394d143a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(position_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56ceb6-f123-4909-ad8f-245c1ece749c",
   "metadata": {},
   "source": [
    "To get the final embeddings we simply _add_ the token embeddings and the positional embeddings.\n",
    "\n",
    "These final embeddings will be passed to the module block as the input to the first layer.\n",
    "Since the `transformers` codebase refers to the intermediate tensors in the module block as \"hidden states\", we will adopt that terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbff8c-1b4d-4b9d-925a-fe68a7a2bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = token_embeds + position_embeds\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d2790-975d-419f-8b1c-440c39952ed5",
   "metadata": {},
   "source": [
    "Here is a graphic representation of the tensor flow so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb1a35-595c-4823-a423-da6abc02b067",
   "metadata": {},
   "source": [
    "![Embeddings](images/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fe52a-302b-4dee-9947-9e4918587e0e",
   "metadata": {},
   "source": [
    "## The First GPT2 Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f18f6-7327-446e-a44b-04fcee4f9c41",
   "metadata": {},
   "source": [
    "Let's now have a look at the module list, specifically its first \"GPT block\".\n",
    "\n",
    "First, we will assign more meaningful names to both the module list and the GPT block instead of using `h` and `h[0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d654011-01fb-49da-96b5-32b6361a09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_blocks = model.transformer.h\n",
    "layer_block = layer_blocks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6493bf-b678-4a47-b8bb-c56ec37d763c",
   "metadata": {},
   "source": [
    "As a reminder, here is how the layer block looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26f84f-1bec-4cdf-bc73-e508f5dc0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061893e8-1dd5-4c27-aff1-c46ebe7d6d34",
   "metadata": {},
   "source": [
    "Basically, a `GPT2Block` has two components: an attention part and an MLP part. \n",
    "\n",
    "The attention part consists of a `LayerNorm` layer, followed by a `GPT2SdpaAttention` block which contains the attention mechanism.\n",
    "\n",
    "The MLP part consists of a `LayerNorm` layer, followed by a `GPT2MLP` block which contains a simple MLP with two linear layers separated by a non-linear activation function.\n",
    "\n",
    "We will now look at both parts in detail.\n",
    "\n",
    "Let's rename the tensor and also save it in another variable since we will need it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff072217-0124-41b6-ae2b-d672ae854411",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_input = hidden_states\n",
    "attention_residual = attention_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773ba5b-b2cb-4012-917c-ccb2906a68b0",
   "metadata": {},
   "source": [
    "### The Attention Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b0977-b7ef-4bc7-989e-96d77c0d1255",
   "metadata": {},
   "source": [
    "First, we perform layer normalization on our tensor using the `LayerNorm` layer.\n",
    "\n",
    "Remember that this operation doesn't change the dimension of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1430d9-e39e-4a81-937a-8b1f544f4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_attention_input = layer_block.ln_1(attention_input)\n",
    "print(normalized_attention_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671df682-f29e-48a0-8387-9111d09e2020",
   "metadata": {},
   "source": [
    "Next, we want to get the query, key and value tensors.\n",
    "\n",
    "This is what the `c_attn` layer is for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5abed-b6ee-4c9e-b9e1-43374da7d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_key_value = layer_block.attn.c_attn(normalized_attention_input)\n",
    "print(query_key_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f82636-0030-46f0-8772-a85a2a1a4faf",
   "metadata": {},
   "source": [
    "Again, the first dimension of the tensor is the batch size (which is `1`), while the second dimension represents the number of tokens (which is `5`).\n",
    "\n",
    "The third dimension is more complicated.\n",
    "In the `transformer` codebase, the calculation of the queries, keys and values are combined in a single operation.\n",
    "Therefore, the output tensor contains the queries, keys and value all in one object.\n",
    "This is why the third dimension is `2304 = 768 * 3` (since we store queries _and_ keys _and_ values and each of these has `768` items).\n",
    "\n",
    "Since we want to work with these tensors separately, we need to split them out using the `split` function.\n",
    "We have three dimensions and the queries, keys and values are split across the dimension number 2, so we need to `split` across `dim=2`.\n",
    "\n",
    "The order of the items in the tensor is query first, key second and value third:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e8c41-9b93-430e-a762-b00cbdfdf1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, keys, values = query_key_value.split(768, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa6fcb-0abe-4ce4-b4ce-e74470ce92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queries.shape, keys.shape, values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073259da-762d-40ae-9fd4-99f1298eefbf",
   "metadata": {},
   "source": [
    "Next, we need to split the attention heads.\n",
    "\n",
    "To accomplish this, we will simply reuse the `_split_heads` helper function from the `transformers` codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91071f0-6579-4570-8fa6-ffa267e4fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Splits hidden_size dim into attn_head_size and num_heads.\n",
    "    This function is taken directly from the transformers codebase.\n",
    "    \"\"\"\n",
    "    new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "    tensor = tensor.view(new_shape)\n",
    "    return tensor.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c66e7e-cb80-49e5-9146-8586294fdcc9",
   "metadata": {},
   "source": [
    "The GPT-2 model has 12 attention heads and a head dimension of 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a50683-2120-4047-bc73-c59833014a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 12\n",
    "head_dim = 64\n",
    "head_queries = _split_heads(queries, num_heads, head_dim)\n",
    "head_keys = _split_heads(keys, num_heads, head_dim)\n",
    "head_values = _split_heads(values, num_heads, head_dim)\n",
    "print(head_queries.shape, head_keys.shape, head_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378360a-fce1-42fb-baf7-9c7df5373be8",
   "metadata": {},
   "source": [
    "Next, we compute the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d189e52-1296-47df-9a77-12540fa728f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdpa_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    head_queries,\n",
    "    head_keys,\n",
    "    head_values,\n",
    "    attn_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    is_causal=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138a3a9-5c85-496d-9aa1-6ec179a58810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sdpa_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09976b-1e4f-45ab-b6cb-296367e25e5f",
   "metadata": {},
   "source": [
    "Now, its time to merge the attention heads back together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e832ee2-e430-476e-b1ae-9aa05021c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdpa_output_transposed = sdpa_output.transpose(1, 2).contiguous()\n",
    "sdpa_output_view = sdpa_output_transposed.view(1, 5, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a61e3d-f35d-4cce-b9c5-c97782693b56",
   "metadata": {},
   "source": [
    "Let's double check the dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6012a-fd50-4382-89fa-382c9eb63fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sdpa_output_view.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424e6c6-5743-4e53-a107-5c839c870e88",
   "metadata": {},
   "source": [
    "Now, we pass the the tensor through the projection layer `c_proj`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a7ae1-bed4-439d-9cd8-cec91481983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_output = layer_block.attn.c_proj(sdpa_output_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1154af-cf1d-4787-af2d-861a64c3325c",
   "metadata": {},
   "source": [
    "Again, we verify the dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706ef6b-5941-49b9-96f1-5c567cac311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(projection_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e4b60-5d70-40fb-87e7-46c538cce9e6",
   "metadata": {},
   "source": [
    "Finally, we add our saved hidden state to the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cfd00-4217-417a-abf0-eb2fee0ea79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = projection_output + attention_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b63a0-68ac-4dd9-9ec1-1161e3827f4b",
   "metadata": {},
   "source": [
    "This is called a **residual connection**.\n",
    "\n",
    "Generally, we speak of residual connections if there is some function of the form $y = f(x) + x$ (in this case `x` is the input hidden state).\n",
    "Residual connections essentially help with propagating the \"signal\" across layers (both in the forward and the backward pass).\n",
    "\n",
    "Especially for the backward pass, such connections can help address the vanishing gradient problem (discussed in the chapter on computational graphs).\n",
    "We can see this by comparing the derivative of a hypothetical loss function (with respect to the input $x$) with a residual connection and without a residual connection.\n",
    "\n",
    "Let's compute $\\frac{\\partial L}{\\partial x}$:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} (1 + \\frac{\\partial}{\\partial x} f(x)) = \\frac{\\partial L}{\\partial y} + \\frac{\\partial L}{\\partial y} \\frac{\\partial}{\\partial x} f(x)$\n",
    "\n",
    "Without the residual connection, the derivative would simply be $\\frac{\\partial L}{\\partial y} \\frac{\\partial}{\\partial x} f(x)$.\n",
    "With the residual connection, however, we directly _add_ the term $\\frac{\\partial L}{\\partial y}$ on top of that.\n",
    "This means that even if the gradient of `f` is vanishingly small, this won't be true for the gradient $\\frac{\\partial L}{\\partial x}$ and there should be a meaningful update when we perform gradient descent.\n",
    "\n",
    "Let's print the shape of `attention_output`.\n",
    "Note that the attention part of the GPT block did not change the _shape_ of the tensor, only its _values_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773eebe-e65b-4384-9dd7-70c1ba670eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0415d7-e073-4035-97aa-5395d4c8cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aaeb7f-f32a-4537-8f91-8ba40a8c586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780bb478-a813-4ae3-8877-e8b1b54a640e",
   "metadata": {},
   "source": [
    "Here is a visualization of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5250a-91fa-447a-8ad3-fd6abd414ad8",
   "metadata": {},
   "source": [
    "![Attention](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4090c45-0f66-4653-9164-918da560cd6e",
   "metadata": {},
   "source": [
    "### The MLP Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb12d8-a721-477a-9630-3512c4febf36",
   "metadata": {},
   "source": [
    "The second part of the GPT block is the MLP part.\n",
    "\n",
    "Again, we first save the current hidden state tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4eff2f-2655-472a-b890-2ce2b6ba775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_input = attention_output\n",
    "mlp_residual = mlp_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8017824-681e-40f2-a8ea-216fb9fede0e",
   "metadata": {},
   "source": [
    "And - again - we first pass the tensor through a layer normalization block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f4abb-28bb-455d-a926-2a77c46cef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_nlp_input = layer_block.ln_2(mlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d60eb-9379-4311-b1a3-a6f300ef8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized_nlp_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d9aef-9429-49e7-b429-b19b3555a48c",
   "metadata": {},
   "source": [
    "Next, we pass the hidden states through the MLP block.\n",
    "\n",
    "The MLP block consists of a linear layer, followed by a non-linear activation function, and then another linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500405a-9d9a-4b2e-9fff-0a7071a936ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_block.mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e63097-0ef8-4da8-86ef-88a33dc70e5b",
   "metadata": {},
   "source": [
    "Here is how the tensor flow looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863582b-c951-4572-bb4b-2fc9876abdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fc_output = layer_block.mlp.c_fc(normalized_nlp_input)\n",
    "act_output = layer_block.mlp.act(c_fc_output)\n",
    "c_proj_output = layer_block.mlp.c_proj(act_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1bf83-e11d-4155-aa86-89988e6ccf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_proj_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebccc0d-717c-42c5-8cee-65b8ad658bff",
   "metadata": {},
   "source": [
    "Finally, we have another residual connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be335bba-0e0e-4bf9-a768-ba3a02f59f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_output = mlp_residual + c_proj_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97415aaf-fc17-4c5e-859b-27987d93d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e94cb6-d18c-4001-a578-95bf40a45e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf91230-e26b-4478-9e99-a468798d3bfa",
   "metadata": {},
   "source": [
    "Here is a visualization of the MLP part:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f05a6-7904-4c5b-88e4-05e1a60b7ede",
   "metadata": {},
   "source": [
    "![MLP](images/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1230e6d-e0c5-436b-a4a7-5857348a66ad",
   "metadata": {},
   "source": [
    "## The Other GPT Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25fee92-1497-45e9-a24c-b00c08537f31",
   "metadata": {},
   "source": [
    "Now, we simply pass the hidden states through one block after the other, where the output of each block is the input to the next block.\n",
    "\n",
    "Note that we already passed the tensor through the first block, so we will only consider the other 11 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe9393-2c8b-43a6-9c01-9cdae44b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = mlp_output\n",
    "\n",
    "for block in model.transformer.h[1:]:\n",
    "    hidden_states = block(hidden_states)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d38ed2-148f-4741-b5c1-b6d9bec08495",
   "metadata": {},
   "source": [
    "Since every block only changes the values of the tensor, but not its shape, the shape of the final tensor is _unchanged_ as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089d5de-d5ec-4026-935f-fcc80f4fa603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ca622-c676-4cbb-92ea-a7c8aa00f559",
   "metadata": {},
   "source": [
    "Finally, we pass the final result through one last layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b971b718-631e-475e-be3b-c9154802b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.transformer.ln_f(hidden_states)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271862ca-3c01-4fa1-9c0b-77a1c2da42af",
   "metadata": {},
   "source": [
    "## Calculating the Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374d0ba-3a98-4cbe-96ee-f7ce779b7cb9",
   "metadata": {},
   "source": [
    "At last, we use the `lm_head` layer to calculate the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd596ea8-71a7-4e51-8be5-b41a4242f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.lm_head(hidden_states)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edf132-7b3e-4ba8-a260-3f94c075c646",
   "metadata": {},
   "source": [
    "Let's verify that our calculations are correct by checking if the `logits` tensor, which we computed manually, is the same as `output.logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9131aa3-6d24-402f-a98b-f4fa7a4b3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((output.logits == logits).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc17527-ab46-4db9-a0f3-1e861b0363ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_block)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
